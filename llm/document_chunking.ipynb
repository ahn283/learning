{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting an entire textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the PyPDF2 library to read a PDF file\n",
    "import PyPDF2       # conda install PyPDF2\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datetime import datetime, timezone\n",
    "import hashlib\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import keyring\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OPENAI API API KEY values\n",
    "\n",
    "openai_api_key = keyring.get_password('openai', 'ahn283')\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "INDEX_NAME = 'semantic-search'\n",
    "NAMESPACE = 'default'\n",
    "ENGINE = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper functions to get lists of embeddings from the OpenAI API\n",
    "def get_embeddings(texts, engine=ENGINE):\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=engine\n",
    "    )\n",
    "    \n",
    "    return [d.embedding for d in list(response.data)]\n",
    "\n",
    "def get_embedding(text):\n",
    "    return get_embeddings([text])[0]\n",
    "\n",
    "len(get_embedding('hi')), len(get_embeddings(['hi', 'hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 428/428 [07:19<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# # Open the PDF file in read-binary mode\n",
    "# with open('./data/pds2.pdf', 'rb') as file:\n",
    "\n",
    "# file path\n",
    "file_path = './data/pds2.pdf'\n",
    "\n",
    "# Creating a PDF reader object\n",
    "reader = PyPDF2.PdfReader(file_path)\n",
    "    \n",
    "# Intializing an empty string hold the next\n",
    "principles_of_ds = \"\"\n",
    "    \n",
    "# Loop through each page in the PDF file\n",
    "for page in tqdm(reader.pages):\n",
    "        \n",
    "    # Extract the text from the page\n",
    "    text = page.extract_text()\n",
    "        \n",
    "    # Find the starting point of the next we want to extact\n",
    "    # In this case, we are extracting text starting from the string']' \n",
    "    principles_of_ds += '\\n\\n' + text[text.find(']') + 2:]\n",
    "        \n",
    "# Strip any leading or trailing whitespace from the resulting string\n",
    "principles_of_ds = principles_of_ds.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rinciples of Data Science\\nSecond Edition\\nA beginner\\'s guide to statistical techniques and theory to\\nbuild eﬀective data-driven applications\\nSinan Ozdemir\\nSunil Kakade\\nBIRMINGHAM - MUMBAI\\n\\nrinciples of Data Science\\nSecond Edition\\nCopyright © 2018 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form\\nor by any means, without the prior written permission of the publisher, except in the case of brief quotations\\nembedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented.\\nHowever, the information contained in this book is sold without warranty, either express or implied. Neither the\\nauthors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to\\nhave been caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies and products\\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy\\nof this information.\\nCommissioning Editor:  Amey Varangoankar\\nAcquisition Editor:  Dayne Castelino\\nContent Development Editor:  Chris D\\'cruz\\nTechnical Editor:  Sneha Hanchate\\nCopy Editor:  Safis Editing\\nProject Coordinator:  Namrata Swetta\\nProofreader:  Safis Editing\\nIndexer:  Pratik Shirodkar\\nGraphics:  Tom Scaria\\nProduction Coordinator:  Nilesh Mohite\\nFirst published: December 2016\\nSecond edition: November 2018\\nProduction reference: 1111218\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK.\\nISBN 978-1-78980-454-6\\nwww.packtpub.com\\n\\n\\nmapt.io\\nMapt is an online digital library that gives you full access to over 5,000 books and videos, as\\nwell as industry leading tools to help you plan your personal development and advance\\nyour career. For more information, please visit our website.\\nWhy subscribe?\\nSpend less time learning and more time coding with practical eBooks and Videos\\nfrom over 4,000 industry professionals\\nImprove your learning with Skill Plans built especially for you\\nGet a free eBook or video every month\\nMapt is fully searchable\\nCopy and paste, print, and bookmark content\\nPackt.com\\nDid you know that Packt offers eBook versions of every book published, with PDF and\\nePub files available? You can upgrade to the eBook version at www.packt.com  and as a print\\nbook customer, you are entitled to a discount on the eBook copy. Get in touch with us at\\ncustomercare@packtpub.com  for more details.\\nAt www.packt.com , you can also read a collection of free technical articles, sign up for a\\nrange of free newsletters, and receive exclusive discounts and offers on Packt books and\\neBooks. \\n\\nontributors\\nAbout the authors\\nSinan Ozdemir  is a data scientist, start-up founder, and educator living in the San\\nFrancisco Bay Area. He studied pure mathematics at Johns Hopkins University. He then\\nspent several years conducting lectures on data science at Johns Hopkins University before\\nfounding his own start-up, Kylie.ai, which uses artificial intelligence to clone brand\\npersonalities and automate customer service communications.\\nSinan is also the author of Principles of Data Science, First Edition  available through Packt.\\nSunil Kakade  is a technologist, educator, and senior leader with expertise in creating data-\\nand AI-driven organizations. He is in the adjunct faculty at Northwestern University,\\nEvanston, IL, where he teaches graduate courses of data science and big data. He has\\nseveral research papers to his credit and has presented his work in big data applications at\\nreputable conferences. He has US patents in areas of big data and retail processes. He is\\npassionate about applying data science to improve business outcomes and save patients\\'\\nlives. At present, Sunil leads the information architecture and analytics team for a large\\nhealthcare organization focused on improving healthcare outcomes and lives with his wife,\\nPratibha, and daughter, Preeti, in Scottsdale, Arizona.\\nI would like to thank my mother, Subhadra, wife; Pratibha; and daughter, Preeti, for\\nsupporting me during my education and career and for supporting my passion for\\nlearning. Many thanks to my mentors, Prof. Faisal Akkawi, Northwestern University; Bill\\nGuise, Sr. Director, Dr. Joseph Colorafi, CMIO, and Deanna Wise, CIO at Dignity\\nHealth for supporting my passion for big data, data science, and artificial intelligence.\\nSpecial thanks to Sinan Ozdemir and Packt Publishing for giving me the opportunity to\\nco-author this book. I appreciate the incredible support of my team at Dignity Health\\nInsights in my journey in data science. Finally, I\\'d like to thank my friend, Anand\\nDeshpande, who inspired me to take on this project.\\n\\nbout the reviewers\\nOleg Okun  got his PhD from the Institute of Engineering Cybernetics, National Academy\\nof Sciences (Minsk, Belarus) in 1996. Since 1998 he has worked abroad, doing both academic\\nresearch (in Belarus and Finland) and industrial research (in Sweden and Germany). His\\nresearch experience includes document image analysis, cancer prediction by analyzing\\ngene expression profiles (bioinformatics), fingerprint verification and identification\\n(biometrics), online and offline marketing analytics, credit scoring (microfinance), and text\\nsearch and summarization (natural language processing). He has 80 publications, including\\none IGI Global-published book and three co-edited books published by Springer-Verlag, as\\nwell as book chapters, journal articles, and numerous conference papers. He has also been a\\nreviewer of several books published by Packt Publishing.\\nJared James Thompson , PhD, is a graduate of Purdue University and has held both\\nacademic and industrial appointments teaching programming, algorithms, and big data\\ntechnology. He is a machine learning enthusiast and has a particular love of optimization.\\nJared is currently employed as a machine learning engineer at Atomwise, a start-up that\\nleverages artificial intelligence to design better drugs faster.\\nPackt is searching for authors like you\\nIf you\\'re interested in becoming an author for Packt, please visit authors.packtpub.com\\nand apply today. We have worked with thousands of developers and tech professionals,\\njust like you, to help them share their insight with the global tech community. You can\\nmake a general application, apply for a specific hot topic that we are recruiting an author\\nfor, or submit your own idea.\\n\\nable of Contents\\nPreface 1\\nChapter 1: How to Sound Like a Data Scientist 6\\nWhat is data science? 9\\nBasic terminology 9\\nWhy data science? 10\\nExample – xyz123 Technologies 11\\nThe data science Venn diagram 12\\nThe math 13\\nExample – spawner-recruit models 14\\nComputer programming 16\\nWhy Python? 16\\nPython practices 17\\nExample of basic Python 18\\nExample – parsing a single tweet 19\\nDomain knowledge 20\\nSome more terminology 21\\nData science case studies 22\\nCase study – automating government paper pushing 23\\nFire all humans, right? 24\\nCase study – marketing dollars 25\\nCase study – what\\'s in a job description? 27\\nSummary 30\\nChapter 2: Types of Data 32\\nFlavors of data 32\\nWhy look at these distinctions? 33\\nStructured versus unstructured data 33\\nExample of data pre-processing 34\\nWord/phrase counts 35\\nPresence of certain special characters 35\\nThe relative length of text 35\\nPicking out topics 36\\nQuantitative versus qualitative data 36\\nExample – coffee shop data 37\\nExample – world alcohol consumption data 39\\nDigging deeper 41\\nThe road thus far 42\\nThe four levels of data 42\\nThe nominal level 43\\nMathematical operations allowed 43\\n\\neasures of center 43\\nWhat data is like at the nominal level 44\\nThe ordinal level 44\\nExamples 44\\nMathematical operations allowed 44\\nMeasures of center 45\\nQuick recap and check 46\\nThe interval level 47\\nExample 47\\nMathematical operations allowed 48\\nMeasures of center 48\\nMeasures of variation 49\\nStandard deviation 49\\nThe ratio level 51\\nExamples 51\\nMeasures of center 51\\nProblems with the ratio level 52\\nData is in the eye of the beholder 53\\nSummary 53\\nChapter 3: The Five Steps of Data Science 55\\nIntroduction to data science 55\\nOverview of the five steps 56\\nAsking an interesting question 56\\nObtaining the data 56\\nExploring the data 56\\nModeling the data 57\\nCommunicating and visualizing the results 57\\nExploring the data 57\\nBasic questions for data exploration 58\\nDataset 1 – Yelp 59\\nDataFrames 61\\nSeries 62\\nExploration tips for qualitative data 62\\nNominal level columns 62\\nFiltering in pandas 64\\nOrdinal level columns 67\\nDataset 2 – Titanic 68\\nSummary 73\\nChapter 4: Basic Mathematics 74\\nMathematics as a discipline 74\\nBasic symbols and terminology 75\\nVectors and matrices 75\\nQuick exercises 78\\nAnswers 78\\nArithmetic symbols 78\\nSummation 78\\n\\nroportional 79\\nDot product 79\\nGraphs 82\\nLogarithms/exponents 83\\nSet theory 86\\nLinear algebra 90\\nMatrix multiplication 90\\nHow to multiply matrices 91\\nSummary 94\\nChapter 5: Impossible or Improbable – A Gentle Introduction to\\nProbability 95\\nBasic definitions 95\\nProbability 96\\nBayesian versus Frequentist 97\\nFrequentist approach 98\\nThe law of large numbers 99\\nCompound events 101\\nConditional probability 104\\nThe rules of probability 104\\nThe addition rule 104\\nMutual exclusivity 106\\nThe multiplication rule 107\\nIndependence 108\\nComplementary events 108\\nA bit deeper 110\\nSummary 111\\nChapter 6: Advanced Probability 112\\nCollectively exhaustive events 112\\nBayesian ideas revisited 113\\nBayes\\' theorem 113\\nMore applications of Bayes\\' theorem 117\\nExample – Titanic 117\\nExample  – medical studies 119\\nRandom variables 120\\nDiscrete random variables 121\\nTypes of discrete random variables 127\\nBinomial random variables 127\\nGeometric random variables 129\\nPoisson random variable 131\\nContinuous random variables 133\\nSummary 136\\nChapter 7: Basic Statistics 137\\nWhat are statistics? 137\\nHow do we obtain and sample data? 139\\n\\nbtaining data 139\\nObservational 139\\nExperimental 139\\nSampling data 141\\nProbability sampling 142\\nRandom sampling 142\\nUnequal probability sampling 143\\nHow do we measure statistics? 144\\nMeasures of center 144\\nMeasures of variation 145\\nDefinition 149\\nExample – employee salaries 150\\nMeasures of relative standing 150\\nThe insightful part – correlations in data 156\\nThe empirical rule 159\\nSummary 161\\nChapter 8: Advanced Statistics 162\\nPoint estimates 162\\nSampling distributions 167\\nConfidence intervals 170\\nHypothesis tests 175\\nConducting a hypothesis test 176\\nOne sample t-tests 177\\nExample of a one-sample t-test 178\\nAssumptions of the one-sample t-test 178\\nType I and type II errors 181\\nHypothesis testing for categorical variables 182\\nChi-square goodness of fit test 182\\nAssumptions of the chi-square goodness of fit test 182\\nExample of a chi-square test for goodness of fit 183\\nChi-square test for association/independence 185\\nAssumptions of the chi-square independence test 185\\nSummary 187\\nChapter 9: Communicating Data 188\\nWhy does communication matter? 189\\nIdentifying effective and ineffective visualizations 189\\nScatter plots 190\\nLine graphs 191\\nBar charts 193\\nHistograms 195\\nBox plots 197\\nWhen graphs and statistics lie 201\\nCorrelation versus causation 201\\nSimpson\\'s paradox 204\\nIf correlation doesn\\'t imply causation, then what does? 206\\n\\nerbal communication 206\\nIt\\'s about telling a story 206\\nOn the more formal side of things 207\\nThe why/how/what strategy of presenting 208\\nSummary 208\\nChapter 10: How to Tell If Your Toaster Is Learning – Machine Learning\\nEssentials 210\\nWhat is machine learning? 211\\nExample – facial recognition 211\\nMachine learning isn\\'t perfect 213\\nHow does machine learning work? 214\\nTypes of machine learning 214\\nSupervised learning 215\\nExample – heart attack prediction 216\\nIt\\'s not only about predictions 218\\nTypes of supervised learning 219\\nRegression 219\\nClassification 219\\nData is in the eyes of the beholder 220\\nUnsupervised learning 221\\nReinforcement learning 223\\nOverview of the types of machine learning 224\\nHow does statistical modeling fit into all of this? 226\\nLinear regression 226\\nAdding more predictors 231\\nRegression metrics 234\\nLogistic regression 240\\nProbability, odds, and log odds 242\\nThe math of logistic regression 245\\nDummy variables 248\\nSummary 253\\nChapter 11: Predictions Don\\'t Grow on Trees – or Do They? 255\\nNaive Bayes classification 255\\nDecision trees 264\\nHow does a computer build a regression tree? 266\\nHow does a computer fit a classification tree? 266\\nUnsupervised learning 271\\nWhen to use unsupervised learning 271\\nk-means clustering 272\\nIllustrative example – data points 274\\nIllustrative example – beer! 279\\nChoosing an optimal number for K and cluster validation 282\\nThe Silhouette Coefficient 282\\nFeature extraction and principal component analysis 284\\n\\nummary 296\\nChapter 12: Beyond the Essentials 297\\nThe bias/variance tradeoff 298\\nErrors due to bias 298\\nError due to variance 298\\nExample – comparing body and brain weight of mammals 299\\nTwo extreme cases of bias/variance tradeoff 308\\nUnderfitting 308\\nOverfitting 308\\nHow bias/variance play into error functions 309\\nK folds cross-validation 310\\nGrid searching 315\\nVisualizing training error versus cross-validation error 318\\nEnsembling techniques 320\\nRandom forests 323\\nComparing random forests with decision trees 329\\nNeural networks 329\\nBasic structure 330\\nSummary 337\\nChapter 13: Case Studies 338\\nCase study 1 – Predicting stock prices based on social media 338\\nText sentiment analysis 338\\nExploratory data analysis 339\\nRegression route 351\\nClassification route 353\\nGoing beyond with this example 355\\nCase study 2 – Why do some people cheat on their spouses? 356\\nCase study 3 – Using TensorFlow 363\\nTensorFlow and neural networks 368\\nSummary 374\\nChapter 14: Microsoft Azure Databricks 375\\nThe Microsoft data science environment 375\\nWhat exactly are Spark and PySpark? 376\\nBasic Azure Databricks use 376\\nSetting up our first cluster 377\\nCase study 1 – bike-sharing usage prediction using parallelization in Azure\\nDatabricks 378\\nCase study 2 – Using MLlib in Azure Databricks to predict credit card fraud 391\\nUsing the MLlib Grid Search module to tune hyperparameters 399\\nCase study 3 – Using Azure Databricks to optimize our hyperparameter\\ntuning 403\\nHow to add Python libraries to your cluster 403\\nUsing spark_sklearn to build an MNIST classifier 404\\nSummary 406\\n\\nther Books You May Enjoy 407\\nIndex 410\\n\\nreface\\nThe topic of this book is data science, which is a field of study that has been growing\\nrapidly for the past few decades. Today, more companies than ever before are investing in\\nbig data and data science to improve business performance, drive innovation, and create\\nnew revenue streams by building data products. According to LinkedIn\\'s 2017 US\\nEmerging Jobs Report, machine learning engineer, data scientist, and big data engineer\\nrank among the top emerging jobs, and companies in a wide range of industries are seeking\\npeople with the requisite skills for those roles.\\nWe will dive into topics from all three areas  and solve complex problems. We will clean,\\nexplore, and analyze data in order to derive scientific and accurate conclusions. Machine\\nlearning and deep learning techniques will be applied to solve complex data tasks.\\nWho this book is for\\nThis book is for people who are looking to understand and utilize the basic practices of data\\nscience for any domain. The reader should be fairly well acquainted with basic mathematics\\n(algebra, and perhaps probability) and should feel comfortable reading snippets in\\nR/Python as well as pseudo code. The reader is not expected to have worked in a data field;\\nhowever, they should have the urge to learn and apply the techniques put forth in this book\\nto either their own datasets or those provided to them.\\nWhat this book covers\\nChapter 1 , How to Sound Like a Data Scientist , introduces the basic terminology used by data\\nscientists and looks at the types of problem we will be solving throughout this book.\\nChapter 2 , Types of Data , looks at the different levels and types of data out there and shows\\nhow to manipulate each type. This chapter will begin to deal with the mathematics needed\\nfor data science.\\nChapter 3 , The Five Steps of Data Science , uncovers the five basic steps of performing data\\nscience, including data manipulation and cleaning, and shows examples of each step in\\ndetail.\\n\\nhapter 4 , Basic Mathematics , explains the basic mathematical principles that guide the\\nactions of data scientists by presenting and solving examples in calculus, linear algebra, and\\nmore.\\nChapter 5 , Impossible or Improbable – a Gentle Introduction to Probability , is a beginner\\'s guide\\nto probability theory and how it is used to gain an understanding of our random universe.\\nChapter 6 , Advanced Probability , uses principles from the previous chapter and introduces\\nand applies theorems, such as Bayes\\' Theorem, in the hope of uncovering the hidden\\nmeaning in our world.\\nChapter 7 , Basic Statistics , deals with the types of problem that statistical inference attempts\\nto explain, using the basics of experimentation, normalization, and random sampling.\\nChapter 8 , Advanced Statistics , uses hypothesis testing and confidence intervals to gain\\ninsight from our experiments. Being able to pick which test is appropriate and how to\\ninterpret p-values and other results is very important as well.\\nChapter 9 , Communicating Data , explains how correlation and causation affect our\\ninterpretation of data. We will also be using visualizations in order to share our results with\\nthe world.\\nChapter 10 , How to Tell Whether Your Toaster Is Learning – Machine Learning Essentials ,\\nfocuses on the definition of machine learning and looks at real-life examples of how and\\nwhen machine learning is applied. A basic understanding of the relevance of model\\nevaluation is introduced.\\nChapter 11 , Predictions Don\\'t Grow on Trees, or Do They? , looks at more complicated\\nmachine learning models, such as decision trees and Bayesian predictions, in order to solve\\nmore complex data-related tasks.\\nChapter 12 , Beyond the Essentials , introduces some of the mysterious forces guiding data\\nscience, including bias and variance. Neural networks are introduced as a modern deep\\nlearning technique.\\nChapter 13 , Case Studies , uses an array of case studies in order to solidify the ideas of data\\nscience. We will be following the entire data science workflow from start to finish multiple\\ntimes for different examples, including stock price prediction and handwriting detection.\\nChapter 14 , Microsoft Databricks Case Studies , will harness the power of the Microsoft data\\nenvironment as well as Apache Spark to put our machine learning in high gear. This\\nchapter makes use of parallelization and advanced visualization software to get the most\\nout of our data.\\n\\no get the most out of this book\\nThis book will attempt to bridge the gap between math, programming, and domain\\nexpertise. Most people today have expertise in at least one of these (maybe two), but proper\\ndata science requires a little bit of all three .\\nDownload the example code files\\nYou can download the example code files for this book from your account at\\nwww.packt.com . If you purchased this book elsewhere, you can visit\\nwww.packt.com/support  and register to have the files emailed directly to you.\\nYou can download the code files by following these steps:\\nLog in or register at www.packt.com . 1.\\nSelect the SUPPORT  tab. 2.\\nClick on Code Downloads & Errata . 3.\\nEnter the name of the book in the Search  box and follow the onscreen 4.\\ninstructions.\\nOnce the file is downloaded, please make sure that you unzip or extract the folder using the\\nlatest version of:\\nWinRAR/7-Zip for Windows\\nZipeg/iZip/UnRarX for Mac\\n7-Zip/PeaZip for Linux\\nThe code bundle for the book is also hosted on GitHub\\nat https://github.com/PacktPublishing/ Principles-of-Data-Science-Second-\\nEdition . In case there\\'s an update to the code, it will be updated on the existing GitHub\\nrepository.\\nWe also have other code bundles from our rich catalog of books and videos available\\nat https:/ \\u200b/\\u200bgithub. \\u200bcom/ \\u200bPacktPublishing/ \\u200b. Check them out!\\nDownload the color images\\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this\\nbook. You can download it here: https:/ \\u200b/\\u200bwww. \\u200bpacktpub. \\u200bcom/ \\u200bsites/ \\u200bdefault/ \\u200bfiles/\\ndownloads/ \\u200b9781789804546_ \\u200bColorImages. \\u200bpdf.\\n\\nonventions used\\nThere are a number of text conventions used throughout this book.\\nCodeInText : Indicates c ode words in text, database table names, folder names, filenames,\\nfile extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an\\nexample:  \"Mount the downloaded WebStorm-10*.dmg  disk image file as another disk in\\nyour system.\"\\nA block of code is set as follows:\\ndict = {\"dog\": \"human\\'s best friend\", \"cat\": \"destroyer of world\"}\\ndict[\"dog\"]# == \"human\\'s best friend\"\\nlen(dict[\"cat\"]) # == 18\\n# but if we try to create a pair with the same key as an existing key\\ndict[\"dog\"] = \"Arf\"\\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines\\nor items are set in bold:\\ndef jaccard(user1, user2):\\n  stores_in_common = len(user1 & user2)\\n  stores_all_together = len(user1 | user2)\\n  return stores / float(stores_all_together)\\nAny command-line input or output is written as follows:\\nimport numpy as np\\nBold : Indicates a new term, an important word, or w ords that you see onscreen. For\\nexample, words in menus or dialog boxes appear in the text like this. Here is an example:\\n\"Select System info  from the Administration  panel. \"\\nWarnings or important notes appear like this.\\nTips and tricks appear like this.\\n\\net in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback : If you have questions about any aspect of this book, mention the book\\ntitle in the subject of your message and  email us at customercare@packtpub.com .\\nErrata : Although we have taken every care to ensure the accuracy of our content, mistakes\\ndo happen. If you have found a mistake in this book, we would be grateful if you would\\nreport this to us. Please visit www.packt.com/submit-errata , selecting your book, clicking\\non the Errata Submission Form link, and entering the details.\\nPiracy : If you come across any illegal copies of our works in any form on the Internet, we\\nwould be grateful if you would provide us with the location address or website name.\\nPlease contact us at copyright@packt.com  with a link to the material.\\nIf you are interested in becoming an author : If there is a topic that you have expertise in\\nand you are interested in either writing or contributing to a book, please visit\\nauthors.packtpub.com .\\nReviews\\nPlease leave a review. Once you have read and used this book, why not leave a review on\\nthe site that you purchased it from? Potential readers can then see and use your unbiased\\nopinion to make purchase decisions, we at Packt can understand what you think about our\\nproducts, and our authors can see your feedback on their book. Thank you!\\nFor more information about Packt, please visit packt.com .\\n\\n\\nHow to Sound Like a Data\\nScientist\\nNo matter which industry you work in —IT, fashion, food, or finance —there is no doubt\\nthat data affects your life and work. At some point this week, you will either have or hear a\\nconversation about data. News outlets are covering more and more stories about data leaks,\\ncybercrimes, and how data can give us a glimpse into our lives. But why now? What makes\\nthis era such a hotbed of data-related industries?\\nIn the nineteenth century, the world was in the grip of the I ndustrial Age . Mankind was\\nexploring its place in the industrial world, working with giant mechanical inventions.\\nCaptains of industry, such as Henry Ford, recognized that using these machines could open\\nmajor market opportunities, enabling industries to achieve previously unimaginable\\nprofits. Of course, the Industrial Age had its pros and cons. While mass production placed\\ngoods in the hands of more consumers, our battle with pollution also began at around this\\ntime.\\nBy the twentie th century, we were quite skilled at making huge machines; the goal now\\nwas to make them smaller and faster. The Industrial Age was over and was replaced by\\nwhat we now refer to as the I nformation Age . We started using machines to gather and\\nstore information (data) about ourselves and our environment for the purpose of\\nunderstanding our universe.\\n\\neginning in the 1940s, machines such as ENIAC  (considered one of the first —if not the\\nfirst—computers) were computing math equations and running models and simulations\\nlike never before. The following photograph shows ENIAC:\\nENIAC —The world\\'s ﬁrst electronic digital computer (Ref: http://ftp.arl.mil/ftp/historic-computers/)\\nWe finally had a decent lab assistant who could run the numbers better than we could! As\\nwith the Industrial Age, the Information Age brought us both the good and the bad. The\\ngood was the extraordinary works of technology, including mobile phones and televisions.\\nThe bad was not as bad as worldwide pollution, but still left us with a problem in the\\ntwenty-fir st century —so much data.\\n\\nhat\\'s right —the Information Age, in its quest to procure data, has exploded the production\\nof electronic data. Estimates show that we created about 1.8 trillion gigabytes of data in\\n2011 (take a moment to just think about how much that is). Just one year later, in 2012, we\\ncreated over 2.8 trillion gigabytes of data! This number is only going to explode further to\\nhit an estimated 40 trillion gigabytes of created data in just one year by 2020. People\\ncontribute to this every time they tweet, post on Facebook, save a new resume on Microsoft\\nWord, or just send their mom a picture by text message.\\nNot only are we creating data at an unprecedented rate, but we are also consuming it at an\\naccelerated pace as well. Just five years ago, in 2013, the average cell phone user used under\\n1 GB of data a month. Today, that number is estimated to be well over 2 GB a month. We\\naren\\'t just looking for the next personality quiz —what we are looking for is insight. With all\\nof this data out there, some of it has to be useful to me! And it can be!\\nSo we, in the twenty-fir st century, are left with a problem. We have so much data and we\\nkeep making more. We have built insanely tiny machines that collect data 24/7, and it\\'s our\\njob to make sense of it all. Enter the D ata Age . This is the age when we take machines\\ndreamed up by our nineteenth century ancestors and the data created by our twentieth\\ncentury counterparts and create insights and sources of knowledge that every human on\\nEarth can benefit from. The United States created an entirely new role in the government of\\nchief data scientist. Many companies are now investing in data science departments and\\nhiring data scientists. The benefit is quite obvious —using data to make accurate predictions\\nand simulations gives us insight into our world like never before.\\nSounds great, but what\\'s the catch?\\nThis chapter will explore the terminology and vocabulary of the modern data scientist. We\\nwill learn keywords and phrases that will be essential in our discussion of data science\\nthroughout this book. We will also learn why we use data science and learn about the three\\nkey domains that data science is derived from before we begin to look at the code in\\nPython, the primary language used in this book. This chapter will cover the following\\ntopics:\\nThe basic terminology of data science\\nThe three domains of data science\\nThe basic Python syntax\\n\\nhat is data science?\\nBefore we go any further, let\\'s look at some  basic definitions that we will use throughout\\nthis book. The great/awful thing about this field is that it is so young that these definitions\\ncan differ from textbook to newspaper to whitepaper.\\nBasic terminology\\nThe definitions that follow are general  enough to be used in daily conversations, and work\\nto serve the purpose of this book, an introduction to the principles of data science.\\nLet\\'s start by defining what data is. This might seem like a silly first definition to look at,\\nbut it is very important. Whenever we use the word \"data,\" we refer to a collection of\\ninformation in either an organized  or unorganized  format. These formats have the\\nfollowing qualities:\\nOrganized data : This refers to data  that is sorted into a row/column structure,\\nwhere every row represents a single observation  and the columns represent the\\ncharacteristics  of that observation.\\nUnorganized data : This is the type of data  that is in a free form, usually text or\\nraw audio/signals that must be parsed further to become organized.\\nWhenever you open Excel (or any other spreadsheet program), you are looking at a blank\\nrow/column structure waiting for organized data. These programs don\\'t do well with\\nunorganized data. For the most part, we will deal with organized data as it is the easiest to\\nglean insights from, but we will not shy away from looking at raw text and methods of\\nprocessing unorganized forms of data.\\nData science is the art and science of acquiring knowledge through data.\\nWhat a small definition for such a big topic, and rightfully so! Data science covers so many\\nthings that it would take pages to list it all out (I should know —I tried and got told to edit it\\ndown).\\nData science is all about how we take data, use it to acquire knowledge, and then use that\\nknowledge to do the following:\\nMake decisions\\nPredict the future\\nUnderstand the past/present\\nCreate new industries/products\\n\\nhis book is all about the methods of data science, including how to process data, gather\\ninsights, and use those insights to make informed decisions and predictions.\\nData science is about using data in order to gain new insights that you would otherwise\\nhave missed.\\nAs an example, using data science, clinics can identify patients who are likely to not show\\nup for an appointment. This can help improve margins, and providers can give other\\npatients available slots.\\nThat\\'s why data science won\\'t replace the human brain, but complement it, working\\nalongside it. Data science should not be thought of as an end-all solution to our data woes;\\nit is merely an opinion —a very informed opinion, but an opinion nonetheless. It deserves a\\nseat at the table.\\nWhy data science?\\nIn this Data Age, it\\'s clear that we have  a surplus of data. But why should that necessitate\\nan entirely new set of vocabulary? What was wrong with our previous forms of analysis?\\nFor one, the sheer volume of data makes it literally impossible for a human to parse it in a\\nreasonable time frame. Data is collected in various forms and from different sources, and\\noften comes in a very unorganized format.\\nData can be missing, incomplete, or just flat out wrong. Oftentimes, we will have data on\\nvery different scales, and that makes it tough to compare it. Say that we are looking at data\\nin relation to pricing used cars. One characteristic of a car is the year it was made, and\\nanother might be the number of miles on that car. Once we clean our data (which we will\\nspend a great deal of time looking at in this book), the relationships between the data\\nbecome more obvious, and the knowledge that was once buried deep in millions of rows of\\ndata simply pops out. One of the main goals of data science is to make explicit practices and\\nprocedures to discover and apply these relationships in the data.\\nEarlier, we looked at data science in a more historical perspective, but let\\'s take a minute to\\ndiscuss its role in business today using a very simple example.\\n\\nxample – xyz123 Technologies\\nBen Runkle, the CEO of xyz123 Technologies,  is trying to solve a huge problem. The\\ncompany is consistently losing long-time customers. He does not know why they are\\nleaving, but he must do something fast. He is convinced that in order to reduce  his churn,\\nhe must create new products and features, and consolidate existing technologies. To be\\nsafe, he calls in his chief data scientist, Dr. Hughan. However, she is not convinced that new\\nproducts and features alone will save the company. Instead, she turns to the transcripts of\\nrecent customer service tickets. She shows Ben the most recent transcripts and finds\\nsomething surprising:\\n\".... Not sure how to export this; are you?\"\\n\"Where is the button that makes a new list?\"\\n\"Wait, do you even know where the slider is?\"\\n\"If I can\\'t figure this out today, it\\'s a real problem...\"\\nIt is clear that customers were having problems with the existing UI/UX, and weren\\'t upset\\nbecause of a lack of features. Runkle and Hughan organized a mass UI/UX overhaul and\\ntheir sales have never been better.\\nOf course, the science  used in the last example was minimal, but it makes a point. We tend\\nto call people like Runkle drivers. Today\\'s common stick-to-your-gut CEO wants to make\\nall decisions quickly and iterate over solutions until something works. Dr. Hughan is much\\nmore analytical. She wants to solve the problem just as much as Runkle, but she turns to\\nuser-generated data instead of her gut feeling for answers. Data science is about applying\\nthe skills of the analytical mind and using them as a driver would.\\nBoth of these mentalities have their place in today\\'s enterprises; however, it is Hughan\\'s\\nway of thinking that dominates the ideas of data science —using data generated by the\\ncompany as her source of information, rather than just picking up a solution and going\\nwith it.\\n\\nhe data science Venn diagram\\nIt is a common misconception that only those with a PhD or geniuses can understand the\\nmath/programming behind data science. This is absolutely false. Understanding data\\nscience begins with three basic areas:\\nMath/statistics : This is the use of equations and formulas to perform analysis.\\nComputer programming : This is the ability to use code to create outcomes on a\\ncomputer.\\nDomain knowledge : This refers  to understanding the problem domain\\n(medicine, finance, social science, and so on).\\nThe following Venn diagram provides a visual representation of how these three areas of\\ndata science intersect:\\nThe Venn diagram of data science\\nThose with hacking skills can conceptualize and program complicated algorithms using\\ncomputer languages. Having a math and statistics background allows you to theorize and\\nevaluate algorithms and tweak the existing procedures to fit specific situations. Having\\nsubstantive expertise (domain expertise) allows you to apply concepts and results in a\\nmeaningful and effective way.\\n\\nhile having only two of these three qualities can make you intelligent, it will also leave a\\ngap. Let\\'s say that you are very skilled in coding and have formal training in day trading.\\nYou might create an automated system to trade in your place, but lack the math skills to\\nevaluate your algorithms. This will mean that you end up losing money in the long run. It\\nis only when you boost your skills in coding, math, and domain knowledge that you can\\ntruly perform data science.\\nThe quality that was probably a surprise  for you was domain knowledge . It is really just\\nknowledge of the area you are working in. If a financial analyst started analyzing data\\nabout heart attacks, they might need the help of a cardiologist to make sense of a lot of the\\nnumbers.\\nData science  is the intersection of the three key areas mentioned earlier. In order to gain\\nknowledge from data, we must be able to utilize computer programming to access the data,\\nunderstand the mathematics behind the models we derive, and, above all, understand our\\nanalyses\\' place in the domain we are in. This includes the presentation of data. If we are\\ncreating a model to predict heart attacks in patients, is it better to create a PDF of\\ninformation, or an app where you can type in numbers and get a quick prediction? All\\nthese decisions must be made by the data scientist.\\nThe intersection of math and coding is machine learning. This book will\\nlook at machine learning in great detail later on, but it is important to note\\nthat without the explicit ability to generalize any models or results to a\\ndomain, machine learning algorithms remain just that —algorithms sitting\\non your computer. You might have the best algorithm to predict cancer.\\nYou could be able to predict cancer with over 99% accuracy based on past\\ncancer patient data, but if you don\\'t understand how to apply this model\\nin a practical sense so that doctors and nurses can easily use it, your\\nmodel might be useless.\\nBoth computer programming and math are covered extensively in this book. Domain\\nknowledge comes with both the practice of data science and reading examples of other\\npeople\\'s analyses.\\nThe math\\nMost people stop listening once  someone says the word \" math.\"  They\\'ll nod along in an\\nattempt to hide their utter disdain for the topic. This book will guide you through the math\\nneeded for data science, specifically statistics and probability. We will use these\\nsubdomains of mathematics to create what are called models .\\n\\n data model  refers to an organized and formal relationship between elements of data,\\nusually meant to simulate a real-world phenomenon.\\nEssentially, we will use math  in order to formalize relationships between variables. As a\\nformer pure mathematician and current math teacher, I know how difficult this can be. I\\nwill do my best to explain everything as clearly as I can. Between the three areas of data\\nscience, math is what allows us to move from domain to domain. Understanding the theory\\nallows us to apply a model that we built for the fashion industry to a financial domain.\\nThe math covered in this book ranges from basic algebra to advanced probabilistic and\\nstatistical modeling. Do not skip over these chapters, even if you already know these topics\\nor you\\'re afraid of them. Every mathematical concept that I will introduce will be\\nintroduced with care and purpose, using examples. The math in this book is essential for\\ndata scientists.\\nExample – spawner-recruit models\\nIn biology, we use, among many  other models, a model known as the spawner-recruit\\nmodel to judge the biological health of a species. It is a basic relationship between the\\nnumber of healthy parental units of a species and the number of new units in the group of\\nanimals. In a public dataset of the number of salmon spawners and recruits, the graph\\nfurther down (titled spawner-recruit model)  was formed to visualize the relationship\\nbetween the two. We can see that there definitely is some sort of positive relationship (as\\none goes up, so does the other). But how can we formalize this relationship? For example, if\\nwe knew the number of spawners in a population, could we predict the number of recruits\\nthat the group would obtain, and vice versa?\\nEssentially, models allow us to plug in one variable to get the other. Consider the following\\nexample :\\nIn this example, let\\'s say we knew that a group of salmon had 1.15 (in thousands) spawners.\\nThen, we would have the following:\\n (in thousands)\\n\\n\\nhis result can be very beneficial to estimate how the health of a population is changing. If\\nwe can create these models, we can visually observe how the relationship between the two\\nvariables can change.\\nThere are many types of data models, including probabilistic and statistical models. Both of\\nthese are subsets  of a larger paradigm, called machine learning . The essential idea behind\\nthese three topics is that we use data in order to come up with the best model possible. We\\nno longer rely on human instincts —rather, we rely on data, such as that displayed in the\\nfollowing graph:\\nThe spawner-recruit model visualized\\nThe purpose of this example is to show how we can define relationships between data\\nelements using mathematical equations. The fact that I used salmon health data was\\nirrelevant! Throughout this book, we will look at relationships involving marketing dollars,\\nsentiment data, restaurant reviews, and much more. The main reason for this is that I\\nwould like you (the reader) to be exposed to as many domains as possible.\\nMath and coding are vehicles that allow data scientists to step back and apply their skills\\nvirtually anywhere.\\n\\nomputer programming\\nLet\\'s be honest: you probably think  computer science is way cooler than math. That\\'s ok, I\\ndon\\'t blame you. The news isn\\'t filled with math news like it is with news on technology.\\nYou don\\'t turn on the TV to see a new theory on primes —rather, you will see investigative\\nreports on how the latest smartphone can take better  photos of cats, or something.\\nComputer languages are how we communicate with machines and tell them to do our\\nbidding. A computer speaks many languages and, like a book, can be written in many\\nlanguages; similarly, data science can also be done in many languages. Python,  Julia,  and R\\nare some of the many languages that are available to us. This book will focus exclusively on\\nusing Python.\\nWhy Python?\\nWe will use Python  for a variety  of reasons, listed as follows:\\nPython is an extremely simple language to read and write, even if you\\'ve never\\ncoded before, which will make future examples easy to understand and read\\nlater on, even after you have read this book.\\nIt is one of the most common languages, both in production and in the academic\\nsetting (one of the fastest growing, as a matter of fact).\\nThe language\\'s online community is vast and friendly. This means that a quick\\nsearch for the solution to a problem should yield many people who have faced\\nand solved similar (if not exactly the same) situations\\nPython has prebuilt data science modules that both the novice and the veteran\\ndata scientist can utilize.\\nThe last point is probably the biggest reason we will focus on Python. These prebuilt\\nmodules are not only powerful, but also easy to pick up. By the end of the first few\\nchapters, you will be very comfortable with these modules. Some of these modules include\\nthe following:\\npandas\\nscikit-learn\\nseaborn\\nnumpy/scipy\\nrequests  (to mine data from the web)\\nBeautifulSoup  (for web –HTML parsing)\\n\\nython practices\\nBefore we move on, it is important  to formalize many of the requisite coding skills in\\nPython.\\nIn Python, we have variables  that are placeholders for objects. We will focus on just a few\\ntypes of basic objects at first, as shown in the following table :\\nObject Type Example\\nint  (an integer) 3, 6, 99, -34, 34, 11111111\\nfloat  (a decimal) 3.14159, 2.71, -0.34567\\nboolean  (either  True  or False )• The statement \"Sunday is a weekend\" is True\\n• The statement \"Friday is a weekend\" is False\\n• The statement \"pi is exactly the ratio of a circle\\'s circumference to its\\ndiameter\" is True  (crazy, right?)\\nstring  (text or words made up of\\ncharacters)\"I love hamburgers\" ( by the way, who doesn\\'t? )\\n\"Matt is awesome\"\\nA tweet is a string\\nlist  (a collection of objects) [1, 5.4, True, \"apple\"]\\nWe will also have to understand some basic logistical operators. For these operators, keep\\nthe Boolean datatype in mind. Every operator will evaluate to either True  or False . Let\\'s\\ntake a look at the following operators :\\nOperators Example\\n==Evaluates to  True  if both sides are equal; otherwise, it evaluates to\\nFalse , as shown in the following examples:\\n• 3 + 4 == 7 (will evaluate to True )\\n• 3 - 2 == 7 (will evaluate to False )\\n< (less than)• 3 < 5 (True )\\n• 5 < 3 (False )\\n\\n= (less than or equal to)• 3 <= 3 (True )\\n• 5 <= 3 (False )\\n> (greater than)• 3 > 5 (Fals e)\\n• 5 > 3 (True )\\n>= (greater than or equal to)• 3 >= 3 (True )\\n• 5 >= 7 (False )\\nWhen coding in Python, I will use a pound sign ( #) to create a \"comment,\" which will not\\nbe processed as code, but is merely there to communicate with the reader. Anything to the\\nright of a  # sign is a comment on the code being executed.\\nExample of basic Python\\nIn Python, we use spaces/tabs to denote  operations that belong to other lines of code.\\nThe print True  statement belongs to the if x + y == 15.3:  line\\npreceding it because it is tabbed right under it. This means that the print\\nstatement will be executed if, and only if, x + y  equals 15.3.\\nNote that the following list variable, my_list , can hold multiple types of objects. This one\\nhas an int, a float , a boolean , and string  inputs (in that order):\\nmy_list = [1, 5.7, True, \"apples\"]\\nlen(my_list) == 4  # 4 objects in the list\\nmy_list[0] == 1    # the first object\\nmy_list[1] == 5.7    # the second object\\n\\nn the preceding code, I used the  len command to get the length of the list (which was 4).\\nAlso, n ote the zero-indexing of Python. Most computer languages start counting at zero\\ninstead of one. So if I want the first element, I call index 0, and if I want the 95th  element, I\\ncall index 94.\\nExample – parsing a single tweet\\nHere is some more Python code. In this example, I will be parsing some tweets about stock\\nprices (one of the important case studies in this book will be trying to predict market\\nmovements based  on popular sentiment regarding stocks on social media):\\ntweet = \"RT @j_o_n_dnger: $TWTR now top holding for Andor, unseating $AAPL\"\\nwords_in_tweet = tweet.split(\\' \\') # list of words in tweet\\nfor word in words_in_tweet:             # for each word in list\\n  if \"$\" in word:                       # if word has a \"cashtag\"\\n  print(\"THIS TWEET IS ABOUT\", word)  # alert the user\\nI will point out a few things about this code snippet line by line, as follows:\\nFirst, we set a variable to hold some text (known as a string in Python). In this\\nexample, the tweet in question is \"RT @robdv: $TWTR now top holding for\\nAndor, unseating $AAPL \".\\nThe words_in_tweet  variable tokenizes  the tweet (separates it by word).  If you\\nwere to print this variable, you would see the following:\\n[\\'RT\\',\\n\\'@robdv:\\',\\n\\'$TWTR\\',\\n\\'now\\',\\n\\'top\\',\\n\\'holding\\',\\n\\'for\\',\\n\\'Andor,\\',\\n\\'unseating\\',\\n\\'$AAPL\\']\\nWe iterate through this list of words; this is called a for loop. It just means that\\nwe go through a list one by one.\\n\\nere, we have another if statement. For each word in this tweet, if the word\\ncontains the $ character which represents stock tickers on Twitter.\\nIf the preceding if statement is True  (that is, if the tweet contains a cashtag),\\nprint it and show it to the user.\\nThe output of this code will be as follows:\\nTHIS TWEET IS ABOUT $TWTR\\nTHIS TWEET IS ABOUT $AAPL\\nWe get this output as these are the only words in the tweet that use the cashtag. Whenever I\\nuse Python in this book, I will ensure that I am as explicit as possible about what I am\\ndoing in each line of code.\\nDomain knowledge\\nAs I mentioned earlier, domain knowledge focuses  mainly on having knowledge of the\\nparticular topic you are working on. For example, if you are a financial analyst working on\\nstock market data, you have a lot of domain knowledge. If you are a journalist looking at\\nworldwide adoption rates, you might benefit from  consulting an expert in the field. This\\nbook will attempt to show examples from several problem domains, including medicine,\\nmarketing, finance, and even UFO sightings!\\nDoes this mean that if you\\'re not a doctor, you can\\'t work with medical data? Of course not!\\nGreat data scientists can apply their skills to any area, even if they aren\\'t fluent in it. Data\\nscientists can adapt to the field and contribute meaningfully when their analysis is\\ncomplete.\\nA big part of domain knowledge is a presentation. Depending on your audience, it can\\nmatter greatly on how you present your findings. Your results are only as good as your\\nvehicle of communication. You can predict the movement of the market with 99.99%\\naccuracy, but if your program is impossible to execute, your results will go unused.\\nLikewise, if your vehicle is inappropriate for the field, your results will go equally unused.\\n\\nome more terminology\\nThis is a good time to define some more vocabulary. By this point, you\\'re probably\\nexcitedly looking up a lot of data  science material and seeing words and phrases I haven\\'t\\nused yet. Here are some common terms that you are likely to encounter.\\nMachine learning : This refers to giving computers the ability to learn from data\\nwithout explicit \"rules\" being given by a programmer. We have seen the concept\\nof machine learning earlier in this chapter as the union of someone who has both\\ncoding and math skills. Here, we are attempting to formalize this definition.\\nMachine learning combines the power of computers with intelligent learning\\nalgorithms in order to automate the discovery of relationships in data and create\\npowerful data models. Speaking of data models, in this book, we will concern\\nourselves with the following two basic types of data model:\\nProbabilistic model : This refers to using  probability to find a\\nrelationship between elements that includes a degree of\\nrandomness.\\nStatistical model : This refers to taking  advantage of statistical\\ntheorems to formalize relationships between data elements in a\\n(usually) simple mathematical formula.\\nWhile both the statistical and probabilistic models\\ncan be run on computers and might be considered\\nmachine learning in that regard, we will keep these\\ndefinitions separate, since machine learning\\nalgorithms generally attempt to learn  relationships\\nin different ways.  We will take a look at the\\nstatistical and probabilistic models in later chapters.\\nExploratory data analysis  (EDA ): This refers to preparing data in order to\\nstandardize results and gain quick insights. EDA is concerned with data\\nvisualization and preparation. This is where we turn unorganized data into\\norganized data and clean up missing/incorrect data points. During EDA, we will\\ncreate many types of plots and use these plots to identify key features and\\nrelationships to exploit in our data models.\\nData mining:  This is the process of finding  relationships between elements of\\ndata. Data mining is the part of data science where we try to find relationships\\nbetween variables (think the spawn-recruit model).\\nI have tried pretty hard  not to use the term big data  up until now. This is because I think\\nthis term is misused, a lot. Big data is data that is too large to be processed by a single\\nmachine (if your laptop crashed, it might be suffering from a case of big data).\\n\\nhe following diagram shows the relationship between these data science concepts:\\nThe state of data science (so far)\\nThe preceding diagram is incomplete and is meant for visualization purposes only.\\nData science case studies\\nThe combination of math, computer programming, and domain knowledge is what makes\\ndata science so powerful. Oftentimes, it is difficult for a single person to master all three  of\\nthese areas. That\\'s why it\\'s very common for companies to hire teams of data scientists\\ninstead of a single person. Let\\'s look at a few powerful examples of data science in action\\nand their outcomes.\\n\\nase study – automating government paper\\npushing\\nSocial security claims are known  to be a major hassle for both the agent reading it and the\\nperson who wrote the claim. Some claims take over two years to get resolved in their\\nentirety, and that\\'s absurd! Let\\'s look at the following diagram, which shows what goes into\\na claim:\\nSample social security form\\n\\not bad. It\\'s mostly just text, though. Fill this in, then that, then this, and so on. You can see\\nhow it would be difficult for an agent to read these all day, form after form. There must be a\\nbetter way!\\nWell, there is. Elder Research Inc. parsed this unorganized data and was able to automate\\n20% of all disability social security forms. This means that a computer could look at 20% of\\nthese written forms and give its opinion on the approval.\\nNot only that —the third-party company that is hired to rate the approvals of the forms\\nactually gave the machine-graded forms a higher grade than the human forms. So not only\\ndid the computer handle 20% of the load on average, it also did better than a human.\\nFire all humans, right?\\nBefore I get a load of angry emails  claiming that data science is bringing about the end of\\nhuman workers, keep in mind that the computer was only  able to handle 20% of the load.\\nThis means that it probably performed terribly on 80% of the forms! This is because the\\ncomputer was probably great at simple forms . The claims that would have taken a human\\nminutes to compute took the computer seconds. But these minutes add up, and before you\\nknow it, each human is being saved over an hour a day!\\nForms that might be easy for a human to read are also likely easy for the computer. It\\'s\\nwhen the forms are very terse, or when the writer starts deviating from the usual grammar,\\nthat the computer starts to fail. This model is great because it lets the humans spend more\\ntime on those difficult claims and gives them more attention without getting distracted by\\nthe sheer volume of papers.\\nNote that I used the word \"model.\" Remember that a model is a\\nrelationship between elements. In this case, the relationship is between\\nwritten words and the approval status of a claim.\\n\\nase study – marketing dollars\\nA dataset shows the relationships between TV, radio, and newspaper sales. The goal is to\\nanalyze the relationships between the three different marketing mediums and how they \\naffect  the sale of a product. In this case, our data is displayed in the form of a table. Each\\nrow represents a sales region, and the columns tell us how much money was spent on each\\nmedium, as well as the profit that was gained in that region. F or example, from the\\nfollowing table, we can see that in the third region, we spent $17,200 on TV advertising and\\nsold 9,300 widgets:\\nUsually, the data scientist must ask for units and the scale. In this case, I\\nwill tell you that the TV, radio, and newspaper categories are measured in\\n\"thousands of dollars\" and the sales in \"thousands of widgets sold.\" This\\nmeans that in the first region, $230,100 was spent on TV advertising,\\n$37,800 on radio advertising, and $69,200 on newspaper advertising. In\\nthe same region, 22,100 items were sold.\\nAdvertising budgets\\' data\\nIf we plot each variable against the sales, we get the following graph:\\nimport pandas as pd\\nimport seaborn as sns\\n%matplotlib inline\\ndata = pd.read_csv(\\'http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\\',\\nindex_col=0)\\ndata.head()\\nsns.pairplot(data, x_vars=[\\'TV\\',\\'radio\\',\\'newspaper\\'], y_vars=\\'sales\\',\\nheight=4.5, aspect=0.7)\\n\\nResults – Graphs of advertising budgets\\nNote how none of these variables form a very strong line, and that therefore they might not\\nwork well in predicting sales on their own. TV comes closest in forming an obvious\\nrelationship, but even that isn\\'t great. In this case, we will have to create a more complex\\nmodel than the one we used in the spawner-recruiter model and combine all three variables\\nin order to model sales.\\nThis type of problem is very common in data science. In this example, we are attempting to\\nidentify key features that are associated with the sales of a product. If we can isolate these\\nkey features, then we can exploit these relationships and change how much we spend on\\nadvertising in different places with the hope of increasing our sales.\\n\\nase study – what\\'s in a job description?\\nLooking for a job in data  science? Great! Let me help. In this case study, I have \"scraped\"\\n(taken from the web) 1,000 job descriptions for companies that are actively hiring data\\nscientists. The goal here is to look at some of the most common keywords that people use in\\ntheir job descriptions, as shown in the following screenshot:\\nAn example of data scientist job listings.\\nNote the second one asking for core Python libraries; we will talk about\\nthese later on in this book.\\n\\nn the following Python code, the first two imports are used to grab web data from the\\nwebsite http:/ \\u200b/\\u200bindeed. \\u200bcom/ \\u200b, and the third import is meant to simply count the number of\\ntimes a word or phrase appears, as shown in the following code:\\nimport requests\\nfrom bs4 import BeautifulSoup\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n# grab postings from the web\\ntexts = []\\nfor i in range(0,1000,10): # cycle through 100 pages  of indeed job\\nresources\\n soup =\\nBeautifulSoup(requests.get(\\'http://www.indeed.com/jobs?q=data+scientist&sta\\nrt=\\'+str(i)).text)\\n texts += [a.text for a in soup.findAll(\\'span\\', {\\'class\\':\\'summary\\'})]\\nprint(type(texts))\\nprint(texts[0]) # first job description\\nOkay, before I lose you, all that this loop is doing is going through 100 pages  of job\\ndescriptions, and for each page, grabbing each job description. The important variable here\\nis texts , which is a list of over 1,000 job descriptions, as shown in the following code:\\ntype(texts) # == list\\nvect = CountVectorizer(ngram_range=(1,2), stop_words=\\'english\\')\\n# Get basic counts of one and two word phrases\\nmatrix = vect.fit_transform(texts)\\n# fit and learn to the vocabulary in the corpus\\nprint len(vect.get_feature_names())  # how many features are there\\n# There are 10,587 total one and two words phrases in my case!!\\nSince web pages are scraped in real-time and these pages may change since\\nthis code is run, you may get different number than 10587.\\n\\n have omitted some code here, but it exists in the GitHub repository for this book. The\\nresults are as follows (represented as the phrase and then the number of times it occurred ):\\n\\\\\\nThe following list shows some  things that we should mention:\\n\"Machine learning\" and \"experience\" are at the top of the list. Experience comes\\nwith practice. A basic idea of machine learning comes with this book.\\nThese words are followed closely by statistical words implying a knowledge of\\nmath and theory.\\nThe word \" team\"  is very high up, implying that you will need to work with a\\nteam of data scientists; you won\\'t be a lone wolf.\\nComputer science words such as \" algorithms\"  and \" programming\"  are prevalent.\\nThe words \" techniques\" , \"understanding\" , and \" methods\"  imply a more\\ntheoretical approach, unrelated to any single domain.\\nThe word \" business\"  implies a particular problem domain.\\n\\nhere are many interesting things to note about this case study, but the biggest take away is\\nthat there are many keywords and phrases that make up a data science role. It isn\\'t just\\nmath, coding, or domain knowledge; it truly is a combination of these three ideas (whether\\nexemplified in a single person or across a multiperson team) that makes data science\\npossible and powerful.\\nSummary\\nAt the beginning of this chapter, I posted a simple question: what\\'s the catch of data\\nscience? Well, there is one. It isn\\'t all fun, games, and modeling. There must be a price for\\nour quest to create ever-smarter machines and algorithms. As we seek new and innovative\\nways to discover data trends, a beast lurks in the shadows. I\\'m not talking about the\\nlearning curve of mathematics or programming, nor am I referring to the surplus of data.\\nThe Industrial Age left us with an ongoing battle against pollution. The subsequent\\nInformation Age left behind a trail of big data. So, what dangers might the Data Age bring\\nus?\\nThe Data Age can lead to something much more sinister —the dehumanization of the\\nindividual through mass data.\\nMore and more people are jumping head first into the field of data science, most with no\\nprior experience of math or CS, which, on the surface, is great. Average data scientists have\\naccess to millions of dating profiles\\' data, tweets, online reviews, and much more in order\\nto jump start their education.\\nHowever, if you jump into data science without the proper exposure to theory or coding\\npractices, and without respect for the domain you are working in, you face the risk of\\noversimplifying the very phenomenon you are trying to model.\\nFor example, let\\'s say you want to automate your sales pipeline by building a simplistic\\nprogram that looks at LinkedIn for very specific keywords in a person\\'s LinkedIn profile.\\nYou could use the following code to do this:\\nkeywords = [\"Saas\", \"Sales\", \"Enterprise\"]\\nGreat. Now you can scan LinkedIn quickly to find people who match your criteria. But\\nwhat about that person who spells out \"Software as a Service\", instead of \"SaaS,\" or\\nmisspells \"enterprise\" (it happens to the best of us; I bet someone will find a typo in my\\nbook). How will your model figure out that these people are also a good match? They\\nshould not be left behind just because the cut-corners data scientist has overgeneralized\\npeople in such an easy way.\\n\\nhe programmer chose to simplify their search for another human by looking for three\\nbasic keywords and ended up with a lot of missed opportunities left on the table.\\nIn the next chapter, we will explore the different types of data that exist in the world,\\nranging from free-form text to highly structured row/column files. We will also look at the\\nmathematical operations that are allowed for different types of data, as well as deduce\\ninsights based on the form of the data that comes in.\\n\\n\\nTypes of Data\\nNow that we have a basic introduction to the world of data science and understand why\\nthe field is so important, let\\'s take a look at the various ways in which data can be formed.\\nSpecifically, in this chapter, we will look at the following topics:\\nStructured versus unstructured data\\nQuantitative versus qualitative data\\nThe four levels of data\\nWe will dive further into each of these topics by showing examples of how data scientists\\nlook at and work with data. The aim of this chapter is to familiarize ourselves with the\\nfundamental ideas underpinning data science.\\nFlavors of data\\nIn the field, it is important to understand the different flavors of data for several reasons.\\nNot only will the type of data  dictate the methods used to analyze and extract results, but\\nknowing whether the data is unstructured, or perhaps quantitative, can also tell you a lot\\nabout the real-world phenomenon being measured.\\nThe first thing to note is my use of the word data. In the last chapter, I defined data as\\nmerely being a collection of information. This vague definition exists because we may\\nseparate data into different categories and need our definition to be loose.\\nThe next thing to remember while we go through this chapter is that for the most part,\\nwhen I talk about the type of data I will refer to either a specific characteristic  of a dataset or\\nto the entire dataset  as a whole. I will be very clear about which one I refer to at any given\\ntime.\\n\\nhy look at these distinctions?\\nIt might seem worthless to stop and think about what type of data we have before getting\\ninto the fun stuff, such as statistics and machine learning, but this is arguably one of the\\nmost important steps you need to take to perform data science.\\nThe same principle applies to data  science. When given a dataset, it is tempting to jump\\nright into exploring, applying statistical models, and researching the applications of\\nmachine learning in order to get results faster. However, if you don\\'t understand the type\\nof data that you are working with, then you might waste a lot of time applying models that\\nare known to be ineffective with that specific type of data.\\nWhen given a new dataset, I always recommend taking about an hour (usually less) to\\nmake the distinctions mentioned in the following sections.\\nStructured versus unstructured data\\nThe distinction between structured  and unstructured data  is usually the first question you\\nwant to ask yourself about the entire  dataset. The answer to this question can mean the\\ndifference between needing three days or three weeks of time to perform a proper analysis.\\nThe basic breakdown is as follows (this is a rehashed definition of organized and\\nunorganized data in the first chapter):\\nStructured (organized) data : This is data that can be thought of as observations\\nand characteristics. It is usually organized using a table method (rows and\\ncolumns).\\nUnstructured (unorganized) data : This data exists  as a free entity and does not\\nfollow any standard organization hierarchy.\\nHere are a few examples that could help you differentiate between the two:\\nMost data that exists in text form, including server logs and Facebook posts, is\\nunstructured\\nScientific observations, as recorded by careful scientists, are kept in a very neat\\nand organized ( structured ) format\\nA genetic sequence of chemical nucleotides (for example, ACGTATTGCA) is\\nunstructured,  even if the order of the nucleotides matters, as we cannot form\\ndescriptors of the sequence using a row/column format without taking a further\\nlook\\n\\ntructured data is generally thought of as being much easier to work with and analyze.\\nMost statistical and machine learning models were built with structured data in mind and\\ncannot work on the loose interpretation of unstructured data. The natural row and column\\nstructure is easy to digest for human and machine eyes. So, why even talk about\\nunstructured data? Because it is so common! Most estimates place unstructured data as\\n80-90% of the world\\'s data. This data exists in many forms and, for the most part, goes\\nunnoticed by humans as a potential source of data. Tweets, emails, literature, and server\\nlogs are generally unstructured forms of data.\\nWhile a data scientist likely prefers structured data, they must be able to deal with the\\nworld\\'s massive amounts of unstructured data. If 90% of the world\\'s data is unstructured,\\nthat implies that about 90% of the world\\'s information is trapped in a difficult format.\\nSo, with most of our data  existing in this free-form format, we must turn to pre-analysis\\ntechniques, called pre-processing , in order to apply structure to at least a part of the data\\nfor further analysis. The next chapter will deal with pre-processing in great detail; for now,\\nwe will consider the part of pre-processing wherein we attempt to apply transformations to\\nconvert unstructured data into a structured counterpart.\\nExample of data pre-processing\\nWhen looking at text data (which is almost always considered unstructured), we have\\nmany options to transform the set into a structured format. We may do this by applying\\nnew characteristics that describe the data. A few such characteristics are as follows:\\nWord/phrase count\\nThe existence of certain special characters\\nThe relative length of text\\nPicking out topics\\nI will use the following tweet as a quick example of unstructured data, but you may use\\nany unstructured free-form text that you like, including tweets and Facebook posts:\\n\"This Wednesday morn, are you early to rise? Then look East. The Crescent Moon joins Venus\\n& Saturn. Afloat in the dawn skies.\"\\nIt is important to reiterate that pre-processing is necessary for this tweet because a vast\\nmajority of learning algorithms require numerical data (which we will get into after this\\nexample).\\n\\nore than requiring a certain type of data, pre-processing allows us to explore features that\\nhave been created from the existing features. For example, we can extract features such as\\nword count and special characters from the mentioned tweet. Now, let\\'s take a look at a few\\nfeatures that we can extract from the text.\\nWord/phrase counts\\nWe may break down a tweet  into its word/phrase count. The word this appears in the tweet\\nonce, as does every other word. We can represent this tweet in a structured format as\\nfollows, thereby converting the unstructured set of words into a row/column format:\\nthis wednesday morn are you\\nWord count 1 1 1 1 1\\nNote that to obtain this format, we can utilize scikit-learn\\'s CountVectorizer , which we\\nsaw in the previous chapter.\\nPresence of certain special characters\\nWe may also look at the presence of special characters, such as the question mark and\\nexclamation mark. The appearance of these characters might imply certain ideas about the\\ndata that are otherwise difficult to know. For example, the fact that this tweet contains a\\nquestion mark might strongly imply that this tweet contains a question for the reader. We\\nmight append the preceding table with a new column, as shown:\\nthis wednesday morn are you ?\\nWord Count 1 1 1 1 1 1\\nThe relative length of text\\nThis tweet is 125 characters long:\\nlen(\"This Wednesday morn, are you early to rise? Then look East. The\\nCrescent Moon joins Venus & Saturn. Afloat in the dawn skies.\")\\n# get the length of this text (number of characters for a string)\\n# 125\\n\\nhe average tweet, as discovered by analysts, is about 30 characters in length. So, we might\\nimpose a new  characteristic, called relative length  (which is the length of the tweet divided\\nby the average length), telling us the length of this tweet as compared with the average\\ntweet. This tweet is actually 4.03 times longer than the average tweet, as shown:\\nWe can add yet another column to our table using this method:\\nthis wednesday morn are you ? Relative length\\nWord count 1 1 1 1 1 1 4.03\\nPicking out topics\\nWe can pick out some topics of the tweet  to add as columns. This tweet is about astronomy,\\nso we can add another column, as illustrated:\\nthis wednesday morn are you ? Relative length Topic\\nWord count 1 1 1 1 1 1 4.03 astronomy\\nAnd just like that, we can convert a piece of text into structured/organized data ready for\\nuse in our models and exploratory analysis.\\nThe topic is the only extracted feature we looked at that is not automatically derivable from\\nthe tweet. Looking at word count and tweet length in Python is easy. However, more\\nadvanced models (called topic models) are able to derive and predict topics of natural text\\nas well.\\nBeing able to quickly recognize whether your data is structured or unstructured can save\\nhours or even days of work in the future. Once you are able to discern the organization of\\nthe data presented to you, the next question is aimed at the individual characteristics of the\\ndataset.\\nQuantitative versus qualitative data\\nWhen you ask a data scientist, \"what type of data is this?\" , they will usually assume that you\\nare asking them whether or not it is mostly quantitative or qualitative. It is likely the most\\ncommon way of describing the specific  characteristics of a dataset.\\n\\nor the most part, when talking about quantitative data, you are usually  (not always)\\ntalking about a structured dataset with a strict row/column structure (because we don\\'t\\nassume unstructured data even has any characteristics). All the more reason why the pre-\\nprocessing step is so important.\\nThese two data types can be defined as follows:\\nQuantitative data : This data  can be described using numbers, and basic\\nmathematical procedures, including addition, are possible on the set.\\nQualitative data : This data  cannot be described using numbers and basic\\nmathematics. This data is generally thought of as being described using natural\\ncategories and language.\\nExample – coffee shop data\\nSay that we were processing  observations of coffee  shops in a major city using the\\nfollowing five descriptors (characteristics):\\nData: Coffee Shop\\nName of coffee shop\\nRevenue (in thousands of dollars)\\nZip code\\nAverage monthly customers\\nCountry of coffee origin\\nEach of these characteristics can be classified as either quantitative or qualitative, and that\\nsimple distinction can change everything. Let\\'s take a look at each one:\\nName of a coffee shop : Qualitative\\nThe name of a coffee shop is not expressed as a number and we cannot perform\\nmathematical operations on the name of the shop.\\nRevenue : Quantitative\\nHow much money a coffee shop brings in can definitely be described using a\\nnumber. Also, we can do basic operations, such as adding up the revenue for 12\\nmonths to get a year\\'s worth of revenue.\\n\\nip code : Qualitative\\nThis one is tricky. A zip code is always represented using numbers, but what\\nmakes it qualitative is that it does not fit the second part of the definition of\\nquantitative —we cannot perform basic mathematical operations on a zip code. If\\nwe add together two zip codes, it is a nonsensical measurement. We don\\'t\\nnecessarily get a new zip code and we definitely don\\'t get \"double the zip code.\"\\nAverage monthly customers : Quantitative\\nAgain, describing this factor using numbers and addition makes sense. Add up\\nall of your monthly customers and you get your yearly customers.\\nCountry of coffee origin : Qualitative\\nWe will assume this is a very small coffee shop with coffee from a single origin.\\nThis country is described using a name (Ethiopian, Colombian), and not\\nnumbers.\\nA couple of important things to note:\\nEven though a zip code is being described using numbers, it is not quantitative.\\nThis is because you can\\'t talk about the sum of all zip codes or an average  zip\\ncode. These are nonsensical descriptions.\\nPretty much whenever a word is used to describe a characteristic, it is a\\nqualitative factor.\\nIf you are having trouble identifying which is which, basically, when trying to decide\\nwhether or not the data is qualitative or quantitative, ask yourself a few basic questions\\nabout the data characteristics:\\nCan you describe it using numbers?\\nNo? It is qualitative\\nYes? Move on to the next question\\nDoes it still make sense after you add them together?\\nNo? They are qualitative\\nYes? You probably have quantitative  data\\nThis method will help you to classify most, if not all, data into one of these two categories.\\n\\nhe difference between these two categories defines the types of questions you may ask\\nabout each column. For a quantitative column, you may ask questions such as the\\nfollowing:\\nWhat is the average value?\\nDoes this quantity increase or decrease over time (if time is a factor)?\\nIs there a threshold where if this number became too high or too low, it would\\nsignal trouble for the company?\\nFor a qualitative column, none of the preceding questions can be answered. However, the\\nfollowing questions only apply to qualitative values:\\nWhich value occurs the most and the least?\\nHow many unique values are there?\\nWhat are these unique values?\\nExample – world alcohol consumption data\\nThe World Health Organization  (WHO ) released a dataset describing  the average drinking\\nhabits of people  in countries across  the world. We will use Python and the data exploration\\ntool, pandas, in order to gain a better look:\\nimport pandas as pd\\n# read in the CSV file from a URL\\ndrinks =\\npd.read_csv(\\'https://raw.githubusercontent.com/sinanuozdemir/principles_of_\\ndata_science/master/data/chapter_2/drinks.csv\\')\\n# examine the data\\'s first five rows\\ndrinks.head()           # print the first 5 rows\\nThese three lines have  done the following:\\nImported pandas , which will be referred to as pd in the future\\nRead in a comma separated value  (CSV ) file as a variable called drinks\\nCalled a method, head , that reveals the first five rows of the dataset\\nNote the neat row/column structure a CSV comes in.\\n\\nThe preceding table lists the first five rows of data from the drink.csv  file. We have six\\ndifferent columns that we are working within this example:\\ncountry : Qualitative\\nbeer_servings : Quantitative\\nspirit_servings : Quantitative\\nwine_servings : Quantitative\\ntotal_litres_of_pure_alcohol : Quantitative\\ncontinent : Qualitative\\nLet\\'s look at the qualitative column continent . We can use Pandas in order to get some\\nbasic summary statistics about this non-numerical characteristic. The describe()  method\\nis being used here, which first identifies whether the column is likely to be quantitative or\\nqualitative, and then gives basic information about the column as a whole. This is done as\\nfollows:\\ndrinks[\\'continent\\'].describe()\\n>> count     170\\n>> unique      5\\n>> top        AF\\n>> freq       53\\nIt reveals that the WHO has gathered data about five unique continents, the most frequent\\nbeing AF (Africa), which occurred 53 times in the 193 observations.\\n\\nf we take a look at one of the quantitative columns and call the same method, we can see\\nthe difference in output, as shown:\\ndrinks[\\'beer_servings\\'].describe()\\nThe output is as follows:\\ncount    193.000000\\nmean     106.160622\\nstd      101.143103\\nmin        0.000000\\n25%       20.000000\\n50%       76.000000\\n75%      188.000000\\nmax      376.000000\\nNow, we can look at the mean (average) beer serving per person per country (106.2\\nservings), as well as the lowest beer serving, zero, and the highest beer serving recorded,\\n376 (that\\'s more than a beer a day).\\nDigging deeper\\nQuantitative data can be broken down  one step further into discrete  and continuous\\nquantities.\\nThese can be defined as follows:\\nDiscrete data : This describes data  that is counted. It can only take on certain\\nvalues.\\nExamples of discrete quantitative data include a dice roll, because it can only take\\non six values, and the number of customers in a coffee shop, because you can\\'t\\nhave a real range of people.\\nContinuous data : This describes data  that is measured. It exists on an infinite\\nrange of values.\\nA good example of continuous data would be a person\\'s weight, because it can\\nbe 150 pounds or 197.66 pounds (note the decimals). The height of a person or\\nbuilding is a continuous number because an infinite scale of decimals is possible.\\nOther examples of continuous data would be time and temperature.\\n\\nhe road thus far\\nSo far in this chapter, we have looked  at the differences between structured and\\nunstructured data, as well as between qualitative and quantitative characteristics. These\\ntwo simple distinctions can have drastic effects on the analysis that is performed. Allow me\\nto summarize before moving on the second half of the chapter.\\nData as a whole can either be structured  or unstructured , meaning that the data can either\\ntake on an organized row/column structure with distinct features that describe each row of\\nthe dataset, or exist in a free-form state that usually must be pre-processed into a form that\\nis easily digestible.\\nIf data is structured, we can look at each column (feature) of the dataset as being either\\nquantitative  or qualitative . Basically, can the column be described using mathematics and\\nnumbers or not? The next part of this chapter will break down data into four very specific\\nand detailed levels. At each order, we will apply more complicated rules of mathematics,\\nand in turn, we can gain a more intuitive and quantifiable understanding of the data.\\nThe four levels of data\\nIt is generally understood that a specific characteristic (feature/column) of structured data\\ncan be broken down into one of four levels of data. The levels are as follows:\\nThe nominal level\\nThe ordinal level\\nThe interval level\\nThe ratio level\\nAs we move down the list, we gain more structure and, therefore, more returns from our\\nanalysis. Each level comes with its own accepted practice in measuring the center  of the\\ndata. We usually think of the mean/average as being an acceptable form of a center.\\nHowever, this is only true for a specific type of data.\\n\\nhe nominal level\\nThe first level of data, the nominal  level, consists of data  that is described purely by name\\nor category. Basic examples include gender, nationality, species, or yeast strain  in a beer.\\nThey are not described by numbers and are therefore qualitative. The following are some\\nexamples:\\nA type of animal is on the nominal level of data. We may also say that if you are\\na chimpanzee, then you belong to the mammalian class as well.\\nA part of speech is also considered on the nominal level of data. The word she is a\\npronoun, and it is also a noun .\\nOf course, being qualitative, we cannot perform any quantitative mathematical operations,\\nsuch as addition or division. These would not make any sense.\\nMathematical operations allowed\\nWe cannot perform mathematics on the nominal  level of data except the basic equality  and\\nset membership  functions, as shown in the following two examples:\\nBeing a tech entrepreneur  is the same as being in the tech industry , but not the other\\nway around\\nA figure described as a square falls under the description of being a rectangle,\\nbut not the other way around\\nMeasures of center\\nA measure of center  is a number that describes what the data tends to . It is sometimes\\nreferred to as the balance point  of the data. Common examples include the mean, median,\\nand mode.\\nIn order to find the center  of nominal data, we generally turn to the mode  (the most\\ncommon element) of the dataset. For example, look back at the WHO alcohol consumption\\ndata. The most common  continent surveyed was Africa, making that a possible choice for\\nthe center of the continent column.\\nMeasures of the center, such as the mean and median, do not make sense at this level as we\\ncannot order the observations or even add them together.\\n\\nhat data is like at the nominal level\\nData at the nominal level is mostly  categorical in nature. Because we generally can only use\\nwords to describe the data, it can be lost in translation between countries, or can even be\\nmisspelled.\\nWhile data at this level can certainly be useful, we must be careful about what insights we\\nmay draw from them. With only the mode as a basic measure of center, we are unable to \\ndraw  conclusions about an average  observation. This concept does not exist at this level. It\\nis only at the next level that we may begin to perform true mathematics on our\\nobservations.\\nThe ordinal level\\nThe nominal level did not provide us with much flexibility in terms of mathematical\\noperations due to one seemingly unimportant fact: we could not order the observations in\\nany natural way. Data in the ordinal  level provides us with a rank order, or the means to\\nplace one observation before the other. However, it does not provide us with relative\\ndifferences between observations, meaning that while we may order the observations from\\nfirst to last, we cannot add or subtract them to get any real meaning.\\nExamples\\nThe Likert  is among the most common  ordinal level scales. Whenever you are given a \\nsurvey  asking you to rate your satisfaction on a scale from 1 to 10, you are providing data at\\nthe ordinal level. Your answer, which must fall between 1 and 10, can be ordered: eight is\\nbetter than seven while three is worse than nine.\\nHowever, differences between the numbers do not make much sense. The difference\\nbetween a seven and a six might be different from the difference between a two and a one.\\nMathematical operations allowed\\nWe are allowed much more freedom  on this level in mathematical operations. We inherit all\\nmathematics from the ordinal level (equality and set membership) and we can also add the\\nfollowing to the list of operations allowed in the nominal level:\\nOrdering\\nComparison\\n\\nrdering refers to the natural order provided to us by the data. However, this can be tricky\\nto figure out sometimes. When speaking about the spectrum of visible light, we can refer to\\nthe names of colors —Red, Orange , Yellow , Green , Blue , Indigo , and Violet . Naturally, as\\nwe move from left to right, the light is gaining energy and other properties. We may refer to\\nthis as a natural order:\\nThe natural order of color\\nHowever, if needed, an artist may impose another order on the data, such as sorting the\\ncolors based on the cost of the material to make said color. This could change the order of\\nthe data, but as long as we are consistent in what defines the order, it does not matter what\\ndefines it.\\nComparisons are another new operation allowed at this level. At the ordinal level, it would\\nnot make sense to say that one country was naturally  better than another or that one part of\\nspeech is worse than another. At the ordinal level, we can make these comparisons. For\\nexample, we can talk about how putting a \"7\" on a survey is worse than putting a \"10.\"\\nMeasures of center\\nAt the ordinal level, the median  is usually an appropriate  way of defining the center of the\\ndata. The mean, however, would be impossible because the division is not allowed  at this\\nlevel. We can also use the mode as we could at the nominal level.\\nWe will now look at an example of using the median.\\nImagine you have conducted a survey among your employees asking \" how happy are you to\\nbe working here on a scale from 1-5 ?,\" and your results are as follows:\\n5, 4, 3, 4, 5, 3, 2, 5, 3, 2, 1, 4, 5, 3, 4, 4, 5, 4, 2, 1, 4, 5, 4, 3, 2,\\n4, 4, 5, 4, 3, 2, 1\\n\\net\\'s use Python to find the median of this data. It is worth noting that most people would\\nargue that the mean of these scores would work just fine. The reason that the mean would\\nnot be as mathematically viable is that if we subtract/add two scores, say a score of four\\nminus a score of two, the difference of two does not really mean anything. If\\naddition/subtraction among the scores doesn\\'t make sense, the mean won\\'t make sense\\neither:\\nimport numpy\\nresults = [5, 4, 3, 4, 5, 3, 2, 5, 3, 2, 1, 4, 5, 3, 4, 4, 5, 4, 2, 1, 4,\\n5, 4, 3, 2, 4, 4, 5, 4, 3, 2, 1]\\nsorted_results = sorted(results)\\nprint(sorted_results)\\n\\'\\'\\'\\n[1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\\n5, 5, 5, 5, 5, 5, 5]\\n\\'\\'\\'\\nprint(numpy.mean(results)) # == 3.4375\\nprint(numpy.median(results)) # == 4.0\\nThe \\'\\'\\' (triple apostrophe) denotes a longer (over two lines) comment. It\\nacts in a way similar to #.\\nIt turns out that the median is not only more sound but makes the survey results look much\\nbetter.\\nQuick recap and check\\nSo far, we have seen half of the levels of data:\\nThe nominal level\\nThe ordinal level\\n\\nt the nominal level, we deal with data usually described using vocabulary (but sometimes\\nwith numbers), with no order and little use of mathematics.\\nAt the ordinal level, we have data that can be described with numbers and we also have a\\n\"natural\" order, allowing us to put one in front of the other.\\nLet\\'s try to classify the following example as either ordinal or nominal ( answers are at the\\nend of the chapter ):\\nThe origin of the beans in your cup of coffee\\nThe place someone receives after completing a foot race\\nThe metal used to make the medal that they receive after placing in said race\\nThe telephone number of a client\\nHow many cups of coffee you drink in a day\\nThe interval level\\nNow, we are getting somewhere interesting. At the interval  level, we are beginning to look\\nat data that can be expressed through very quantifiable means, and where much more\\ncomplicated mathematical  formulas are allowed. The basic difference between the ordinal\\nlevel and the interval level is, well, just that difference.\\nData at the interval level allows meaningful subtraction between data points.\\nExample\\nTemperature is a great example of data  at the interval level. If it is 100 degrees Fahrenheit in\\nTexas and 80 degrees Fahrenheit in Istanbul, Turkey, then Texas is 20 degrees warmer than\\nIstanbul. This simple example allows for so much more manipulation at this level than\\nprevious examples.\\n(Non) Example\\nIt seems as though the example in the ordinal level (using the one to five survey) fits the bill\\nof the interval level. However, remember that the difference  between the scores (when you\\nsubtract them) does not make sense; therefore, this data cannot be called at the interval\\nlevel.\\n\\nathematical operations allowed\\nWe can use all the operations allowed  on the lower levels (ordering, comparisons, and so\\non), along with two other notable operations:\\nAddition\\nSubtraction\\nThe allowance of these two operations allows us to talk about data at this level in a whole\\nnew way.\\nMeasures of center\\nAt this level, we can use the median  and mode to describe this data. However, usually the\\nmost accurate description of the center of data would be the arithmetic mean , more\\ncommonly referred to as simply the mean . Recall that the definition of the mean  requires us\\nto add together all the measurements. At the previous levels, an addition was meaningless.\\nTherefore, the mean would have lost extreme value. It is only at the interval level and\\nabove that the arithmetic mean makes sense.\\nWe will now look at an example of using the mean.\\nSuppose we look at the temperature of a fridge containing a pharmaceutical company\\'s\\nnew vaccine. We measure the temperature every hour with the following data points (in\\nFahrenheit):\\n31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26\\nUsing Python again, let\\'s find the mean and median of the data:\\nimport numpy\\ntemps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\\nprint(numpy.mean(temps))    # == 30.73\\nprint(numpy.median(temps))  # == 31.0\\n\\note how the mean and median are quite close to each other and both are around 31\\ndegrees. The question, on average, how cold is the fridge?,  has an answer of about 31.\\nHowever, the vaccine comes with a warning:\\n\"Do not keep this vaccine at a temperature under 29 degrees.\"\\nNote that at least twice the temperature dropped below 29 degrees, but you ended up\\nassuming that it isn\\'t enough for it to be detrimental.\\nThis is where the measure of variation can help us understand how bad the fridge situation\\ncan be.\\nMeasures of variation\\nThis is something new that we have  not yet discussed. It is one thing to talk about the\\ncenter of the data but, in data science, it is also very important to mention how \"spread out\"\\nthe data is. The measures that describe this phenomenon are called measures of variation .\\nYou have  likely heard of \"standard deviation\" from your statistics classes. This idea is\\nextremely important and I would like to address it briefly.\\nA measure of variation (such as the standard deviation) is a number that attempts to\\ndescribe how spread out the data is.\\nAlong with a measure of center, a measure of variation can almost entirely describe a\\ndataset with only two numbers.\\nStandard deviation\\nArguably, the standard deviation is the most common measure  of variation of data at the\\ninterval level and beyond. The standard deviation can be thought of as the \"average\\ndistance a data point is at from the mean.\" While this description is technically and\\nmathematically incorrect, it is a good way to think about it. The formula for standard\\ndeviation can be broken down into the following steps:\\nFind the mean of the data1.\\nFor each number in the dataset, subtract it from the mean and then square it2.\\nFind the average of each square difference3.\\nTake the square root of the number obtained in Step 3  and this is the standard 4.\\ndeviation\\nNotice how, in the steps, we do actually take an arithmetic mean as one of the steps.\\n\\nor example, look back at the temperature dataset. Let\\'s find the standard deviation of the\\ndataset using Python:\\nimport numpy\\ntemps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\\nmean = numpy.mean(temps)    # == 30.73\\nsquared_differences = []\\n# empty list o squared differences\\nfor temperature in temps:\\n    difference = temperature - mean\\n # how far is the point from the mean\\n    squared_difference = difference**2\\n    # square the difference\\n    squared_differences.append(squared_difference)\\n    # add it to our list\\naverage_squared_difference = numpy.mean(squared_differences)\\n# This number is also called the \"Variance\"\\nstandard_deviation = numpy.sqrt(average_squared_difference)\\n# We did it!\\nprint(standard_deviation)  # == 2.5157\\nAll of this code led to us find out that the standard deviation of the dataset is around 2.5,\\nmeaning that \"on average,\" a data point is 2.5 degrees off from the average temperature of\\naround 31 degrees, meaning that the temperature could likely dip below 29 degrees again\\nin the near future.\\nThe reason we want the \"square difference\" between each point and the\\nmean and not the \"actual difference\" is because squaring the value\\nactually puts emphasis on outliers —data points that are abnormally far\\naway.\\nMeasures of variation give us a very clear picture of how spread out or dispersed our data\\nis. This is especially important when we are concerned with ranges of data and how data\\ncan fluctuate (think percentage return on stocks).\\n\\nhe big difference between data at this level and at the next level lies in something that is\\nnot obvious.\\nData at the interval level does not have a \"natural starting point or a natural zero.\"\\nHowever, being at zero degrees Celsius does not mean that you have \"no temperature\".\\nThe ratio level\\nFinally, we will take a look  at the ratio level. After moving through three different levels\\nwith differing levels of allowed mathematical operations, the ratio level proves to be the\\nstrongest of the four.\\nNot only can we define order  and difference, but the ratio level also allows us to multiply\\nand divide  as well. This might seem like not much to make a fuss over but it changes almost\\neverything about the way we view data at this level.\\nExamples\\nWhile Fahrenheit and Celsius are stuck  in the interval level, the Kelvin scale of temperature\\nboasts a natural zero. A measurement of zero Kelvin literally means the absence of heat. It\\nis a non-arbitrary starting zero. We can actually scientifically say that 200 Kelvin is twice as\\nmuch heat as 100 Kelvin.\\nMoney in the bank is at the ratio level. You can have \"no money in the bank\" and it makes\\nsense that $200,000 is \"twice as much as\" $100,000.\\nMany people may argue that Celsius and Fahrenheit also have a starting\\npoint (mainly because we can convert from Kelvin to either of the two).\\nThe real difference here might seem silly, but because the conversion to\\nCelsius and Fahrenheit make the calculations go into the negative, it does\\nnot define a clear and \"natural\" zero.\\nMeasures of center\\nThe arithmetic mean still holds meaning  at this level, as does a new type of mean called the\\ngeometric mean . This measure is generally not used as much, even at the ratio  level, but is\\nworth mentioning. It is the square root of the product of all the values.\\n\\nor example, in our fridge temperature data, we can calculate the geometric mean as shown\\nhere:\\nimport numpy\\ntemps = [31, 32, 32, 31, 28, 29, 31, 38, 32, 31, 30, 29, 30, 31, 26]\\nnum_items = len(temps)\\nproduct = 1.\\nfor temperature in temps:\\n    product *= temperature\\ngeometric_mean = product**(1./num_items)\\nprint(geometric_mean)   # == 30.634\\nNote again how it is close to the arithmetic mean and median as calculated before. This is\\nnot always the case and will be talked about at great length in the statistics chapter of this\\nbook.\\nProblems with the ratio level\\nEven with all of this added  functionality at this level, we must generally also make a very\\nlarge assumption that actually makes the ratio level a bit restrictive.\\nData at the ratio level is usually non-negative.\\nFor this reason alone, many data scientists prefer the interval level to the ratio level. The\\nreason for this restrictive property is because if we allowed negative values, the ratio might\\nnot always make sense.\\nConsider that we allowed debt to occur in our money in the bank example. If we had a\\nbalance of $50,000, the following ratio would not really make sense at all:\\n\\n\\nata is in the eye of the beholder\\nIt is possible to impose structure  on data. For example, while I said that you technically\\ncannot use a mean for the one to five data at the ordinal scale, many statisticians would not\\nhave a problem using this number as a descriptor of the dataset.\\nThe level at which you are interpreting data is a huge  assumption that should be made at\\nthe beginning of any analysis. If you are looking at data that is generally thought of at the\\nordinal level and applying tools such as the arithmetic mean and standard deviation, this is\\nsomething that data scientists must be aware of. This is mainly because if you continue to\\nhold these assumptions as valid in your analysis, you may encounter problems. For\\nexample, if you also assume divisibility at the ordinal level by mistake, you are imposing a\\nstructure where the structure may not exist.\\nSummary\\nThe type of data that you are working with is a very large piece of data science. It must\\nprecede most of your analysis because the type of data you have impacts the type of\\nanalysis that is even possible!\\nWhenever you are faced with a new dataset, the first three questions you should ask about\\nit are the following:\\nIs the data organized or unorganized?  For example, does our data exist in a nice,\\nclean row/column structure?\\nIs each column quantitative or qualitative?  For example, are the values\\nnumbers, strings, or do they represent quantities?\\nAt what level is the data in each column?  For example, are the values at the\\nnominal, ordinal, interval, or ratio level?\\nThe answers to these questions will not only impact your knowledge of the data at the end\\nbut will also dictate the next steps of your analysis. They will dictate the types of graphs\\nyou are able to use and how you interpret them in your upcoming data models. Sometimes,\\nwe will have to convert from one level to another in order to gain more perspective. In the\\ncoming chapters, we will take a much deeper look at how to deal with and explore data at\\ndifferent levels.\\n\\ny the end of this book, we will be able to not only recognize data at different levels, but\\nwill also know how to deal with it at these levels. In the next chapter, we will review how\\ntypes of data are used by data scientists to do data discovery and visualization. \\nAnswers for classification of the following example as either ordinal or nominal:\\nThe origin of the beans in your cup of coffee : Nominal\\nThe place someone receives after completing a foot race : Ordinal\\nThe metal used to make the medal that they receive after placing in the race :\\nNominal\\nThe telephone number of a client : Nominal\\nHow many cups of coffee you drink in a day : Ordinal\\n\\n\\nThe Five Steps of Data Science\\nWe have spent much time looking at the preliminaries of data science, including outlining\\nthe types of data and how to approach datasets depending on their type.\\nIn this chapter, in addition to the introduction of data science, we will focus on the\\nfollowing topics:\\nSteps to perform data science\\nData exploration\\nData visualization\\nWe will use the Python packages pandas  and matplotlib  to explore different datasets.\\nIntroduction to data science\\nMany people ask me the biggest difference between data science and data analytics. While\\nsome can argue  that there is no difference between the two, many will argue that there are\\nhundreds! I believe that regardless of how many differences there are between the two\\nterms, the biggest is that data science follows a structured, step-by-step process that, when\\nfollowed, preserves the integrity of the results .\\nLike any other scientific endeavor, this process must be adhered to, or else the analysis and\\nthe results are in danger of scrutiny. On a simpler level, following a strict process can make\\nit much easier for amateur data scientists to obtain results faster than if they were exploring\\ndata with no clear vision.\\nWhile these steps are a guiding lesson for amateur analysts, they also provide the\\nfoundation for all data scientists, even those in the highest levels of business and academia.\\nEvery data scientist recognizes the value of these steps and follows them in some way or\\nanother.\\n\\nverview of the five steps\\nThe five essential steps to perform data  science are as follows:\\nAsking an interesting question1.\\nObtaining the data2.\\nExploring the data3.\\nModeling the data4.\\nCommunicating and visualizing the results5.\\nFirst, let\\'s look at the five steps with reference to the big picture.\\nAsking an interesting question\\nThis is probably my favorite step. As an entrepreneur, I ask myself (and others) interesting\\nquestions every day. I would treat this step as you would treat a brainstorming session.\\nStart writing down questions regardless of whether or not you think the data  to answer\\nthese questions even exists. The reason for this is twofold. First off, you don\\'t want to start\\nbiasing yourself even before searching for data. Secondly, obtaining data might involve\\nsearching in both public and private locations and, therefore, might not be very\\nstraightforward. You might ask a question and immediately tell yourself \"Oh, but I bet\\nthere\\'s no data out there that can help me\" and cross it off your list. Don\\'t do that! Leave it\\non your list.\\nObtaining the data\\nOnce you have selected the question  you want to focus on, it is time to scour the world for\\nthe data that might be able to answer that question. As mentioned before, the data can\\ncome from a variety of sources; so, this step can be very creative!\\nExploring the data\\nOnce we have the data, we use the lessons  learned in Chapter 2 , Types of Data , and begin to\\nbreak down the types of data that we are dealing with. This is a pivotal step in the process.\\nOnce this step is completed, the analyst has generally  spent several hours learning about\\nthe domain, using code or other tools to manipulate and explore the data, and has a very\\ngood sense of what the data might be trying to tell them.\\n\\nodeling the data\\nThis step involves the use of statistical  and machine learning models. In this step, we are\\nnot only fitting and choosing models, but we are also implanting mathematical validation\\nmetrics in order to quantify the models and their effectiveness.\\nCommunicating and visualizing the results\\nThis is arguably the most important step. While it might seem obvious and simple, the\\nability to conclude your  results in a digestible format is much more difficult than it seems.\\nWe will look at different  examples of cases when results were communicated poorly and\\nwhen they were displayed very well.\\nIn this book, we will focus mainly on steps 3, 4, and 5.\\nWhy are we skipping steps 1 and 2 in this book?\\nWhile the first two steps are undoubtedly imperative to the process, they\\ngenerally precede statistical and programmatic systems. Later in this\\nbook, we will touch upon the different ways to obtain data; however, for\\nthe purpose of focusing on the more scientific aspects of the process, we\\nwill begin with exploration right away.\\nExploring the data\\nThe process of exploring data is not simply defined. It involves the ability to recognize the\\ndifferent types of data, transform data types, and use code to systemically improve the \\nquality  of the entire dataset to prepare it for the modeling stage. In order to best represent\\nand teach the art of exploration, I will present several different datasets and use the Python\\npackage pandas  to explore the data. Along the way, we will run into different tips and\\ntricks on how to handle data.\\nThere are three basic questions we should ask ourselves when dealing with a new dataset\\nthat we have not seen before. Keep in mind that these questions are not the beginning and\\nthe end of data science; they are some guidelines that should be followed when exploring a\\nnewly obtained set of data.\\n\\nasic questions for data exploration\\nWhen looking at a new dataset, whether it is familiar to you or not, it is important to use\\nthe following questions  as guidelines for your preliminary analysis:\\nIs the data organized or not? : We are checking for whether or not the data is\\npresented in a row/column structure. For the most part, data will be presented in\\nan organized fashion. In this book, over 90% of our examples will begin with\\norganized data. Nevertheless, this is the most basic question that we can answer\\nbefore diving any deeper into our analysis. A general rule of thumb is that if we\\nhave unorganized data, we want to transform it into a row/column structure. For\\nexample, earlier in this book, we looked at ways to transform text into a\\nrow/column structure by counting the number of words/phrases.\\nWhat does each row represent? : Once we have an answer to how the data is\\norganized and are looking at a nice row/column-based dataset, we should\\nidentify what each row actually represents. This step is usually very quick and\\ncan help put things into perspective much more quickly.\\nWhat does each column represent? : We should identify each column by the level\\nof data and whether or not it is quantitative/qualitative, and so on. This\\ncategorization might change as our analysis progresses, but it is important to\\nbegin this step as early as possible.\\nAre there any missing data points? : Data isn\\'t perfect. Sometimes, we might be\\nmissing data because of human or mechanical error. When this happens, we, as\\ndata scientists, must make decisions about how to deal with these discrepancies.\\nDo we need to perform any transformations on the columns? : Depending on\\nthe level/type of data in each column, we might need to perform certain types of\\ntransformation. For example, generally speaking, for the sake of statistical\\nmodeling and machine learning, we would like each column to be numerical, so\\nwould use Python to make any transformations.\\nAll the while, we are asking ourselves the overall question, what can we infer from the\\npreliminary inferential statistics?  We want to be able to understand our data better than when\\nwe first found it.\\nEnough talk, let\\'s see an example in the following section.\\n\\nataset 1 – Yelp\\nThe first dataset we will look at is a public  dataset made available by the restaurant review\\nsite, Yelp. All personally identifiable information has been removed. Let\\'s read  in the data\\nfirst, as shown here:\\nimport pandas as pd\\nyelp_raw_data = pd.read_csv(\"yelp.csv\")\\nyelp_raw_data.head()\\nA quick recap of what the preceding code does:\\nImports the pandas  package and nickname it pd 1.\\nReads in the .csv  from the web; call is yelp_raw_data 2.\\nLooks at the head  of the data (just the first few rows) 3.\\nWe get the following:\\nIs the data organized or not? :\\nBecause we have a nice row/column structure, we can conclude that this data\\nseems pretty organized.\\n\\nhat does each row represent? :\\nIt seems pretty obvious that each row represents a user giving a review of a\\nbusiness. The next thing we should do is examine each row and label it by the\\ntype of data it contains. At this point, we can also use Python to figure out just\\nhow big our dataset is. We can use the shape  quality of a DataFrame to find this\\nout, as shown:\\nyelp_raw_data.shape\\n# (10000,10)\\nIt tells us that this dataset has 10000  rows and 10 columns. Another way to say\\nthis is that this dataset has 10,000 observations and 10 characteristics.\\nWhat does each column represent? (Note that we have 10 columns) :\\nbusiness_id : This is likely to be a unique identifier for the business the review\\nis for. This would  be at the nominal level  because there is no natural order to this\\nidentifier.\\ndate : This is probably the date on which the review was posted. Note that it\\nseems to be only specific to the day, month, and year. Even though time is \\nusually  considered continuous, this column would likely be considered discrete\\nand at the ordinal level  because of the natural order that dates have.\\nreview_id : This is likely to be a unique identifier for the review that each post\\nrepresents. This would be at the nominal level  because, again, there is no natural\\norder to this identifier.\\nstars : From a quick look (don\\'t worry; we will perform some further analysis\\nsoon), we can see that this is an ordered column that represents what the\\nreviewer gave the restaurant as a final score. This is ordered and qualitative, so is\\nat the ordinal level .\\ntext : This is probably the raw text that each reviewer wrote. As with most text,\\nwe place this at the nominal level .\\ntype : In the first five columns, all we see is the word review . This might be a\\ncolumn that identifies that each row is a review, implying that there might be\\nanother type of row other than a review. We will take a look at this later. We\\nplace this at the nominal level .\\nuser_id : This is likely to be a unique identifier for the user who is writing the\\nreview. Just like the other unique IDs, we place this data at the nominal level .\\n\\note that after we have looked at all of the columns, and found that all of\\nthe data is either at the ordinal or nominal level, we have to look at the\\nfollowing things. This is not uncommon, but it is worth mentioning.\\nAre there any missing data points? :\\nPerform an isnull  operation. For example, if your DataFrame is called\\nawesome_dataframe , then try the Python command\\nawesome_dataframe.isnull().sum() , which will show the number of\\nmissing values in each column.\\nDo we need to perform any transformations on the columns? :\\nAt this point, we are looking for a few things. For example, will we need to\\nchange the scale of some of the quantitative data, or do we need to create dummy\\nvariables for the qualitative variables? As this dataset only has qualitative\\ncolumns, we can only focus on transformations at the ordinal and nominal scale.\\nBefore starting, let\\'s go over some quick terminology for pandas, the Python data\\nexploration module.\\nDataFrames\\nWhen we read in a dataset, pandas creates a custom  object called a DataFrame . Think of\\nthis as the Python version of a spreadsheet (but way better). In this case, the variable,\\nyelp_raw_data , is a DataFrame.\\nTo check whether this is true in Python, type in the following code:\\ntype(yelp_raw_data)\\n# pandas.core.frame.DataFrame\\nDataFrames are two-dimensional in nature, meaning that they are organized in a\\nrow/column structure just as spreadsheets are. The main benefits of using DataFrames over,\\nsay, spreadsheet software would be that a DataFrame can handle much larger data than\\nmost common spreadsheet software. If you are familiar with the R language, you might\\nrecognize the word DataFrame. This is because the name was actually borrowed from the\\nlanguage!\\nAs most of the data that we will deal with is organized, DataFrames are likely to be the\\nmost used object in pandas, second only to the Series  object.\\n\\neries\\nThe Series  object is simply a DataFrame, but only with one dimension. Essentially, it is a\\nlist of data  points. Each column of a DataFrame is considered to be a Series  object. Let\\'s\\ncheck this —the first thing we need to do is grab a single column from our DataFrame; we\\ngenerally use what is known as bracket notation . The following is an example:\\nyelp_raw_data[\\'business_id\\'] # grabs a single column of the Dataframe\\nWe will list the first and last few rows:\\n0     9yKzy9PApeiPPOUJEtnvkg\\n1     ZRJwVLyzEJq1VAihDhYiow\\n2     6oRAC4uyJCsJl1X0WZpVSA\\n3     _1QQZuf4zZOyFCvXc0o6Vg\\n4     6ozycU1RpktNG2-1BroVtw\\n5     -yxfBYGB6SEqszmxJxd97A\\n6     zp713qNhx8d9KCJJnrw1xA\\nLet\\'s use the type function to check that this column is a Series :\\ntype(yelp_raw_data[\\'business_id\\'])\\n# pandas.core.series.Series\\nExploration tips for qualitative data\\nUsing these two pandas objects, let\\'s start performing  some preliminary data exploration.\\nFor qualitative  data, we will specifically look at the nominal and ordinal levels.\\nNominal level columns\\nAs we are at the nominal level, let\\'s recall that at this level, data is qualitative and is\\ndescribed purely by name. In this dataset, this refers to business_id , review_id , text ,\\ntype , and user_id . Let\\'s use pandas in order to dive a bit deeper, as shown here:\\nyelp_raw_data[\\'business_id\\'].describe()\\nThe output is as follows:\\n# count                      10000\\n# unique                      4174\\n# top       ntN85eu27C04nwyPa8IHtw\\n# freq                          37\\n\\nhe describe  function will give us some quick stats about the column whose name we\\nenter into the quotation marks. Note how pandas automatically recognized that\\nbusiness_id  was a qualitative column and gave us stats that make sense. When\\ndescribe  is called on a qualitative column, we will always get the following four items:\\ncount : How many values are filled in\\nunique : How many unique values are filled in\\ntop: The name of the most common item in the dataset\\nfreq : How often the most common item appears in the dataset\\nAt the nominal level, we are usually looking for a few things, which would signal a\\ntransformation:\\nDo we have a reasonable number (usually under 20) of unique items?\\nIs this column free text?\\nIs this column completely unique across all rows?\\nSo, for the business_id  column, we have a count of 10000 . Don\\'t be fooled though! This\\ndoes not mean that we have 10,000 businesses being reviewed here. It just means that of the\\n10,000 rows of reviews, the business_id  column is filled in all 10,000 times. The next\\nqualifier, unique , tells us that we have 4174  unique businesses being reviewed in this\\ndataset. The most reviewed business is business JokKtdXU7zXHcr20Lrk29A , which was\\nreviewed 37 times:\\nyelp_raw_data[\\'review_id\\'].describe()\\nThe output is as follows:\\n# count                      10000\\n# unique                     10000\\n# top       M3jTv5NIipi_N4mgmZiIEg\\n# freq                           1\\nWe have a count  of 10000  and a unique  of 10000 . Think for a second, does this make\\nsense? Think about what each row represents and what this column represents.\\n(Insert Jeopardy theme song here)\\n\\nf course, it does! Each row of this dataset is supposed to represent a single, unique review\\nof a business and this column is meant to serve as a unique identifier for a review; so, it\\nmakes sense that the review_id  column has 10000  unique items in it. So, why is eTa5KD-\\nLTgQv6UT1Zmijmw  the most common  review? This is just a random choice from the 10,000\\nand means nothing:\\nyelp_raw_data[\\'text\\'].describe()\\nThe output is as follows:\\ncount                                                 10000\\nunique                                                 9998\\ntop       This review is for the chain in general. The l...\\nfreq                                                      2\\nThis column, which represents the actual text people wrote, is interesting. We would\\nimagine that this should also be similar to review_id  in that each one should contain\\nunique text, because it would be weird if two people wrote exactly the same thing; but we\\nhave two reviews with the exact same text! Let\\'s take a second to learn about DataFrame\\nfiltering to examine this further.\\nFiltering in pandas\\nLet\\'s talk a bit about how  filtering works. Filtering rows based on certain criteria is quite\\neasy in pandas. In a DataFrame, if we wish to filter out rows based on some search criteria,\\nwe will need to go row by row and check whether or not a row satisfies that particular\\ncondition; pandas handles this by passing in a Series  of True  and False  (Booleans).\\nWe literally pass into the DataFrame a list of True  and False  data that mean the following:\\nTrue : This row satisfies the condition\\nFalse : This row does not satisfy the condition\\nSo, first, let\\'s make the conditions. In the following lines of code, I will grab the text that\\noccurs twice:\\nyelp_raw_data[\\'text\\'].describe()[\\'top\\']\\n\\nere is a snippet of the text:\\n\"This review is for the chain in general. The location we went to is new so\\nit isn\\'t in Yelp yet. Once it is I will put this review there as\\nwell.......\"\\nRight off the bat, we can guess that this might actually be one person who went to review\\ntwo businesses that belong to the same chain and wrote the exact same review. However,\\nthis is just a guess right now.\\nThe duplicate_text  variable is of  string  type.\\nNow that we have this text, let\\'s use some magic to create that Series  of True  and False :\\nduplicate_text = yelp_raw_data[\\'text\\'].describe()[\\'top\\']\\ntext_is_the_duplicate = yelp_raw_data[\\'text\\'] == duplicate_text\\nRight away you might be confused. What we have done here is take the text column of the\\nDataFrame and compared it to the string, duplicate_text . This is strange because we\\nseem to be comparing a list of 10,000 elements to a single string. Of course, the answer\\nshould be a straight false, right?\\nSeries has a very interesting feature in that if you compare the Series  to an object, it will\\nreturn another Series of Booleans of the same length where each True  and False  is the\\nanswer to the question is this element the same as the element you are comparing it to?  Very\\nhandy!\\ntype(text_is_the_duplicate) # it is a Series of Trues and Falses\\ntext_is_the_duplicate.head() # shows a few Falses out of the Series\\nIn Python, we can add and subtract true and false as if they were 1 and 0, respectively. For\\nexample, True + False - True + False + True == 1 . So, we can verify that this Series  is correct\\nby adding up all of the values. As only two of these rows should contain the duplicate text,\\nthe sum of the Series should only be 2, which it is! This is as follows:\\nsum(text_is_the_duplicate) # == 2\\n\\now that we have our Series  of Booleans, we can pass it directly into our DataFrame,\\nusing bracket notation, and get our filtered rows, as illustrated:\\nfiltered_dataframe = yelp_raw_data[text_is_the_duplicate]\\n# the filtered Dataframe\\nfiltered_dataframe\\nIt seems that our suspicions were correct and one person, on the same day, gave the exact\\nsame review to two different business_id , presumably a part of the same chain. Let\\'s\\nkeep moving along to the rest of our columns:\\nyelp_raw_data[\\'type\\'].describe()\\n# count      10000\\n# unique         1\\n# top       review\\n# freq       10000\\nRemember this column? Turns out they are all the exact same type, namely review :\\nyelp_raw_data[\\'user_id\\'].describe()\\n# count                      10000\\n# unique                      6403\\n# top       fczQCSmaWF78toLEmb0Zsw\\n# freq                          38\\n\\nimilar to the business_id  column, all the 10000  values are filled in with 6403  unique\\nusers and one user reviewing 38 times!\\nIn this example, we won\\'t have to perform any transformations.\\nOrdinal level columns\\nAs far as ordinal columns go, we are looking  at date and stars . For each of these columns,\\nlet\\'s look at what the described method brings back:\\nyelp_raw_data[\\'stars\\'].describe()\\n# count    10000.000000\\n# mean         3.777500\\n# std          1.214636\\n# min          1.000000\\n# 25%          3.000000\\n# 50%          4.000000\\n# 75%          5.000000\\n# max          5.000000\\nWoah! Even though this column is ordinal, the describe  method returned stats that we\\nmight expect for a quantitative column. This is because the software saw a bunch of\\nnumbers and just assumed that we wanted stats like the mean  or the min and max. This is\\nnot a problem. Let\\'s use a method called value_counts  to see the count distribution, as\\nshown here:\\nyelp_raw_data[\\'stars\\'].value_counts()\\n# 4    3526\\n# 5    3337\\n# 3    1461\\n# 2     927\\n# 1     749\\n\\nhe value_counts  method will return the distribution of values for any column. In this\\ncase, we see that the star rating 4 is the most common, with 3526  values, followed closely\\nby rating 5. We can also plot this data to get a nice visual. First, let\\'s sort by star rating, and\\nthen use the prebuilt plot  method to make a bar chart:\\nimport datetime\\ndates = yelp_raw_data[\\'stars\\'].value_counts()\\ndates.sort_values\\ndates.plot(kind=\\'bar\\')\\nFrom this graph, we can conclude that people are definitely more likely to give good star\\nratings over bad ones! We can follow this procedure for the date column. I will leave you to\\ntry it on your own. For now, let\\'s look at a new dataset.\\nDataset 2 – Titanic\\nThe titanic  dataset contains a sample  of people who were  on the Titanic when it struck\\nan iceberg in 1912. Let\\'s go ahead and import it, as shown here:\\ntitanic = pd.read_csv(\\'short_titanic.csv\\')\\ntitanic.head()\\n\\nThis table represents the DataFrame for the dataset short_titanic.csv . This data is\\ndefinitely organized in a row/column structure, as is most spreadsheet data. Let\\'s take a\\nquick peek at its size, as shown here:\\ntitanic.shape\\n# (891, 5)\\nSo, we have 891 rows and 5 columns. Each row seems to represent a single passenger on\\nthe ship and as far as columns are concerned, the following list tells us what they indicate:\\nSurvived : This is a binary variable that indicates whether or not the passenger\\nsurvived the accident ( 1 if they survived, 0 if they died). This is likely to be at the\\nnominal level because there are only two options.\\nPclass : This is the class that the passenger was traveling in ( 3 for third class,\\nand so on). This is at the ordinal level.\\nName : This is the name of the passenger, and it is definitely at the nominal level.\\nSex: This indicates the gender of the passenger. It is at the nominal level.\\nAge: This one is a bit tricky. Arguably, you may place age at either a qualitative\\nor quantitative level; however, I think that age belongs to a quantitative state,\\nand thus, to the ratio level.\\nAs far as transformations are concerned, usually, we want all columns to be numerical,\\nregardless of their qualitative state. This means that Name  and Sex will have to be\\nconverted into numerical columns somehow. For Sex, we can change the column to hold 1\\nif the passenger was female and 0 if they were male. Let\\'s use pandas  to make the change.\\nWe will have to import another Python module, called numpy  or numerical Python, as\\nillustrated:\\nimport numpy as np\\ntitanic[\\'Sex\\'] = np.where(titanic[\\'Sex\\']==\\'female\\', 1, 0)\\n\\nhe np.where  method takes in three things:\\nA list of Booleans ( True  or False )\\nA new value\\nA backup value\\nThe method will replace all true  with the first value (in this case 1) and the false  with the\\nsecond value (in this case 0), leaving us with a new numerical column that represents the\\nsame thing as the original Sex column:\\ntitanic[\\'Sex\\']\\n# 0     0\\n# 1     1\\n# 2     1\\n# 3     1\\n# 4     0\\n# 5     0\\n# 6     0\\n# 7     0\\nLet\\'s use a shortcut and describe all the columns at once, as shown:\\ntitanic.describe()\\n\\n\\nhis table lists descriptive statistics of the titanic  dataset. Note how our qualitative\\ncolumns are being treated as quantitative; however, I\\'m looking for something irrelevant to\\nthe data type. Note the count row: Survived , Pclass , and Sex all have 891 values (the\\nnumber of rows), but Age only has 714 values. Some are missing! To double verify, let\\'s use\\nthe pandas functions, called isnull  and sum, as shown:\\ntitanic.isnull().sum()\\nSurvived      0\\nPclass        0\\nName          0\\nSex           0\\nAge         177\\nThis will show us the number of missing values in each column. So, Age is the only column\\nwith missing values to deal with.\\nWhen dealing with missing values, you usually have the following two options:\\nDrop the row with the missing value\\nTry to fill it in\\nDropping the row is the easy choice; however, you run the risk of losing valuable data! For\\nexample, in this case, we have 177 missing age values (891-714), which are nearly 20% of\\nthe data. To fill in the data, we could either go back to the history books, find each person\\none by one, and fill in their age, or we can fill in the age with a placeholder value.\\nLet\\'s fill in each missing value of the Age column with the overall average age of the people\\nin the dataset. For this, we will use two new methods, called mean  and fillna . We use\\nisnull  to tell us which values are null and the mean  function to give us the average value\\nof the Age column. The fillna  method is a pandas method that replaces null values with a\\ngiven value:\\nprint sum(titanic[\\'Age\\'].isnull()) # == 177 missing values\\naverage_age = titanic[\\'Age\\'].mean() # get the average age\\ntitanic[\\'Age\\'].fillna(average_age, inplace = True) #use the fillna method\\nto remove null values\\nprint sum(titanic[\\'Age\\'].isnull()) # == 0 missing values\\n\\ne\\'re done! We have replaced each value with 26.69 , the average age in the dataset. The\\nfollowing code now confirms that no null values exist:\\ntitanic.isnull().sum()\\nSurvived      0\\nPclass        0\\nName          0\\nSex           0\\nAge           0\\nGreat! Nothing is missing, and we did not have to remove any rows:\\ntitanic.head()\\nAt this point, we could start getting a bit more complicated with our questions, for\\nexample, what is the average age for a female or a male?  To answer this, we can filter by each\\ngender and take the mean age; pandas has a built-in function for this, called groupby , as\\nillustrated here:\\ntitanic.groupby(\\'Sex\\')[\\'Age\\'].mean()\\nThis means group the data by the Sex column, and then give me the mean age for each group . This\\ngives us the following output:\\nSex\\n0      30.505824\\n1      28.216730\\nWe will ask more of these difficult and complex questions and will be able to answer them\\nwith Python and statistics.\\n\\nummary\\nAlthough this is only our first look at data exploration, don\\'t worry, this is definitely not the\\nlast time we will follow these steps for data science and exploration. From now on, every\\ntime we look at a new piece of data, we will use our steps of exploration to transform, break\\ndown, and standardize our data. The steps outlined in this chapter, while they are only\\nguidelines, form a standard practice that any data scientist can follow in their work. The\\nsteps can also be applied to any dataset that requires analysis.\\nWe are rapidly approaching the section of the book that deals with statistical, probabilistic,\\nand machine learning models. Before we can truly jump into these models, we have to look\\nat some of the basics of mathematics. In the next chapter, we will take a look at some of the\\nmath necessary to perform some of the more complicated operations in modeling, but don\\'t\\nworry, the math required for this process is minimal, and we will go through it step by\\nstep.\\n\\n\\nBasic Mathematics\\nIt\\'s time to start looking at some basic mathematics principles that are handy when dealing\\nwith data science. The word math  tends to strike fear in the hearts of many, but I aim to \\nmake  this as enjoyable as possible. In this chapter, we will go over the basics of the\\nfollowing topics:\\nBasic symbols/terminology\\nLogarithms/exponents\\nSet theory\\nCalculus\\nMatrix (linear) algebra\\nWe will also cover other fields of mathematics. Moreover, we will see how to apply each of\\nthese to various aspects of data science, as well as other scientific endeavors.\\nRecall that, in a previous chapter, we identified math as being one of the three key\\ncomponents of data science. In this chapter, I will introduce concepts that will become\\nimportant later on in this book – when looking at probabilistic and statistical models – and\\nwe will also be looking at concepts that will be useful in this chapter. Regardless of this, all\\nof the concepts in this chapter should be considered fundamental to your quest to become a\\ndata scientist.\\nMathematics as a discipline\\nMathematics, as a science, is one of the oldest known forms of logical thinking. Since\\nancient Mesopotamia (3,000 BCE), and probably before, humans have been relying on \\narithmetic  and more challenging forms of math to answer life\\'s biggest questions.\\n\\noday, we rely on math for most  aspects of our daily lives; yes, I know that sounds like a\\ncliche, but I mean it. Whether you are watering your plants or feeding your dog, your\\ninternal mathematical engine is constantly working —calculating how much water the plant\\nhad per day over the last week and predicting the next time your dog will be hungry given\\nthat it is eating right now. Whether or not you are consciously using the principles of math,\\nthe concepts live deep inside everyone\\'s brains. It\\'s my job as a math teacher to get you to\\nrealize it.\\nBasic symbols and terminology\\nIn the following section, we will review  the mathematical concepts of vectors, matrices,\\narithmetic symbols, and linear algebra, as well as some more subtle notations used  by data\\nscientists.\\nVectors and matrices\\nA vector  is defined as an object with  both magnitude and direction. This definition,\\nhowever, is a bit complicated. For our purpose, a vector is simply a 1-dimensional array\\nrepresenting a series of numbers. Put another way, a vector is a list of numbers.\\nIt is generally represented using an arrow or bold font, shown as follows:\\nVectors are broken into components, which are individual members of the vector. We use\\nindex notations to denote the element that we are referring to, illustrated as follows:\\nIn math, we generally refer to the first element as index  1, as opposed to\\ncomputer science, where we generally refer to the first element as index  0.\\nIt is important to remember which index system you are using.\\n\\nn Python, we can represent arrays in many ways. We could simply use a Python list to\\nrepresent the preceding array:\\nx = [3, 6, 8]\\nHowever, it is better to use the numpy  array type to represent arrays, as shown, because it\\ngives us much more utility when performing vector operations:\\nimport numpy as np\\nx = np.array([3, 6, 8])\\nRegardless of the Python representation, vectors give us a simple way of storing multiple\\ndimensions  of a single data point/observation.\\nIf we measure the average satisfaction rating (0-100) of employees in three departments of a\\ncompany as being 57 for HR, 89 for engineering, and 94 for management. We can represent\\nthis as a vector with the following formula:\\nThis vector holds three different bits of information about our data. This is the perfect use\\nof a vector in data science.\\nYou can also think of a vector as being the theoretical generalization of the pandas Series\\nobject. So, naturally, we need something to represent the DataFrame.\\nWe can extend our notion of an array to move beyond a single dimension and represent\\ndata in multiple dimensions.\\nA matrix  is a two-dimensional representation of arrays of numbers. Matrices (plural) have\\ntwo main characteristics that we need to be aware of. The dimension of a matrix, denoted\\nby n x m (n by m ), tells us that the matrix has n rows and m columns. Matrices are generally\\ndenoted by a capital, bold-faced letter, such as X. Consider the following example:\\n\\n\\nhis is a 3 x 2 (3 by 2 ) matrix because it has three rows and two columns.\\nIf a matrix has the same number of rows and columns, it is called a square\\nmatrix .\\nThe matrix is our generalization of the pandas DataFrame. It is arguably one of the most\\nimportant mathematical objects in our toolkit. It is used to hold organized information, in\\nour case, data.\\nRevisiting our previous example, let\\'s say we have three offices in different locations, each\\nwith the same three departments: HR, engineering, and management. We could make three\\ndifferent vectors, each holding a different office\\'s satisfaction scores, as shown:\\nHowever, this is not only cumbersome, but also unscalable. What if you have 100 different\\noffices? Then you would need to have 100 different one-dimensional arrays to hold this\\ninformation.\\nThis is where a matrix alleviates this problem. Let\\'s make a matrix where each row\\nrepresents a different department and each column represents a different office, as shown :\\nThis is much more natural. Now, let\\'s strip away the labels, and we are left with a matrix!\\n\\n\\nuick exercises\\nIf we added a fourth office, would we need  a new row or column? 1.\\nWhat would the dimension of the matrix  be after we added the fourth office? 2.\\nIf we eliminate the management department from the original X matrix, what 3.\\nwould the dimension of the new matrix be?\\nWhat is the general formula to find out the number of elements in the matrix?4.\\nAnswers\\nColumn1.\\n3 x 4 2.\\n2 x 3 3.\\nn × m (n being the number  of rows and m being the number  of columns) 4.\\nArithmetic symbols\\nIn this section, we will go over some  symbols associated with basic arithmetic that appear\\nin most, if not all, data science tutorials and books.\\nSummation\\nThe uppercase sigma ∑ symbol is a universal symbol for addition. Whatever is to the right\\nof the sigma symbol is usually  something iterable, meaning that we can go over it one by\\none (for example, a vector).\\nFor example, let\\'s create the representation of a vector:\\nX = [1, 2, 3, 4, 5]\\nTo find the sum of the content, we can use the following formula:\\nIn Python, we can use the following formula:\\nsum(x) # == 15\\n\\nor example, the formula for calculating the mean of a series of numbers is quite common.\\nIf we have a vector ( x) of length n, the mean of the vector can be calculated as follows:\\nThis means that we will add up each element of x, denoted by xi, and then multiply the sum\\nby 1/n, otherwise known as dividing by n, (the length of the vector).\\nProportional\\nThe lowercase alpha symbol, α, represents values  that are proportional to each other. This\\nmeans that as one value changes, so does the other. The direction in which the values move\\ndepends on how the values are proportional. Values can either vary directly or indirectly. If\\nvalues vary directly, they both move in the same direction (as one goes up, so does the\\nother). If they vary indirectly, they move in opposite directions (if one goes down, the other\\ngoes up).\\nConsider the following examples:\\nThe sales of a company vary directly with the number of customers. This can be\\nwritten as Sales α Customers .\\nGas prices vary (usually) indirectly with oil availability, meaning that as the\\navailability of oil goes down (it\\'s more scarce), gas prices go up. This can be\\ndenoted as Gas α Availability .\\nLater on, we will see a very  important formula called the Bayes\\' formula , which includes a\\nvariation symbol.\\nDot product\\nThe dot product is an operator like addition and multiplication. It is used to combine two\\nvectors, as shown:\\n\\n\\no, what does this mean? Let\\'s say we have a vector that represents a customer\\'s sentiments\\ntoward three genres of movies: comedy, romance, and action.\\nWhen using a dot product, note that an answer is a single number, known\\nas a scalar .\\nOn a scale of 1-5, a customer loves  comedies, hates romantic movies, and is alright with\\naction movies. We might represent this as follows:\\nHere, 5 denotes their love for comedies, 1 their hatred of romantic movies, and 3 the\\ncustomer\\'s indifference toward action movies.\\nNow, let\\'s assume that we have two new movies, one of which is a romantic comedy and\\nthe other is a funny action movie. The movies would have their own vector of qualities, as\\nshown:\\nHere, m1 is our romantic comedy and m2 is our funny action movie.\\nIn order to make a recommendation, we will apply the dot product between the customer\\'s\\npreferences for each movie. The higher value will win and, therefore, will be recommended\\nto the user.\\nLet\\'s compute the recommendation score for each movie. For movie 1, we want to compute\\nthe following:\\n\\n\\ne can think of this problem like as follows:\\nThe answer we obtain is 28, but what does this number mean? On what scale is it? Well, the\\nbest score anyone can ever get is when all values are 5, making the outcome as follows:\\nThe lowest possible score is when all values are 1, as shown:\\nSo, we must think about 28 on a scale from 3 to 75. To do this, imagine a number line from 3\\nto 75 and where 28 would be on it. This is illustrated as follows:\\nNot that far. Let\\'s try for movie 2:\\n\\n\\nhis is higher than 28! Putting this number on the same timeline as before, we can also\\nvisually observe that it is a much better score, as shown:\\nSo, between movie 1 and movie 2, we would definitely recommend movie 2 to our user.\\nThis is, in essence, how most movie prediction engines work. They build a customer profile,\\nwhich is represented as a vector. They then take a vector representation of each movie they\\nhave to offer, combine them with the customer profile (perhaps with a dot product), and\\nmake recommendations from there. Of course, most companies must do this on a much\\nlarger scale, which is where  a particular field of mathematics, called linear algebra , can be\\nvery useful; we will look at it later in the chapter.\\nGraphs\\nNo doubt you have encountered dozens, if not hundreds, of graphs in your life so far. I\\'d\\nlike to mostly talk about conventions with regard to graphs and notations.\\nThe following is a basic Cartesian graph  (x and y coordinates). The x and y notations are\\nvery standard but sometimes do not entirely explain the big picture. We sometimes refer  to\\nthe x variable as being the independent variable and the y as the dependent variable. This is\\nbecause when we write functions, we tend to speak about them as being y is a function of x ,\\nmeaning that the value of y is dependent on the value of x. This is what a graph is trying to\\nshow.\\nSuppose we have two points on a graph, as shown:\\nWe refer to the points as (x1, y1) and (x2, y2).\\n\\nhe slope  between these two points is defined as follows:\\nYou have probably seen this formula before, but it is worth mentioning, if only for its\\nsignificance. The slope defines the rate of change between the two points. Rates of change\\ncan be very important in data science, specifically in areas involving differential equations\\nand calculus.\\nRates of change are a way of representing how variables move together and to what degree.\\nImagine we are modeling the temperature of your coffee in relation to the time that it has\\nbeen sitting outside. Perhaps we have a rate of change as follows:\\nThis rate of change is telling us that for every single minute, our coffee\\'s temperature is\\ndropping by two degrees Fahrenheit.\\nLater on in this book,  we will look at a machine learning algorithm called linear regression.\\nIn linear regression, we are concerned with the rates of change between variables, as they\\nallow us to exploit this relationship for predictive purposes.\\nThink of the Cartesian plane as being an infinite plane of vectors with two\\nelements. When people refer to higher dimensions, such as 3D or 4D, they\\nare merely referring to an infinite space that holds vectors with more\\nelements. A 3D space holds vectors of length three while a 7D space holds\\nvectors with seven elements in them.\\nLogarithms/exponents\\nAn exponent  tells you how many  times you have  to multiply a number by itself, as\\nillustrated:\\n\\n\\n logarithm  is the number that answers the question \"what exponent gets me from the base\\nto this other number?\" This can be denoted as follows:\\nIf these two concepts seem similar, then you are correct! Exponents and logarithms are\\nheavily related. In fact, the words exponent and logarithm actually mean the same thing! A\\nlogarithm is an exponent. The preceding two equations are actually two versions of the\\nsame thing. The basic idea is that 2 times 2 times 2 times 2 is 16.\\nThe following is a depiction of how we can use both versions to say the same thing. Note\\nhow I use arrows to move from the log formula to the exponent formula:\\nConsider the following examples:\\nNote something interesting. Let\\'s rewrite the first equation:\\nWe then replace 81 with the equivalent statement,  , as follows:\\nSomething interesting to note : the 3s seem to cancel out . This is actually very important\\nwhen dealing with numbers more difficult to work with than 3s and 4s.\\nExponents and logarithms  are most important when  dealing with growth. More often than\\nnot, if a quantity is growing (or declining in growth), an exponent/logarithm can help\\nmodel this behavior.\\n\\nor example, the number e is around 2.718 and has many practical applications. A very\\ncommon application is interest  calculation for saving . Suppose you have $5,000 deposited\\nin a bank with continuously compounded interest at the rate of 3%, then we can use the\\nfollowing formula to model the growth of your deposit:\\nIn this formula:\\nA denotes the final amount\\nP denotes the principal investment ( 5000 )\\ne denotes a constant ( 2.718 )\\nr denotes the rate of growth ( .03)\\nt denotes the time (in years)\\nWe are curious; when will our investment double? How long would I have to have my\\nmoney in this investment to achieve 100% growth? We can write this in mathematical form,\\nas follows:\\n (divided by 5000 on both sides)\\nAt this point, we have a variable in the exponent that we want to solve. When this happens,\\nwe can use the logarithm notation to figure it out:\\nThis leaves us with  \\nWhen we take the logarithm of a number with a base of e, it is called a natural logarithm . We\\nrewrite the logarithm as follows:\\n\\n\\nsing a calculator (or Python), we find that \\nThis means that it would take 2.31 years to double our money.\\nSet theory\\nSet theory involves mathematical operations at the set level. It is sometimes thought of as a\\nbasic fundamental group  of theorems that governs the rest of mathematics. For our\\npurpose, we\\'ll use set theory to manipulate groups of elements.\\nA set is a collection of distinct objects.\\nThat\\'s it! A set can be thought of as a list in Python, but with no repeat objects. In fact, there\\nis even a set of objects in Python:\\ns = set()\\ns = set([1, 2, 2, 3, 2, 1, 2, 2, 3, 2])\\n# will remove duplicates from a list\\ns == {1, 2, 3}\\nNote that, in Python, curly braces {, } can denote either a set or a\\ndictionary.\\nRemember that a dictionary in Python is a set of key-value pairs, for\\nexample:\\ndict = {\"dog\": \"human\\'s best friend\", \"cat\": \"destroyer of world\"}\\ndict[\"dog\"]# == \"human\\'s best friend\"\\nlen(dict[\"cat\"]) # == 18\\n# but if we try to create a pair with the same key as an existing key\\ndict[\"dog\"] = \"Arf\"\\ndict\\n{\"dog\": \"Arf\", \"cat\": \"destroyer of world\"}\\n# It will override the previous value\\n# dictionaries cannot have two values for one key.\\n\\nhey share this notation because they share a quality in that sets cannot have duplicate\\nelements, just as dictionaries cannot have duplicate keys.\\nThe magnitude  of a set is the number of elements in the set and is represented as follows:\\ns  # == {1,2,3}\\nlen(s) == 3 # magnitude of s\\nThe concept of an empty set exists and is denoted by the character  {}.\\nThis null  set is said to have a magnitude of 0.\\nIf we wish to denote that an element is within a set, we use the epsilon notation, as shown:\\n2 ∈ {1,2,3}\\nThis means that the  2 element exists in the set of 1, 2, and 3. If one set is entirely inside\\nanother set, we say that it is a subset  of its larger counterpart:\\nA= {1,5,6}, B={1,5,6,7,8}\\nA ⊆ B                                                                                                   \\n \\nSo, A is a subset of B and B is called the superset  of A. If A is a subset of B but A does not\\nequal B (meaning that there is at least one element in B that is not in A), then A is called a\\nproper subset  of B.\\nConsider the following examples:\\nA set of even numbers is a subset of all integers\\nEvery set is a subset, but not a proper subset, of itself\\nA set of all tweets is a superset of English tweets\\nIn data science, we use sets (and lists) to represent a list of objects and, often, to generalize\\nthe behavior of consumers. It is common to reduce a customer to a set of characteristics.\\n\\nmagine we are a marketing firm  trying to predict where a person wants to shop for clothes.\\nWe are given a set of clothing brands the user has previously visited, and our goal is to\\npredict a new store that they would also enjoy. Suppose a specific user has previously\\nshopped at the following stores:\\nuser1 = {\"Target\",\"Banana Republic\",\"Old Navy\"}\\n# note that we use {} notation to create a set\\n# compare that to using [] to make a list\\nSo, user1  has previously shopped at Target , Banana Republic , and Old Navy . Let\\'s\\nalso look at a different user, called user2 , as shown:\\nuser2 = {\"Banana Republic\",\"Gap\",\"Kohl\\'s\"}\\nSuppose we are wondering how similar these users are. With the limited information we\\nhave, one way to define similarity is to see how many stores there are that they both shop\\nat. This is called an intersection :\\nThe intersection of two sets is a set whose elements appear in both sets. It is denoted using\\nthe ∩ symbo l, as shown:\\nThe intersection of the two users is just one store. So, right away, that doesn\\'t seem great.\\nHowever, each user only has three elements in their set, so having 1/3 does not seem as bad.\\nSuppose we are curious about how many stores are represented between the two of them;\\nthis is called a union .\\nThe union of two sets is a set whose elements appear in either set. It is denoted using the\\nsymbol ∪, as shown:\\n\\n\\nhen looking at the similarities between user1  and user2 , we should use a combination of\\nthe union and the intersection of their sets. user1  and user2  have one element in common\\nout of a total of five distinct elements between them. So, we can define the similarity\\nbetween the two users as follows:\\n                                                     \\n          \\nIn fact, this has a name in set theory. It is called the Jaccard measure . In general, for the A\\nand B sets, the Jaccard  measure (Jaccard similarity) between the two sets is defined as\\nfollows:\\n                                                          \\nIt can also be defined as the magnitude of the intersection of the two sets divided by the\\nmagnitude of the union of the two sets.\\nThis gives us a way to quantify similarities between elements represented with sets.\\nIntuitively, the Jaccard measure is a number between 0 and 1, such that when the number is\\ncloser to 0, people are more dissimilar and when the measure is closer to 1, people are\\nconsidered similar to each other.\\nIf we think about the definition, then it actually makes sense. Take a look at the measure\\nonce more:\\nHere, the numerator represents the number of stores that the users have in common (in the\\nsense that they like shopping there), while the denominator represents the unique number\\nof stores that they like put together.\\nWe can represent this in Python using some simple code, as shown:\\nuser1 = {\"Target\",\"Banana Republic\",\"Old Navy\"}\\nuser2 = {\"Banana Republic\",\"Gap\",\"Kohl\\'s\"}\\ndef jaccard(user1, user2):\\n  stores_in_common = len(user1 & user2)\\n  stores_all_together = len(user1 | user2)\\n\\n return stores / float(stores_all_together)\\n# I cast stores_all_together as a float to return a decimal answer instead\\nof python\\'s default integer division\\n# so\\njaccard(user1, user2) == # 0.2 or 1/5\\nSet theory becomes highly prevalent when we enter the world of probability and also when\\ndealing with high-dimensional data. We will use sets to represent real-world events taking\\nplace, and probability becomes set theory with vocabulary on top of it.\\nLinear algebra\\nRemember the movie recommendation engine  we looked at earlier? What if we had 10,000\\nmovies to recommend and we had to choose only 10 to give to the user? We\\'d have to take\\na dot product between the user profile and each of the 10,000 movies. Linear algebra\\nprovides the tools to make these calculations much more efficient.\\nIt is an area of mathematics that deals with the math of matrices and vectors. It has the aim\\nof breaking down these objects and reconstructing them in order to provide practical\\napplications. Let\\'s look at a few linear algebra rules before proceeding.\\nMatrix multiplication\\nLike numbers, we can multiply  matrices together. Multiplying matrices is, in essence, a\\nmass-produced way of taking several dot products at once. Let\\'s, for example, try to \\nmultiply  the following matrices:\\nWe need to consider a couple of things:\\nUnlike numbers, multiplication of matrices is not commutative , meaning that the\\norder in which you multiply matrices matters a great deal.\\nIn order to multiply matrices, their dimensions must match up. This means that\\nthe first matrix must have the same number of columns as the second matrix has\\nrows.\\n\\no remember this, write out the dimensions of the matrices. In this case, we have a 3 x 2\\ntimes a 2 x 2 matrix. You can multiply matrices together if the second number in the first\\ndimension pair is the same as the first number in the second dimension pair:\\n                                                                          \\nThe resulting matrix will always have dimensions equal to the outer numbers in the\\ndimension pairs (the ones you did not circle). In this case, the resulting matrix will have a\\ndimension of 3 x 2 .\\nHow to multiply matrices\\nTo multiply matrices, there is actually quite  a simple procedure. Essentially, we are\\nperforming a bunch of dot products.\\nRecall our earlier sample problem, which was as follows:\\nWe know that our resulting matrix will have a dimension of 3 x 2. So, we know it will look\\nsomething like the following:\\nNote that each element of the matrix is indexed using a double index. The\\nfirst number represents the row, and the second number represents the\\ncolumn. So, the m32 element is the element in the third row of the second\\ncolumn. Each element is the result of a dot product between rows and\\ncolumns of the original matrices.\\n\\nhe mxy element is the result of the dot product of the xth row of the first matrix and the yth\\ncolumn of the second matrix. Let\\'s solve a few:\\nMoving on, we will eventually get a resulting matrix that looks as follows:\\nWay to go! Let\\'s come back to the movie recommendation example. Recall the user\\'s movie\\ngenre preferences of comedy, romance, and action, which are illustrated as follows:\\nNow suppose we have 10,000 movies, all with a rating for these three categories. To make a\\nrecommendation, we need to take the dot product of the preference vector with each of the\\n10,000 movies. We can use matrix multiplication to represent this.\\nInstead of writing them all out, let\\'s express them using the matrix notation. We already\\nhave U, defined here as the user\\'s preference vector (it can also be thought of as a 3 x 1\\nmatrix), and we also need a movie matrix:\\nM = movies = 3 x 10,000\\nSo, now we have two matrices; one is 3 x 1 and the other is 3 x 10,000 . We can\\'t multiply\\nthese matrices as they are, because the dimensions do not work out. We will have to change\\nU a bit. We can take the transpose  of the matrix (turning all rows into columns and columns\\ninto rows). This will switch the dimensions around:\\n\\n\\no, now we have two matrices  that can be multiplied together. Let\\'s visualize what this\\nlooks like:\\n                                                                                     1  x  3                     3  x  1000\\nThe resulting matrix will be a 1 x 1,000  matrix (a vector) of 10,000 predictions for each\\nindividual movie. Let\\'s try this out in Python:\\nimport numpy as np\\n# create user preferences\\nuser_pref = np.array([5, 1, 3])\\n# create a random movie matrix of 10,000 movies\\nmovies = np.random.randint(5,size=(3,1000))+1\\n# Note that the randint will make random integers from 0-4\\n# so I added a 1 at the end to increase the scale from 1-5\\nWe are using the numpy  array function to create our matrices. We will have both a\\nuser_pref  and a movies  matrix to represent our data.\\nTo check our dimensions, we can use the numpy shape  variable, as shown:\\nprint(user_pref.shape) # (1, 3)\\nprint(movies.shape) # (3, 1000)\\nThis checks out. Last but not least, let\\'s use the matrix multiplication method of numpy\\n(called dot) to perform the operation, as illustrated:\\n# np.dot does both dot products and matrix multiplication\\nnp.dot(user_pref, movies)\\nThe result is an array of integers that represents the recommendations for each movie.\\n\\nor a quick extension of this, let\\'s run some code  that predicts across more than 10,000\\nmovies, as shown:\\nimport numpy as np\\nimport time\\nfor num_movies in (10000, 100000, 1000000, 10000000, 100000000):\\n   movies = np.random.randint(5,size=(3, num_movies))+1\\n    now = time.time()\\n    np.dot(user_pref, movies)\\n    print((time.time() - now), \"seconds to run\", num_movies, \"movies\")\\n0.000160932540894 seconds to run 10000 movies\\n0.00121188163757 seconds to run 100000 movies\\n0.0105860233307 seconds to run 1000000 movies\\n0.096577167511 seconds to run 10000000 movies\\n4.16197991371 seconds to run 100000000 movies\\nIt took only a bit over four seconds to run through 100,000,000 movies using matrix\\nmultiplication.\\nSummary\\nIn this chapter, we took a look at some basic mathematical principles that will become very\\nimportant as we progress through this book. Between logarithms/exponents, matrix\\nalgebra, and proportionality, mathematics clearly has a big role not just in the analysis of\\ndata but in many aspects of our lives.\\nThe coming chapters will take a much deeper dive into two big areas of mathematics:\\nprobability and statistics. Our goal will be to define and interpret the smallest and biggest\\ntheorems in these two giant fields of mathematics.\\nIt is in the next few chapters that everything will start to come together. So far in this book,\\nwe have looked at math examples, data exploration guidelines, and basic insights into\\ntypes of data. It is time to begin to tie all of these concepts together.\\n\\n\\nImpossible or Improbable – A\\nGentle Introduction to\\nProbability\\nOver the next few chapters, we will explore both probability and statistics as methods of\\nexamining both data-driven situations and real-world scenarios. The rules of probability\\ngovern the basics of prediction. We use probability to define the chances of the occurrence\\nof an event.\\nIn this chapter, we will look at the following topics:\\nWhat is the probability?\\nThe differences between the Frequentist approach and the Bayesian approach\\nHow to visualize probability\\nHow to utilize the rules of probability\\nUsing confusion matrices to look at the basic metrics\\nProbability will help us model real-life events that include a sense of randomness and\\nchance. Over the next two chapters, we will look at the terminology behind probability\\ntheorems and how to apply them to model situations that can appear unexpectedly.\\nBasic definitions\\nOne of the most basic concepts of probability is the concept of a procedure. A procedure  is\\nan act that leads  to a result, for example, throwing a die or visiting a website.\\n\\nn event  is a collection of the outcomes of a procedure, such as getting a head on a coin flip\\nor leaving a website after only 4 seconds. A simple event is an outcome/event of a\\nprocedure that cannot be broken down further. For example, rolling two dice can be broken\\ndown into two simple events: rolling die 1 and rolling die 2.\\nThe sample space  of a procedure is the set of all possible simple events. For example, an\\nexperiment is performed in which a coin is flipped three times in succession. What is the\\nsize of the sample space for this experiment?\\nThe answer is eight because the results could be any one of the possibilities in the following\\nsample space: {HHH, HHT, HTT, HTH, TTT, TTH, THH, or THT}.\\nProbability\\nThe probability  of an event represents the frequency, or chance, that the event will happen.\\nFor notation , if A is an event, P(A)  is the probability of the occurrence of the event.\\nWe can define the actual probability of an event, A, as follows:\\nHere, A is the event in question. Think of an entire universe of events where anything is\\npossible, and let\\'s represent it as a circle. We can think of a single event, A, as being a\\nsmaller circle within that larger universe, as shown in the following diagram:\\n\\n\\net\\'s now pretend that our universe involves a research study on humans and the A event\\nis people in that study who have cancer.\\nIf our study has 100 people and A has 25 people, the probability of A or P(A)  is 25/100.\\nThe maximum probability of any event is 1. This can be understood as the red circle grows\\nso large that it is the size of the universe (the larger circle).\\nThe most basic examples (I promise they will get more interesting) are coin flips. Let\\'s say\\nwe have two coins and we want the probability that we will get two heads. We can very\\neasily count the number of ways two coins could end up being two heads. There\\'s only one!\\nBoth coins have to be heads. But how many options are there? It could either be two heads,\\ntwo tails, or a heads/tails combination.\\nFirst, let\\'s define A. It is the event in which two heads occur. The number of ways that A can\\noccur is 1.\\nThe sample space of the experiment is {HH, HT, TH, TT} , where each two-letter word\\nindicates the outcome of the first and second coin simultaneously. The size of the sample\\nspace is four. So, P(getting two heads) = 1/4 .\\nLet\\'s refer to a quick visual table to prove it. The following table denotes the options for\\ncoin 1 as the columns, and the options for coin 2 as the rows. In each cell, there is\\neither True  or False . A True  value indicates that it satisfies the condition (both heads),\\nand False  indicates otherwise:\\nCoin 1 is heads Coin 1 is tails\\nCoin 2 is heads True False\\nCoin 2 is tails False False\\nSo, we have one out of a total of four possible outcomes.\\nBayesian versus Frequentist\\nThe preceding example was almost  too easy. In practice, we can hardly ever truly count the\\nnumber of ways something can happen. For example, let\\'s say that we want to know the\\nprobability of a random  person smoking cigarettes at least once a day. If we wanted  to\\napproach this problem using  the classical way (the previous formula), we would need to\\nfigure out how many different ways a person is a smoker —someone who smokes at least\\nonce a day —which is not possible!\\n\\nhen faced with such a problem, two main schools of thought are considered when it\\ncomes to calculating probabilities in practice: the Frequentist approach  and the Bayesian\\napproach . This chapter will focus heavily on the Frequentist approach, while the\\nsubsequent chapter will dive into the Bayesian approach.\\nFrequentist approach\\nIn a Frequentist approach, the probability of an event  is calculated through\\nexperimentation. It uses the past in order to predict the future chance of an event. The basic\\nformula is as follows:\\nBasically, we observe several instances of the event and count the number of times A was\\nsatisfied. The division of these numbers is an approximation of the probability.\\nThe Bayesian approach differs by dictating that probabilities must be discerned using\\ntheoretical means. Using the Bayes approach, we would have to think a bit more critically\\nabout events and why they occur. Neither methodology is wholly the correct answer all of\\nthe time. Usually, it comes down to the problem and the difficulty of using either approach.\\nThe crux of the Frequentist approach is the relative frequency.\\nThe relative frequency  of an event  is how often an event occurs divided by the total\\nnumber of observations.\\nExample – marketing stats\\nLet\\'s say that you are interested in ascertaining how often a person who visits your website\\nis likely to return on a later date. This is sometimes called the rate of repeat visitors. In the\\nprevious definition, we would define our A event as being a visitor coming back to the site.\\nWe would then have to calculate the number of ways a person can come back, which\\ndoesn\\'t really make sense at all! In this case, many people would turn to a Bayesian\\napproach; however, we can calculate what is known as relative frequency.\\nSo, in this case, we can take the visitor logs and calculate the relative frequency of event A\\n(repeat visitors). Let\\'s say, of the 1,458  unique visitors in the past week, 452 were repeat\\nvisitors. We can calculate this as follows:\\n\\n\\no, about 31% of your visitors are repeat visitors.\\nThe law of large numbers\\nThe reason that even the Frequentist approach  can do this is that of the law of large\\nnumbers, which states that if we repeat a procedure over and over, the relative frequency\\nprobability will approach the actual probability. Let\\'s try to demonstrate this using Python.\\nIf I were to ask you the average of the numbers 1 and 10, you would very quickly answer\\naround 5. This question is identical to asking you to pick the average  number between 1 and\\n10.\\nPython will choose n random numbers between 1 and 10 and find their average.\\nWe will repeat this experiment several times using a larger n each time, and then we will\\ngraph the outcome. The steps are as follows:\\nPick a random number between 1 and 10 and find the average1.\\nPick two random numbers between 1 and 10 and find their average2.\\nPick three random numbers between 1 and 10 and find their average3.\\nPick 10,000 random numbers between 1 and 10 and find their average4.\\nGraph the results5.\\nLet\\'s take a look at the code:\\nimport numpy as np\\nimport pandas as pd\\nfrom matplotlib import pyplot as plt\\n%matplotlib inline\\nresults = []\\nfor n in range(1,10000):\\n    nums = np.random.randint(low=1,high=10, size=n) # choose n numbers\\nbetween 1 and 10\\n    mean = nums.mean()                              # find the average of\\nthese numbers\\n    results.append(mean)                            # add the average to a\\nrunning list\\n# POP QUIZ: How large is the list results?\\nlen(results) # 9999\\n# This was tricky because I took the range from 1 to 10000 and usually we\\ndo from 0 to 10000\\ndf = pd.DataFrame({ \\'means\\' : results})\\nprint (df.head()) # the averages in the beginning are all over the place!\\n# means\\n\\n 9.0\\n# 5.0\\n# 6.0\\n# 4.5\\n# 4.0\\nprint (df.tail()) # as n, our size of the sample size, increases, the\\naverages get closer to 5!\\n# means\\n# 4.998799\\n# 5.060924\\n# 4.990597\\n# 5.008802\\n# 4.979198\\ndf.plot(title=\\'Law of Large Numbers\\')\\nplt.xlabel(\"Number of throws in sample\")\\nplt.ylabel(\"Average Of Sample\")\\nCool, right? What this is essentially showing us is that as we increase the sample size of our\\nrelative frequency, the frequency approaches the actual average (probability) of 5.\\nIn our statistics chapters, we will work to define this law much more rigorously, but for\\nnow, just know that it is used to link the relative frequency of an event to its actual\\nprobability.\\n\\nompound events\\nSometimes, we need to deal with two or more events. These are called compound events . A\\ncompound event is an event that combines two or more simple events. When this happens,\\nwe need a special notation.\\nGiven events A and B:\\nThe probability that A and B occur is P(A ∩ B) = P(A and B)\\nThe probability that either A or B occurs is P(A ∪ B) = P(A or B)\\nUnderstanding why we use the set notation for these compound events is very important.\\nRemember how we represented events in a universe using circles earlier? Let\\'s say that our\\nuniverse  is 100 people who showed up for an experiment in which a new test for cancer is\\nbeing developed:\\nIn the preceding diagram, the red circle, A, represents 25 people who actually have cancer.\\nUsing the relative frequency approach, we can say that P(A) = number of people with\\ncancer/number of people in study , that is, 25/100 = ¼ = .25 . This means that there is a 25%\\nchance that someone has cancer.\\n\\net\\'s introduce a second event, called B, as shown, which contains people for whom the test\\nwas positive (it claimed that they had cancer). Let\\'s say that this is for 30 people. So, P(B) =\\n30/100 = 3/10 = .3 . This means that there is a 30% chance that the test said positive for any\\ngiven person:\\nThese are two separate events, but they interact with each other. Namely, they might\\nintersect  or have people in common, as shown here:\\n\\n\\nnyone in the space that both A and B occupies, otherwise known as A intersect B  or A ∩ B,\\nare people for whom the test claimed they were positive for cancer ( A), and they actually\\ndo have cancer. Let\\'s say that\\'s 20 people. The test said positive for 20 people, that is, they\\nhave cancer, as shown here:\\nThis means that P(A and B) = 20/100 = 1/5 = .2 = 20% .\\nIf we want to say that someone has cancer, or the test came back positive, this would be the\\ntotal sum (or union) of the two events, namely, the sum of 5, 20, and 10, which is 35. So,\\n35/100 people either have cancer or had a positive test outcome. That means, P(A or B) =\\n35/100 = .35 = 35% .\\nAll in all, we have people in the following four different classes:\\nPink : This refers to those people who have cancer and had a negative test\\noutcome\\nPurple ( A intersect B): These people have cancer and had a positive test outcome\\nBlue : This refers to those people with no cancer and a positive test outcome\\nWhite : This refers to those people with no cancer and a negative test outcome\\nSo, effectively, the only times the test was accurate  was in the white and purple regions. In\\nthe blue and pink regions, the test was incorrect.\\n\\nonditional probability\\nLet\\'s pick an arbitrary person from  this study of 100 people. Let\\'s also assume that you are\\ntold that their test result was positive. What is the probability of them actually having\\ncancer? So, we are told that event B has already taken place, and that their test came back\\npositive. The question now is: what is the probability that they have cancer, that is P(A) ?\\nThis is called a conditional probability of A given B or P(A|B) . Effectively, it is asking you\\nto calculate the probability of an event given that another event has already happened.\\nYou can think of conditional probability as changing the relevant universe. P(A|B)  (called\\nthe probability of A given B) is a way of saying, given that my entire universe is now B,\\nwhat is the probability of A? This is also known as transforming the sample space.\\nZooming in on our previous diagram, our universe is now B, and we are concerned with\\nAB (A and B) inside B.\\nThe formula can be given as follows:\\nP(A|B) = P(A and B) / P(B) = (20/100) / (30/100) = 20/30 = .66 = 66%\\nThere is a 66%  chance that if a test result came back positive, that person had cancer. In\\nreality, this is the main probability that the experimenters want. They want to know how\\ngood the test is at predicting cancer.\\nThe rules of probability\\nIn probability, we have some rules  that become very useful when visualization gets too\\ncumbersome. These rules help us calculate compound probabilities with ease.\\nThe addition rule\\nThe addition rule is used to calculate the probability of either or  events. To calculate P(A ∪ B)\\n= P(A or B) , we use the following formula:\\nP(A ∪ B) = P(A) + P(B) − P(A ∩ B)\\n\\nhe first part of the formula (P(A) + P(B))  makes complete sense. To get the union of the\\ntwo events, we have to add together the area of the circles in the universe. But why the\\nsubtraction of P(A and B) ? This is because when we add the two circles, we are adding the\\narea of intersection twice, as shown in the following diagram:\\nSee how both the red circles include the intersection of A and B? So, when we add them, we\\nneed to subtract just one of them to account for this, leaving us with our formula.\\nYou will recall that we wanted the number of people who either had cancer or had a\\npositive test result. If A is the event that someone has cancer, and B is that the test result\\nwas positive, we have the following:\\nP(A or B) = P(A) + P(B) - P(A and B) = .25 + .30 - .2 = .35\\n\\nhis is represented visually in the following diagram:\\nMutual exclusivity\\nWe say that two events  are mutually exclusive if they cannot occur at the same time. This\\nmeans that A∩B=∅, or just that the intersection of the events is the empty set. When this\\nhappens, P(A ∩ B) = P(A and B) = 0 .\\nIf two events are mutually exclusive, then the following applies:\\nP(A ∪ B) = P(A or B)\\n\\u2028= P(A) + P(B) − P(A ∩ B) = P(A) + P(B)\\nThis makes the addition rule much easier. Some examples of mutually exclusive events\\ninclude the following:\\nA customer seeing your site for the first time on both Twitter and Facebook\\nToday is Saturday and today is Wednesday\\nI failed Econ 101 and I passed Econ 101\\nNone of these events can occur simultaneously.\\n\\nhe multiplication rule\\nThe multiplication rule is used  to calculate the probability of and events. To calculate P(A ∩\\nB) = P(A and B) , we use the following formula:\\nP(A ∩ B) = P(A and B) = P(A) P(B|A)\\nWhy do we use B|A instead of B? This is because it is possible that B depends on A. If this\\nis the case, then just multiplying P(A)  and P(B)  does not give us the whole picture.\\nIn our cancer trial example, let\\'s find P(A and B) . To do this, let\\'s redefine A to be the event\\nthat the trial is positive, and B to be the person having cancer (because it doesn\\'t matter\\nwhat we call the events). The equation will be as follows:\\nP(A ∩ B) = P(A and B) = P(A) P(B|A) = .3 * .6666 = .2 = 20%\\nIt\\'s difficult to see the true necessity of using conditional probability, so let\\'s try another,\\nmore difficult problem.\\nFor example, of a randomly selected set of 10 people, 6 have iPhones and 4 have Androids.\\nWhat is the probability that if I randomly select 2 people, they both will have iPhones? This\\nexample can be retold using event spaces, as follows:\\nI have the following two events:\\nA: This event shows the probability that I choose a person with an iPhone first\\nB: This event shows the probability that I choose a person with an iPhone second\\nSo, basically, I want the following:\\nP(A and B) : P(I choose a person with an iPhone and a person with an iPhone )\\nSo, I can use my P(A and B) = P(A) P(B|A)  formula.\\nP(A)  is simple, right? People with iPhones are 6 out of 10, so, I have a 6/10 = 3/5 = 0.6  chance\\nof A. This means P(A) = 0.6 .\\nSo, if I have a 0.6 chance of choosing someone with an iPhone, the probability of choosing\\ntwo should just be 0.6 * 0.6 , right?\\nBut wait! We only have 9 people left to choose our second person from, because one was\\ntaken away. So, in our new transformed sample space, we have 9 people in total, 5 with\\niPhones and 4 with Androids, making P(B) = 5/9 = .555 .\\n\\no, the probability of choosing two people with iPhones is 0.6 * 0.555 = 0.333 = 33% .\\nI have a 1/3 chance of choosing two people with iPhones out of 10. The conditional\\nprobability is very important in the multiplication rule as it can drastically alter your\\nanswer.\\nIndependence\\nTwo events are independent if one event does not affect the outcome of the other, that is\\nP(B|A) = P(B)  and P(A|B) = P(A) .\\nIf two events are independent, then the following applies:\\nP(A ∩ B) = P(A) P(B|A) = P(A) P(B)\\nSome examples of independent events are as follows:\\nIt was raining in San Francisco, and a puppy was born in India\\nI flip a coin and get heads, and I flip another coin and get tails\\nNone of these pairs of events affect each other.\\nComplementary events\\nThe complement of A is the opposite  or negation of A. If A is an event,  Ā represents the\\ncomplement of A. For example, if A is the event where someone has cancer, Ā is the event\\nwhere someone is cancer free.\\nTo calculate the probability of, Ā, use the following formula:\\nP(Ā) = 1 − P(A)\\nFor example, when you throw two dice, what is the probability that you rolled higher than\\na 3?\\nLet A represent rolling higher than a 3.\\nĀ represents rolling a 3 or less.\\nP(A) = 1 − P(Ā)\\nP(A) = l - (P(2)+P(3))\\n\\n 1 - (2/36 + 2/36)\\n= 1 - (4/36)\\n= 32/36 = 8 / 9\\n= .89\\nFor example, a start-up team has three investor meetings coming up. We will have the\\nfollowing probabilities:\\nA 60% chance of getting money from the first meeting\\nA 15% chance of getting money from the second\\nA 45% chance of getting money from the third\\nWhat is the probability of them getting money from at least one meeting?\\nLet A be the team getting money from at least one investor, and, Ā, be the team not getting\\nany money. P(A)  can be calculated as follows:\\nP(A) = 1 − P(Ā)\\nTo calculate P(Ā), we need to calculate the following:\\nP(Ā) = P(no money from investor 1 AND no money from investor 2 AND no money from investor\\n3)\\nLet\\'s assume that these events are independent (they don\\'t talk to each other):\\nP(Ā) = P(no money from investor 1) * P(no money from investor 2) * P(no money from investor 3) =\\n0.4 * 0.85 * 0.55 = 0.187\\nP(A) = 1 - 0.187 = 0.813 = 81%\\nSo, the start-up has an 81% chance of getting money from at least one meeting!\\n\\n bit deeper\\nWithout getting too deep into the machine learning  terminology, this test is what is known\\nas a binary classifier , which means that it is trying to predict from only two options:\\nhaving cancer or not having cancer. When we are dealing with binary classifiers, we can \\ndraw  what is called confusion matrices , which are 2 x 2 matrices that house all the four\\npossible outcomes of our experiment.\\nLet\\'s try some different numbers. Let\\'s say 165 people walked in for the study. So, our n\\n(sample size) is 165 people. All 165 people are given the test and asked whether they  have\\ncancer (provided through various other means). The following confusion matrix  shows us\\nthe results of this experiment:\\nThe matrix shows that 50 people were predicted to have no cancer and did not have it, 100\\npeople were predicted to have cancer and actually did have it, and so on. We have the\\nfollowing four classes, again, all with different names:\\nThe true positives  are the tests correctly predicting positive (cancer) == 100\\nThe true negatives  are the tests correctly predicting negative (no cancer) == 50\\nThe false positives  are the tests incorrectly predicting positive (cancer) == 10\\nThe false negatives  are the tests incorrectly predicting negative (no cancer) == 5\\nThe first two classes indicate where the test was correct, or true . The last two classes\\nindicate where the test was incorrect, or false .\\nFalse positives are sometimes called a Type I error , whereas false negatives are called a\\nType II error :\\n\\nType I and Type II errors\\n(Source: http://marginalrevolution.com/marginalrevolution/2014/05/type-i-and-type-ii-errors-simpliﬁed.html)\\nWe will get into this in the later chapters. For now, we just need to understand why we use\\nthe set notation to denote probabilities for compound events. This is because that\\'s what\\nthey are. When events A and B exist in the same universe, we can use intersections and\\nunions to represent them happening either at the same time,  or to represent one happening\\nversus the other.\\nWe will go into this much more in later chapters, but it is good to introduce it now.\\nSummary\\nIn this chapter, we looked at the basics of probability and will continue to dive deeper into\\nthis field in the following chapter. We approached most of our thinking as a Frequentist,\\nand expressed the basics of experimentation and using probability to predict an outcome.\\nThe next chapter will look at the Bayesian approach to probability and will also explore the\\nuse of probability to solve much more complex problems. We will incorporate these basic\\nprobability principles in much more difficult scenarios.\\n\\n\\nAdvanced Probability\\nIn the previous chapter,  we went over the basics of probability and how we can apply\\nsimple theorems to complex tasks. To briefly summarize, probability is the mathematics of\\nmodeling events that may or may not occur. We use formulas in order to describe  these\\nevents and even look at how multiple events can behave together. In this chapter, we will\\nexplore more complicated theorems  of probability and how we can use them in a predictive\\ncapacity. Advanced topics, such as Bayes\\' theorem  and random variables , give rise to\\ncommon machine learning algorithms, such as the Naïve Bayes algorithm (also covered in\\nthis book). This chapter will focus on some of the more advanced topics  in probability\\ntheory, including the following topics:\\nExhaustive events\\nBayes\\' theorem\\nBasic prediction rules\\nRandom variables\\nWe have one more definition to look at before we get started (the last one before the fun\\nstuff, I promise). We have to look at collectively exhaustive events .\\nCollectively exhaustive events\\nWhen given a set of two or more events, if at least  one of the events must occur, then such a\\nset of events  is said to be collectively exhaustive . Consider the following examples:\\nGiven a set of events {temperature < 60, temperature > 90} , these events\\nare not collectively exhaustive because there is a third option that is not given in\\nthis set of events; the temperature could be between 60 and 90. However, they\\nare mutually exhaustive  because both  cannot happen at the same time.\\nIn a dice roll, the set of events of rolling a {1, 2, 3, 4, 5, or 6}  are\\ncollectively exhaustive  because these are the only possible events, and at least\\none of them must happen.\\n\\nayesian ideas revisited\\nIn the last chapter, we talked, very briefly, about Bayesian ways of thinking. In short, when\\nspeaking about Bayes, you are speaking about the following three things and how they  all\\ninteract with each other:\\nA prior distribution\\nA posterior distribution\\nA likelihood\\nBasically, we are concerned with finding the posterior. That\\'s the thing we want to know.\\nAnother way to phrase the Bayesian way of thinking is that data shapes and updates our\\nbelief. We have a prior probability, or what we naively think about a hypothesis, and then\\nwe have a posterior probability, which is what we think about a hypothesis, given some\\ndata.\\nBayes\\' theorem\\nBayes\\' theorem is the big result of Bayesian  inference. Let\\'s see how it even comes about.\\nRecall that we previously defined the following:\\nP(A) = The probability that event A occurs\\nP(A|B) = The probability that A occurs, given that B occurred\\nP(A, B) = The probability that A and B occur\\nP(A, B) = P(A) * P(B|A)\\nThat last bullet can be read as the probability that A and B occur is the probability that A occurs\\ntimes the probability that B occurred, given that A already occurred .\\nIt\\'s from that last bullet point that Bayes\\' theorem takes its shape.\\nWe know the following:\\nP(A, B) = P(A) * P(B|A)\\nP(B, A) = P(B) * P(A|B)\\nP(A, B) = P(B, A)\\n\\no, we can infer this:\\nP(B) * P(A|B) = P(A) * P(B|A)\\nDividing both sides by P(B)  gives us Bayes\\' theorem, as shown:\\nYou can think of Bayes\\' theorem as follows:\\nIt is a way to get from P(A|B)  to P(B|A)  (if you only have one of them)\\nIt is a way to get P(A|B)  if you already know P(A)  (without knowing B)\\nLet\\'s try thinking about Bayes using the terms hypothesis  and data. Suppose H = your\\nhypothesis about the given data  and D = the data that you are given .\\nBayes can be interpreted as trying to figure out P(H|D)  (the probability that our hypothesis is\\ncorrect, given the data at hand ).\\nLet\\'s use our terminology from before:\\nLet\\'s take a look at that formula:\\nP(H)  is the probability of the hypothesis before we observe the data, called the\\nprior probability , or just prior\\nP(H|D)  is what we want to compute, the probability of the hypothesis after we\\nobserve the data, called the posterior\\nP(D|H)  is the probability of the data under the given hypothesis, called the\\nlikelihood\\nP(D)  is the probability of the data under any hypothesis, called the normalizing\\nconstant\\nThis concept is not far off from the idea of machine learning and predictive analytics. In\\nmany cases, when considering predictive analytics, we use the given data to predict an\\noutcome. Using the current terminology, H (our hypothesis) can be considered our outcome\\nand P(H|D)  (the probability that our hypothesis is true, given our data) is another way of\\nsaying what is the chance that my hypothesis is correct, given the data in front of me?\\n\\net\\'s take a look at an example of how we can use Bayes\\' formula in the workplace.\\nConsider that you have two people in charge of writing blog posts for your company, Lucy\\nand Avinash. From past performance, you have liked 80% of Lucy\\'s work and only 50% of\\nAvinash\\'s work. A new blog post comes to your desk in the morning, but the author isn\\'t\\nmentioned. You love the article. A+. What is the probability that it came from Avinash?\\nEach blogger blogs at a very similar rate.\\nBefore we freak out, let\\'s do what  any experienced mathematician (and now you) would\\ndo. Let\\'s write out all of our information, as shown:\\nH = hypothesis = the blog came from Avinash\\nD = data = you loved the blog post\\nP(H|D)  = the chance that it came from Avinash, given that you loved it\\nP(D|H)  = the chance that you loved it, given that it came from Avinash\\nP(H)  = the chance that an article came from Avinash\\nP(D)  = the chance that you love an article\\nNote that some of these variables make almost no sense without context. P(D) , the\\nprobability that you would love any given article put on your desk, is a weird concept, but\\ntrust me, in the context of Bayes\\' formula, it will be relevant very soon.\\nAlso, note that in the last two items, they assume nothing else. P(D)  does not assume the\\norigin of the blog post; think of P(D)  as if an article was plopped on your desk from some\\nunknown source, what is the chance that you\\'d like it?  (again, I know it sounds weird out of\\ncontext).\\nSo, we want to know P(H|D) . Let\\'s try to use Bayes\\' theorem, as shown here:\\nBut, do we know the numbers on the right-hand side of this equation? I claim we do! Let\\'s\\nsee here:\\nP(H)  is the probability that any given blog post comes from Avinash. As bloggers\\nwrite at a very similar rate, we can assume this is 0.5 because we have a 50/50\\nchance that it came from either blogger (note how I did not assume D, the data,\\nfor this).\\nP(D|H)  is the probability that you love a post from Avinash, which we\\npreviously said was 50%, so, 0.5.\\n\\n(D)  is interesting. This is the chance that you love an article in general . It means\\nthat we must take into account the scenario whether the post came from Lucy or\\nAvinash. Now, if the hypothesis forms a suite, then we can use our laws of\\nprobability, as mentioned in the previous chapter. A suite is formed when a set of\\nhypotheses is both collectively exhaustive and mutually exclusive. In layman\\'s\\nterms, in a suite of events, exactly one and only one hypothesis can occur. In our\\ncase, the two hypotheses are that the article came from Lucy, or that the article\\ncame from Avinash. This is definitely a suite because of the following reasons:\\nAt least one of them wrote it\\nAt most one of them wrote it\\nTherefore, exactly one  of them wrote it\\nWhen we have a suite, we can use our multiplication and addition rules, as follows:\\nD= (From, Avinash AND loved it) OR (From Lucy AND loved it)\\nP(D)= P(Loved AND from Avinash) OR P(Loved AND from Lucy)\\nP(D)= P(From Avinash) P(loved|from Avinash) +P(from Lucy) P(Loved|from Lucy)\\nP(D)=.5(.5)+.5(.8)= .65\\nWhew! Way to go. Now we can finish our equation, as shown:\\nThis means that there is a 38% chance that this article comes from Avinash. What is\\ninteresting is that P(H) = 0.5  and P(H|D) = 0.38 . It means that without any data, the chance\\nthat a blog post came from Avinash was a coin flip, or 50/50. Given some data (your\\nthoughts on the article), we updated our beliefs about the hypothesis and it actually\\nlowered the chance. This is what Bayesian thinking is all about: updating our posterior\\nbeliefs about something from a prior assumption, given some new data about the subject.\\n\\nore applications of Bayes\\' theorem\\nBayes\\' theorem shows up in a lot of applications, usually when we need to make fast\\ndecisions based on data and probability. Most recommendation engines, such as Netflix\\'s,\\nuse some elements of Bayesian updating. And if you think through why that might be, it\\nmakes sense.\\nLet\\'s suppose that in our simplistic world, Netflix only has 10 categories to choose from.\\nNow suppose that given no data, a user\\'s chance of liking a comedy movie out of 10\\ncategories is 10% (just 1/10).\\nOkay, now suppose that the user has given a few comedy movies 5/5 stars. Now, when\\nNetflix is wondering what the chance is that the user would like another comedy, the\\nprobability that they might like a comedy, P(H|D) , is going to be larger than a random\\nguess of 10%!\\nLet\\'s try some more examples of applying Bayes\\' theorem using more data. This time, let\\'s\\nget a bit grittier.\\nExample – Titanic\\nA very famous dataset involves looking at the survivors of the sinking of the Titanic in\\n1912. We will use an application of probability in order to figure out if there were any\\ndemographic features  that showed a relationship to passenger survival. Mainly, we are\\ncurious to see if we can isolate any features of our dataset that can tell us more about the\\ntypes of people who were likely to survive this disaster. First, let\\'s read in the data, as\\nshown here:\\ntitanic =\\npd.read_csv(https://raw.githubusercontent.com/sinanuozdemir/SF_DAT_15/maste\\nr/data/titanic.csv \\')#read in a csv\\ntitanic = titanic[[\\'Sex\\', \\'Survived\\']]  #the Sex and Survived column\\ntitanic.head()\\n\\ne get the following table:\\nIn the preceding table, each row represents a single passenger on the ship, and, for now, we\\nare looking at two specific features: the sex of the individual and whether or not they\\nsurvived the sinking. For example, the first row represents a man who did not survive,\\nwhile the fourth row (with index 3, remember how Python indexes lists) represents a\\nfemale who did survive.\\nLet\\'s start with some basics. We\\'ll start by calculating the probability that any given person\\non the ship survived, regardless of their gender. To do this, we\\'ll count the number of yeses\\nin the Survived  column and divide this figure by the total number of rows, as shown here:\\nnum_rows = float(titanic.shape[0]) # == 891 rows\\np_survived = (titanic.Survived==\"yes\").sum() / num_rows # == .38\\np_notsurvived = 1 - p_survived                          # == .61\\nNote that I only had to calculate P(Survived) , and I used the law of conjugate probabilities to\\ncalculate P(Died)  because those two events are complementary. Now, let\\'s calculate the\\nprobability that any single passenger is male or female:\\np_male = (titanic.Sex==\"male\").sum() / num_rows     # == .65\\np_female = 1 - p_male # == .35\\nNow, let\\'s ask ourselves a question: did having a certain gender affect the survival rate? For\\nthis, we can estimate P(Survived|Female)  or the chance that someone survived given that\\nthey were a female. For this, we need to divide the number of women who survived by the\\ntotal number of women, as shown here:\\n\\n\\nere\\'s the code for that calculation:\\nnumber_of_women = titanic[titanic.Sex==\\'female\\'].shape[0] # == 314\\nwomen_who_lived = titanic[(titanic.Sex==\\'female\\') &\\n(titanic.Survived==\\'yes\\')].shape[0]                       # == 233\\np_survived_given_woman = women_who_lived / float(number_of_women)\\np_survived_given_woman            # == .74\\nThat\\'s a pretty big difference. It seems that gender plays a big part in this dataset.\\nExample  – medical studies\\nA classic use of Bayes\\' theorem is the interpretation of medical trials. Routine testing for\\nillegal drug use is increasingly common in workplaces and schools. The companies that\\nperform these tests maintain that the tests have a high sensitivity, which means that they\\nare likely to produce a positive result if there are drugs in their system . They claim that these\\ntests are also highly specific, which means that they are likely to yield a negative result if\\nthere are no drugs .\\nOn average, let\\'s assume that the sensitivity of common drug tests is about 60% and the\\nspecificity is about 99%. It means that if an employee is using drugs, the test has a 60%\\nchance of being positive, while if an employee is not on drugs, the test has a 99% chance of\\nbeing negative. Now, suppose these tests are applied to a workforce where the actual rate\\nof drug use is 5%.\\nThe real question here is of the people who test positive, how many actually use drugs?\\nIn Bayesian terms, we want to compute the probability of drug use, given a positive test:\\nLet D = the event that drugs are in use\\nLet E = the event that the test is positive\\nLet N = the event that drugs are NOT  in use\\nWe are looking for P(D|E) .\\nBy using Bayes\\' theorem, we can extrapolate it as follows:\\n\\n\\nhe prior, P(D)  is the probability of drug use before we see the outcome of the test, which is\\n5%. The likelihood, P(E|D) , is the probability of a positive test assuming drug use, which is\\nthe same thing as the sensitivity of the test. The normalizing constant, P(E) , is a little bit\\ntrickier.\\nWe have to consider two things: P(E and D)  as well as P(E and N) . Basically, we must\\nassume that the test is capable of being incorrect when the user is not using drugs. Check\\nout the following equations:\\nSo, our original equation becomes as follows:\\nThis means that of the people who test positive for drug use, about a quarter are innocent!\\nRandom variables\\nA random variable  uses real numerical values  to describe a probabilistic event. In our\\nprevious work with variables (both in math and programming), we were used to the fact\\nthat a variable takes on a certain value. For example, we might have a right-angled triangle\\nin which we are given the variable h for the hypotenuse, and we must figure out the length\\nof the hypotenuse. We also might have the following, in Python:\\nx = 5\\nBoth of these variables are equal to one value at a time. In a random variable, we are subject\\nto randomness, which means that our variables\\' values are, well, just that, variable! They\\nmight take on multiple values depending on the environment.\\n\\n random variable still, as shown previously, holds a value. The main distinction between\\nvariables as we have seen them and a random variable is the fact that a random variable\\'s\\nvalue may change depending on the situation.\\nHowever, if a random variable can have many values, how do we keep track of them all?\\nEach value that a random variable might take on is associated with a percentage. For every\\nvalue that a random variable might take on, there is a single probability that the variable\\nwill be this value.\\nWith a random variable, we can also obtain our probability distribution of a random\\nvariable, which gives the variable\\'s possible values and their probabilities.\\nWritten out, we generally use single capital letters (mostly the specific letter X) to denote\\nrandom variables. For example, we might have the following:\\nX: The outcome of a dice roll\\nY: The revenue earned by a company this year\\nZ: The score of an applicant on an interview coding quiz (0-100%)\\nEffectively, a random variable is a function that maps values from the sample space of an\\nevent (the set of all possible outcomes) to a probability value (between 0 and 1). Think\\nabout the event as being expressed as the following:\\nIt will assign a probability to each individual option. There are two main types of random\\nvariables: discrete and continuous.\\nDiscrete random variables\\nA discrete random variable only  takes on a countable number  of possible values, such as\\nthe outcome of a dice roll, as shown here:\\n\\n\\note how I use a capital X to define the random variable. This is a common practice. Also\\nnote how the random variable maps a probability to each individual outcome. Random\\nvariables have  many properties, two of which are their expected value  and the variance . We\\nwill use a probability mass function  (PMF ) to describe a discrete random variable.\\nThey take on the following appearance:\\nP(X = x) = PMF\\nSo, for a dice roll, P(X = 1) = 1/6  and P(X = 5) = 1/6 .\\nConsider the following examples of discrete variables:\\nThe likely result of a survey question (for example, on a scale of 1-10)\\nWhether the CEO will resign within the year (either true or false)\\nThe expected value of a random variable defines the mean value of a long run of repeated\\nsamples of the random variable. This is sometimes called the mean of the variable.\\nFor example, refer to the following Python code, which defines the random variable of a\\ndice roll:\\nimport random\\ndef random_variable_of_dice_roll():\\n    return random.randint(1, 7) # a range of (1,7) # includes 1, 2, 3, 4,\\n5, 6, but NOT 7\\nThis function will invoke a random variable and come out with a response. Let\\'s roll 100\\ndice and average the result, as follows:\\ntrials = []\\nnum_trials = 100\\nfor trial in range(num_trials):\\ntrials.append( random_variable_of_dice_roll() )\\nprint(sum(trials)/float(num_trials))  # == 3.77\\nSo, taking 100 dice rolls and averaging them gives us a value of 3.77 ! Let\\'s try this with a\\nwide variety of trial numbers, as illustrated here:\\nnum_trials = range(100,10000, 10)\\navgs = []\\nfor num_trial in num_trials:\\n    trials = []\\n    for trial in range(1,num_trial):\\n        trials.append( random_variable_of_dice_roll() )\\n    avgs.append(sum(trials)/float(num_trial))\\n\\nlt.plot(num_trials, avgs)\\nplt.xlabel(\\'Number of Trials\\')\\nplt.ylabel(\"Average\")\\nWe get the following graph:\\nThe preceding graph represents the average dice roll as we look at more and more dice\\nrolls. We can see that the average dice roll is rapidly approaching 3.5. If we look towards\\nthe left of the graph, we see that if we only roll a die about 100 times, then we are not\\nguaranteed to get an average dice roll of 3.5. However, if we roll 10,000 dice one after\\nanother, we see that we would very likely expect the average dice roll to be about 3.5.\\nFor a discrete random variable, we can also use a simple formula, shown as follows, to\\ncalculate the expected value:\\nExpected value = E[X]= μx=∑\\nHere, xi is the ith outcome and pi is the ith probability.\\nSo, for our dice roll, we can find the exact expected value as follows:\\n\\n\\nhe preceding result shows us that for any given dice roll, we can \"expect\" a dice roll of 3.5.\\nNow, obviously, that doesn\\'t make sense because we can\\'t get a 3.5 on a dice roll, but it\\ndoes make sense when put in the context of many dice rolls. If you roll 10,000 dice, your\\naverage dice roll should approach 3.5, as shown in the graph and code previously.\\nThe average of the expected value of a random variable is generally not enough to grasp\\nthe full idea behind the variable. For this reason, we introduce a new concept, called\\nvariance.\\nThe variance of a random variable represents the spread of the variable. It quantifies the\\nvariability of the expected value.\\nThe formula for the variance of a discrete random variable is expressed as follows:\\nxi and pi represent the same values as before and represents the expected value of the\\nvariable. In this formula, I also mentioned the sigma of X. Sigma, in this case, is the\\nstandard deviation, which is defined simply as the square root of the variance. Let\\'s look at\\na more complicated example of a discrete random variable.\\nVariance can be thought of like a give or take  metric. If I say you can expect  to win $100 from\\na poker hand, you might be very happy. If I append that statement with the additional\\ndetail that you might win $100, give or take $80, you now have a wide  range of\\nexpectations to deal with, which can be frustrating and might make a risk-averse player\\nmore wary of joining the game. We can usually say that we have  an expected value, give or\\ntake the standard deviation.\\nConsider that your team measures the success  of a new product on a Likert scale , that is, as\\nbeing in one of five categories, where a value of 0 represents a complete failure and 4\\nrepresents a great success. They estimate that a new project has the following chances of\\nsuccess based on user testing and the preliminary results of the performance of the product.\\nWe first have to define our random variable.\\nLet the X random variable represent the success of our product. X is indeed a discrete\\nrandom variable because the X variable can only take on one of five options: 0, 1, 2, 3, or 4.\\n\\nhe following is the probability distribution of our random variable, X. Note how we have\\na column for each potential outcome of X and, following each outcome, we have the\\nprobability that that particular outcome will be achieved:\\nFor example, the project has a 2% chance of failing completely and a 26% chance of being a\\ngreat success! We can calculate our expected value as follows:\\nE[X] = 0(0.02) + 1(0.07) + 2(0.25) + 3(0.4) + 4(0.26) = 2.81\\nThis number means that the manager can expect  a success of about 2.81 out of this project.\\nNow, by itself, that number is not very useful. Perhaps, if given several products to choose\\nfrom, an expected value might be a way to compare the potential successes of several\\nproducts. However, in this case, when we have but the one product to evaluate, we will\\nneed more.\\nNow, let\\'s check the variance, as shown here:\\nVariance= V[X]=σX2 = (xi −μX)2pi = (0 − 2.81)2(0.02) + (1 − 2.81)2(0.07)+ \\u2028(2 − 2.81)2(0.25) + (3 −\\n2.81)2(0.4) + (4 − 2.81)2(0.26) = .93\\nNow that we have both the standard deviation and the expected value of the score of the\\nproject, let\\'s try to summarize our results. We could say that our project will have an\\nexpected score of 2.81 plus or minus 0.96, meaning that we can expect something between\\n1.85 and 3.77.\\nSo, one way we can address this project is that it is probably going to have a success rating\\nof 2.81, give or take about a point.\\nYou might be thinking, wow, Sinan, so at best the project will be a 3.8 and at worst it will be a\\n1.8?. Not quite.\\nIt might be better than a 4 and it might also be worse than a 1.8. To take this one step\\nfurther, let\\'s calculate the following:\\nP(X >= 3)\\n\\nirst, take a minute and convince yourself that you can read that formula to yourself. What\\nam I asking when I am asking for P(X >= 3) ? Honestly, take a minute and figure it out.\\nP(X >= 3)  is the probability that our random variable will take on a value at least as big as 3.\\nIn other words, what is the chance that our product will have a success rating of 3 or\\nhigher? To calculate this, we can calculate the following:\\nP(X >= 3) = P(X = 3) + P(X = 4) = .66 = 66%\\nThis means that we have a 66% chance that our product will rate as either a 3 or a 4.\\nAnother way to calculate this would be the conjugate way, as shown here:\\nP(X >= 3) = 1 - P(X < 3)\\nAgain, take a moment to convince yourself that this formula holds up. I am claiming that to\\nfind the probability that the product will be rated at least a 3 is the same as 1 minus the\\nprobability that the product will receive a rating below 3. If this is true, then the two events\\n(X >=3  and X < 3 ) must complement one another.\\nThis is obviously true! The product can be either of the following two options:\\nBe rated 3 or above\\nBe rated below a 3\\nLet\\'s check our math:\\nP(X < 3) = P(X = 0) + P(X = 1) + P(X = 2)\\n= 0.02 + 0.07 + 0.25\\n= .0341 - P(X < 3)\\n= 1 - .34\\n= .66\\n= P( x >= 3)\\nIt checks out!\\n\\nypes of discrete random variables\\nWe can get a better idea of how random  variables work in practice by looking at specific\\ntypes of random variables. These specific types of random variables model different types\\nof situations and end up revealing much simpler calculations for very complex event\\nmodeling.\\nBinomial random variables\\nThe first type of discrete random  variable we will look at is called a binomial random\\nvariable . With a binomial random  variable, we look at a setting in which a single event\\nhappens over and over and we try to count the number of times the result is positive.\\nBefore we can understand the random variable itself, we must look at the conditions in\\nwhich it is even appropriate.\\nA binomial setting has the following four conditions:\\nThe possible outcomes are either success or failure\\nThe outcomes of trials cannot affect the outcome of another trial\\nThe number of trials was set (a fixed sample size)\\nThe chance of success of each trial must always be p\\nA binomial random variable is a discrete random variable, X, that counts the number of\\nsuccesses in a binomial setting. The parameters are n = the number of trials  and p = the chance\\nof success of each trial .\\nExample: Fundraising meetings\\nA start-up is taking 20 VC meetings to fund and count the number of offers they receive.\\nThe PMF  for a binomial random  variable is as follows:\\n\\n\\nxample: Restaurant openings\\nA new restaurant in a town has a 20% chance of surviving its first year. If 14 restaurants\\nopen this year, find the probability that exactly four restaurants survive their first year of\\nbeing open to the public.\\nFirst, we should prove that this is a binomial setting:\\nThe possible outcomes are either success or failure (the restaurants either survive\\nor not)\\nThe outcomes of trials cannot affect the outcome of another trial (assume that the\\nopening of one restaurant doesn\\'t affect another restaurant\\'s opening and\\nsurvival)\\nThe number of trials was set (14 restaurants opened)\\nThe chance of success of each trial must always be p (we assume that it is always\\n20%)\\nHere, we have our two parameters of n = 14  and p = 0.2 . So, we can now plug these numbers\\ninto our binomial formula, as shown here:\\nSo, we have a 17% chance that exactly 4 of these restaurants will be open after a year.\\nExample: Blood types\\nA couple has a 25% chance of a having a child with type O blood. What is the chance that\\nthree of their five kids have type O blood?\\nLet X = the number of children with type O blood  with n = 5  and p = 0.25 , as shown here:\\nWe can calculate this probability for the values of 0, 1, 2, 3, 4, and 5 to get a sense of the\\nprobability distribution:\\n\\n\\nrom here, we can calculate  an expected value  and the variance of this variable:\\nSo, this family can expect to have probably one or two kids with type O blood!\\nWhat if we want to know the probability that at least three of their kids have type O blood?\\nTo know the probability that at least three of their kids have type O blood, we can use the\\nfollowing formula for discrete random variables:\\nP(x>=3) =P(X=3)+P(X=4)+P(X=3) = .00098+.01465+.08789=0.103\\nSo, there is about a 10% chance that three of their kids have type O blood.\\nShortcuts to binomial expected value and variance\\nBinomial random variables have special calculations for the exact values\\nof the expected values and variance. If X is a binomial random variable,\\nthen we get the following:\\nE(X) = np\\nV(X) = np(1 − p)\\nFor our preceding example, we can use the following formulas to calculate\\nan exact expected value and variance:\\nE(X) = .25(5) = 1.25\\nV(X) = 1.25(.75) = 0.9375\\nA binomial random variable is a discrete random variable that counts the number of\\nsuccesses in a binomial setting. It is used in a wide variety of data-driven experiments, such\\nas counting the number of people who will sign up for a website given a chance of\\nconversion, or even, at a simple level, predicting stock price movements given a chance of\\ndecline (don\\'t worry; we will be applying much more sophisticated models to predict the\\nstock market later).\\nGeometric random variables\\nThe second discrete random variable we will take a look at is called a geometric random\\nvariable. It is actually quite similar to the binomial random variable in that we are\\nconcerned with a setting in which a single event is occurring over and over. However, in\\nthe case of a geometric setting, the major difference is that we are not fixing the sample size.\\n\\ne are not going into exactly 20 VC meetings as a start-up, nor are we having exactly five\\nkids. Instead, in a geometric setting, we are modeling the number of trials we will need to\\nsee before we obtain even a single success. Specifically, a geometric setting has the\\nfollowing four conditions:\\nThe possible outcomes are either success or failure\\nThe outcomes of trials cannot affect the outcome of another trial\\nThe number of trials was not set\\nThe chance of success of each trial must always be p\\nNote that these are the exact same conditions as a binomial variable, except the third\\ncondition.\\nA geometric random variable  is a discrete  random variable, X, that counts the number of\\ntrials needed to obtain one success. The parameters are p = the chance of success of each trial\\nand (1 − p) = the chance of failure of each trial .\\nTo transform the previous binomial examples into geometric examples, we might do the\\nfollowing:\\nCount the number of VC meetings that a start-up must take in order to get their\\nfirst yes\\nCount the number of coin flips needed in order to get a head (yes, I know it\\'s\\nboring, but it\\'s a solid example!)\\nThe formula for the PMF is as follows:\\nBoth the binomial and geometric settings involve outcomes that are either successes or\\nfailures. The big difference is that binomial random variables have a fixed number of trials,\\ndenoted as n. Geometric random variables do not have a fixed number of trials. Instead,\\ngeometric random variables model the number of samples needed in order to obtain the\\nfirst successful trial, whatever success might mean in those experimental conditions.\\nExample: Weather\\nThere is a 34% chance that it will rain on any day in April. Find the probability that the first\\nday of rain in April will occur on April 4.\\nLet X = the number of days until it rains  (success) with p = 0.34  and (1 − p) = 0.66\\n\\nets\\' work it out: \\n= 0.054\\nThe probability that it will rain by the April 4 is as follows:\\nP(X<=4) = P(1) + P(2) + P(3) + P(4) = .34 + .22 + .14 +>1 = .8\\nSo, there is an 80% chance that the first rain of the month will happen within the first four\\ndays.\\nShortcuts to geometric expected value and variance\\nGeometric random variables also have special calculations for the exact\\nvalues of the expected values and variance. If X is a geometric random\\nvariable, then we get the following:\\nE(X) = 1/p\\nPoisson random variable\\nThe third and last specific example of a discrete  random variable is a Poisson random\\nvariable.\\nTo understand why we would need  this random variable, imagine that an event that we\\nwish to model has a small probability of happening and that we wish to count the number\\nof times that the event occurs in a certain time frame. If we have an idea of the average\\nnumber of occurrences, µ, over a specific period of time, given from past instances, then the\\nPoisson random variable, denoted by X = Poi( µ), counts the total number of occurrences of\\nthe event during that given time period.\\nIn other words, the Poisson distribution is a discrete probability distribution that counts the\\nnumber of events that occur in a given interval of time.\\nConsider the following examples of Poisson random variables:\\nFinding the probability of having a certain number of visitors on your site within\\nan hour, knowing the past performance of the site\\nEstimating the number of cars crashes at an intersection based on past police\\nreports\\n\\nf we let X = the number of events in a given interval , and the average number of events per\\ninterval is the λ number, then the probability of observing x events in a given interval is\\ngiven by the following formula:\\nHere, e = Euler\\'s constant (2.718....) .\\nExample: Call center\\nThe number of calls arriving at your call center follows a Poisson distribution at the rate of\\nfive calls per hour. What is the probability that exactly six calls will come in between 10 and\\n11 p.m.?\\nTo set up this example, let\\'s write out our given information. Let X be the number of calls\\nthat arrive between 10 and 11 p.m. This is our Poisson random variable with the mean λ = 5.\\nThe mean is 5 because we are using 5 as our previous expected value of the number of calls\\nto come in at this time. This number could have come from the previous work on\\nestimating the number of calls that come in every hour or specifically that come in after 10\\np.m. The main idea is that we do have some idea of how many calls should be coming in,\\nand then we use that information to create our Poisson random variable and use it to make\\npredictions.\\nContinuing with our example, we have the following:\\nP(X = 6) = 0.146\\nThis means that there is about a 14.6% chance that exactly six calls will come between 10\\nand 11 p.m.\\nShortcuts to Poisson expected value and variance\\nPoisson random variables also have special calculations for the exact\\nvalues of the expected values and variance. If X is a Poisson random\\nvariable with mean, then we get the following:\\nE(X) = λ\\nV(X) = λ\\nThis is actually interesting because both the expected value and the variance are the same\\nnumber, and that number is simply the given parameter! Now that we\\'ve seen three\\nexamples of discrete random variables, we must take a look at the other type of random\\nvariable, called the continuous random variable.\\n\\nontinuous random variables\\nSwitching gears entirely, unlike a discrete  random variable, a continuous random variable\\ncan take on an infinite  number of possible values, not just a few countable ones. We call the\\nfunctions that describe the distribution density curves instead  of PMF .\\nConsider the following examples of continuous variables:\\nThe length of a sales representative\\'s phone call (not the number of calls)\\nThe actual amount of oil in a drum marked 20 gallons (not the number of oil\\ndrums)\\nIf X is a continuous random variable, then there is a function, f(x), for any constants a and b:\\nThe preceding f(x) function is known  as the probability density function  (PDF ). The PDF is\\nthe continuous random variable version of the PMF for discrete random variables.\\nThe most important continuous  distribution is the standard normal distribution . You have,\\nno doubt, either heard of the normal distribution or dealt with it. The idea behind it is quite\\nsimple. The PDF of this distribution is as follows:\\nHere, μ is the mean of the variable and σ is the standard deviation. This might look\\nconfusing, but let\\'s graph it in Python with a mean of 0 and a standard deviation of 1, as\\nshown here:\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\ndef normal_pdf(x, mu = 0, sigma = 1):\\n return (1./np.sqrt(2*3.14 * sigma**2)) * np.exp((-(x-mu)**2 / (2.*\\nsigma**2)))\\nx_values = np.linspace(-5,5,100)\\ny_values = [normal_pdf(x) for x in x_values]\\nplt.plot(x_values, y_values)\\n\\ne get this graph:\\nThis gives rise to the all-too-familiar bell curve. Note that the graph is symmetrical around\\nthe x = 0  line. Let\\'s try changing some of the parameters. First, let\\'s try with :\\n\\n\\next, let\\'s try with the value :\\nLastly, we will try with the values :\\n\\n\\nn all the graphs, we have the standard bell shape  that we are all familiar with, but as we\\nchange our parameters, we see that the bell might get skinnier, thicker, or move from left to\\nright.\\nIn the following chapters, which focus on statistics, we will make much more use of the\\nnormal distribution as it applies to statistical thinking.\\nSummary\\nProbability as a field works to explain our random and chaotic world. Using the basic laws\\nof probability, we can model real-life events that involve randomness. We can use random\\nvariables to represent values that may take on several values, and we can use the\\nprobability mass or density functions to compare product lines or look at the test results.\\nWe have seen some of the more complicated uses of probability in prediction. Using\\nrandom variables and Bayes\\' theorem are excellent ways to assign probabilities to real-life\\nsituations. In later chapters, we will revisit Bayes\\' theorem and use it to create a very\\npowerful and fast machine learning algorithm, called the Na ïve Bayes algorithm. This\\nalgorithm captures the power of Bayesian thinking and applies it directly to the problem of\\npredictive learning.\\nThe next two chapters are focused on statistical thinking. Like probability, these chapters\\nwill use mathematical formulas to model real-world events. The main difference, however,\\nwill be the terminology we use to describe the world and the way we model different types\\nof events. In these upcoming chapters, we will attempt to model entire populations of data\\npoints based solely on a sample.\\nWe will revisit many concepts in probability to make sense of statistical theorems as they\\nare closely linked, and both are important mathematical concepts in the realm of data\\nscience.\\n\\n\\nBasic Statistics\\nThis chapter will focus on the statistical knowledge required by any aspiring data scientist.\\nWe will explore ways of sampling and obtaining data without being affected by bias and\\nthen use measures of statistics to quantify and visualize our data. Using the z-score and the\\nempirical rule, we will see how we can standardize data for the purpose of both graphing\\nand interpretability.\\nIn this chapter, we will look at the following topics:\\nHow to obtain and sample data\\nThe measures of center, variance, and relative standing\\nNormalization of data using the z-score\\nThe empirical rule\\nWhat are statistics?\\nThis might seem like an odd question to ask, but I am frequently surprised by the number\\nof people who cannot answer this simple and yet powerful question: what are statistics?\\nStatistics are the numbers you always see on the news and in the paper. Statistics are useful\\nwhen trying to prove a point or trying to scare someone, but what are they?\\nTo answer this question, we need to back up for a minute and talk about why we even\\nmeasure them in the first place. The goal of this field is to try to explain and model the\\nworld around us. To do that, we have to take a look at the population.\\nWe can define a population  as the entire pool of subjects of an experiment or a model.\\n\\nssentially, your population is who you care about. Who are you trying to talk about? If\\nyou are trying to test whether smoking leads to heart disease, your population would be\\nthe smokers of the world. If you are trying to study teenage drinking problems, your\\npopulation would be all teenagers.\\nNow, consider that you want to ask a question about your population. For example, if your\\npopulation is all of your employees (assume that you have over 1,000 employees), perhaps\\nyou want  to know what percentage of them use illicit drugs. The question is called a\\nparameter .\\nWe can define a parameter as a numerical measurement describing a characteristic of a\\npopulation.\\nFor example, if you ask all 1,000 employees and 100 of them are using drugs, the rate of\\ndrug use is 10%. The parameter here is 10%.\\nHowever, let\\'s get real; you probably can\\'t ask every single employee whether they are\\nusing drugs. What if you have over 10,000 employees? It would be very difficult to track\\neveryone down in order to get your answer. When this happens, it\\'s impossible to figure\\nout this parameter. In this case, we can estimate  the parameter.\\nFirst, we will take a sample  of the population.\\nWe can define a sample of a population as a subset (not necessarily random) of the\\npopulation.\\nSo, we perhaps ask 200 of the 1,000 employees you have. Of these 200, suppose 26 use\\ndrugs, making the drug use rate 13%. Here, 13% is not a parameter because we didn\\'t get a\\nchance to ask everyone. This 13% is an estimate of a parameter. Do you know what that\\'s\\ncalled?\\nThat\\'s right, a statistic !\\nWe can define a statistic as a numerical measurement describing a characteristic of a sample\\nof a population.\\nA statistic is just an estimation of a parameter. It is a number that attempts to describe an\\nentire population by describing a subset of that population. This is necessary because you\\ncan never hope to give a survey to every single teenager or to every single smoker in the\\nworld. That\\'s what the field of statistics is all about: taking samples of populations and\\nrunning tests on these samples.\\nSo, the next time you are given a statistic, just remember that number only represents a\\nsample of that population, not the entire pool of subjects.\\n\\now do we obtain and sample data?\\nIf statistics is about taking samples  of populations, it must be very important to know how\\nwe obtain these samples, and you\\'d be correct. Let\\'s focus on just a few of the many ways  of\\nobtaining and sampling data.\\nObtaining data\\nThere are two main ways of collecting data for our analysis: observational  and\\nexperimentation . Both these ways  have their pros and cons, of course. They each produce\\ndifferent types of behavior and, therefore, warrant different types of analysis.\\nObservational\\nWe might obtain data through observational  means, which consists of measuring specific\\ncharacteristics but not attempting to modify the subjects being studied. For example, you\\nhave a tracking software on your website that observes users\\' behavior on the website, such\\nas length of time spent on certain pages and the rate of clicking  on ads, all the while not\\naffecting the user\\'s experience, then that would be an observational study.\\nThis is one of the most common ways to get data because it\\'s just plain easy. All you have\\nto do is observe and collect data. Observational studies are also limited in the types of data\\nyou may collect. This is because the observer (you) is not in control of the environment. You\\nmay only watch and collect natural behavior. If you are looking to induce a certain type of\\nbehavior, an observational study would not be useful.\\nExperimental\\nAn experiment  consists of a treatment and the observation of its effect on the subjects.\\nSubjects in an experiment are called experimental units. This is usually how most scientific\\nlabs collect data. They will put people into two or more groups (usually just two) and call\\nthem the control and the experimental group.\\nThe control group is exposed to a certain environment and then observed. The\\nexperimental group is then exposed to a different environment and then observed. The\\nexperimenter then aggregates data from both the groups and makes a decision about which\\nenvironment was more favorable (favorable is a quality that the experimenter gets to\\ndecide).\\n\\nn a marketing example, consider that we expose half of our users to a certain landing page\\nwith certain images and a certain style (website A), and we measure whether or not they\\nsign up for the service. Then, we expose the other half to a different landing page, different\\nimages, and different styles (website B) and again measure whether or not they sign up. We\\ncan then decide which of the two sites performed better and should be used going further.\\nThis, specifically, is called an A/B test . Let\\'s see an example in Python! Let\\'s suppose we run\\nthe preceding test and obtain the following results as a list of lists:\\nresults = [ [\\'A\\', 1], [\\'B\\', 1], [\\'A\\', 0], [\\'A\\', 0] ... ]\\nHere, each object in the list result represents a subject (person). Each person then has the\\nfollowing two attributes:\\nWhich website they were exposed to, represented by a single character\\nWhether or not they converted ( 0 for no and 1 for yes)\\nWe can then aggregate and come up with the following results table:\\nusers_exposed_to_A = []\\nusers_exposed_to_B = []\\n# create two lists to hold the results of each individual website\\nOnce we create these two lists that will eventually hold each individual conversion Boolean\\n(0 or 1), we will iterate all of our results of the test and add them to the appropriate list, as\\nshown:\\nfor website, converted in results: # iterate through the results\\n  # will look something like website == \\'A\\' and converted == 0\\n  if website == \\'A\\':\\n    users_exposed_to_A.append(converted)\\n  elif website == \\'B\\':\\n    users_exposed_to_B.append(converted)\\nNow, each list contains a series of 1s and 0s.\\nRemember that a 1 represents a user actually converting to the site after\\nseeing that web page, and a 0 represents a user seeing the page and\\nleaving before signing up/converting.\\nTo get the total number of people exposed to website A, we can use the len()  feature in\\nPython, as illustrated:\\nlen(users_exposed_to_A) == 188 #number of people exposed to website A\\nlen(users_exposed_to_B) == 158 #number of people exposed to website B\\n\\no count the number of people who converted, we can use the sum()  of the list, as shown:\\nsum(users_exposed_to_A) == 54 # people converted from website A\\nsum(users_exposed_to_B) == 48 # people converted from website B\\nIf we subtract the length of the lists and the sum of the list, we are left with the number of\\npeople who did not convert for each site, as illustrated:\\nlen(users_exposed_to_A) - sum(users_exposed_to_A) == 134 # did not convert\\nfrom website A\\nlen(users_exposed_to_B) - sum(users_exposed_to_B) == 110 # did not convert\\nfrom website B\\nWe can aggregate and summarize our results in the following table, which represents our\\nexperiment of website conversion testing:\\nDid not sign up Signed up\\nWebsite A 134 54\\nWebsite B 110 48\\nThe results of our A/B test\\nWe can quickly drum up some descriptive statistics. We can say that the website conversion\\nrates for the two websites are as follows:\\nConversion for website A : 154 /(154+34) = .288\\nConversion for website B : 48/(110+48)= .3\\nNot much difference, but different nonetheless. Even though B has the higher conversion\\nrate, can we really say that version B significantly converts better? Not yet. To test the\\nstatistical significance  of such a result, a hypothesis test should be used. These tests will be\\ncovered in depth in the next chapter, where we will revisit this exact same example and\\nfinish it using a proper statistical test.\\nSampling data\\nRemember how statistics are the result of measuring a sample of a population. Well, we\\nshould talk about two very common ways to decide who gets the honor of being in the\\nsample that we measure. We will discuss the main type of sampling, called random\\nsampling, which is the most common way to decide our sample sizes and our sample\\nmembers.\\n\\nrobability sampling\\nProbability sampling is a way  of sampling from a population, in which every person has a\\nknown probability of being chosen but that number might  be a different probability than\\nanother user. The simplest (and probably the most common) probability sampling method\\nis a random sampling.\\nRandom sampling\\nSuppose that we are running an A/B test and we need to figure out who will be in group A\\nand who  will be in group B. There are the following three suggestions from your data team:\\nSeparate users based on location : Users on the West coast are placed in group A,\\nwhile users on the East coast are placed in group B\\nSeparate users based on the time of day they visit the site : Users who visit\\nbetween 7 p.m. and 4 a.m. are group A, while the rest are placed in group B\\nMake it completely random : Every new user has a 50/50 chance of being placed\\nin either group\\nThe first two are valid options for choosing samples and are fairly simple to implement, but\\nthey both have one fundamental flaw: they are both at risk of introducing a sampling bias.\\nA sampling bias  occurs when the way  the sample is obtained systemically favors some\\noutcome over the target outcome.\\nIt is not difficult to see why choosing option 1 or option 2 might introduce bias. If we chose\\nour groups based on where they live or what time they log in, we are priming our\\nexperiment incorrectly and, now, we have much less control over the results.\\nSpecifically, we are at risk of introducing a confounding factor  into our analysis, which is bad\\nnews.\\nA confounding factor  is a variable that we are not directly measuring but connects the\\nvariables that are being measured.\\nBasically, a confounding factor is like the missing element in our analysis that is invisible\\nbut affects our results.\\nIn this case, option 1 is not taking into account the potential confounding factor of\\ngeographical taste . For example, if website A is unappealing, in general, to West coast users,\\nit will affect your results drastically.\\n\\nimilarly, option 2 might introduce a temporal (time-based) confounding factor. What if\\nwebsite B is better viewed in a night-time environment (which was reserved for A), and\\nusers are reacting negatively to the style purely because of what time it is? These are both\\nfactors that we want to avoid, so, we should go with option 3, which is a random sample.\\nWhile sampling bias can cause confounding, it is a different concept than\\nconfounding. Options 1 and 2 were both sampling biases because we\\nchose the samples incorrectly and were also examples of confounding\\nfactors because there was a third variable in each case that affected our\\ndecision.\\nA random sample is chosen such that every single member of a population has an equal\\nchance of being chosen as any other member.\\nThis is probably one of the easiest and most convenient ways to decide who will be a part\\nof your sample. Everyone has the exact same chance of being in any particular group.\\nRandom sampling is an effective way of reducing the impact of confounding factors.\\nUnequal probability sampling\\nRecall that I previously said that a probability sampling might have different probabilities\\nfor different potential sample members. But what if this actually introduced problems?\\nSuppose we are interested in measuring the happiness level of our employees. We already\\nknow that we can\\'t ask every single member of staff because that would be silly and\\nexhausting. So, we need to take a sample. Our data team suggests random sampling, and at\\nfirst everyone high-fives because they feel very smart and statistical. But then someone asks\\na seemingly harmless question: does anyone know the percentage of men/women who\\nwork here?\\nThe high fives stop and the room goes silent.\\nThis question is extremely important because sex is likely to be a confounding factor. The\\nteam looks into it and discovers a split of 75% men and 25% women in the company.\\nThis means that if we introduce a random sample, our sample will likely have a similar\\nsplit and thus favor the results for men and not women. To combat this, we can favor\\nincluding more women than men in our survey in order to make the split of our sample less\\nfavored for men.\\n\\nt first glance, introducing a favoring system in our random sampling seems like a bad\\nidea; however, alleviating unequal sampling and, therefore, working to remove systematic\\nbias among gender, race, disability, and so on is much more pertinent. A simple random\\nsample, where everyone has the same chance as everyone else, is very likely to drown out\\nthe voices and opinions of minority population members. Therefore, it can be okay to\\nintroduce such a favoring system in your sampling techniques.\\nHow do we measure statistics?\\nOnce we have our sample, it\\'s time to quantify  our results. Suppose we wish to generalize\\nthe happiness of our employees or we want to figure out whether salaries in the company\\nare very different from person to person.\\nThese are some common ways of measuring our results.\\nMeasures of center\\nMeasures of the center are how we define  the middle, or center, of a dataset. We do this\\nbecause sometimes we wish to make generalizations about data values. For example,\\nperhaps we\\'re curious about what the average rainfall in Seattle is or what the median\\nheight of European males is. It\\'s a way to generalize a large set of data so that it\\'s easier to\\nconvey to someone.\\nA measure of center is a value in the middle  of a dataset.\\nHowever, this can mean different things to different people. Who\\'s to say where the middle\\nof a dataset is? There are so many different ways of defining the center of data. Let\\'s take a\\nlook at a few.\\nThe arithmetic mean  of a dataset is found  by adding up all of the values and then dividing\\nit by the number of data values.\\nThis is likely the most common way to define the center of data, but can be flawed! Suppose\\nwe wish to find the mean of the following numbers:\\nimport numpy as np\\nnp.mean([11, 15, 17, 14]) == 14.25\\n\\nimple enough; our average is 14.25  and all of our values are fairly close to it. But what if\\nwe introduce a new value: 31?\\nnp.mean([11, 15, 17, 14, 31]) == 17.6\\nThis greatly affects the mean because the arithmetic mean is sensitive to outliers. The new\\nvalue, 31, is almost twice as large as the rest of the numbers and therefore skews  the mean.\\nAnother, and sometimes better, measure of center is the median.\\nThe median  is the number found  in the middle of the dataset when it is sorted in order, as\\nshown:\\nnp.median([11, 15, 17, 14]) == 14.5\\nnp.median([11, 15, 17, 14, 31]) == 15\\nNote how the introduction of 31 using the median did not affect the median of the dataset\\ngreatly. This is because the median is less sensitive to outliers.\\nWhen working with datasets with many outliers, it is sometimes more useful to use the\\nmedian of the dataset, while if your data does not have many outliers and the data points\\nare mostly close to one another, then the mean is likely a better option.\\nBut how can we tell if the data is spread out? Well, we will have to introduce a new type of\\nstatistic.\\nMeasures of variation\\nMeasures of the center are used  to quantify the middle of the data, but now we will explore\\nways of measuring how to spread out  the data we collect is. This is a useful way to identify if\\nour data has many outliers lurking inside. Let\\'s start with an example.\\nConsider that we take a random sample of 24 of our friends on Facebook and wrote down\\nhow many friends that they had on Facebook. Here\\'s the list:\\nfriends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\\n455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\\nnp.mean(friends) == 789.1\\n\\nhe average of this list is just over 789. So, we could say that according to this sample, the\\naverage Facebook friend has 789 friends. But what about the person who only has 89\\nfriends or the person who has over 1,600 friends? In fact, not a lot of these numbers are\\nreally that close to 789.\\nWell, how about we use the median? The median generally is not as affected by outliers:\\nnp.median(friends) == 769.5\\nThe median is 769.5 , which is fairly close to the mean. Hmm, good thought, but still, it\\ndoesn\\'t really account for how drastically different a lot of these data points are to one\\nanother. This is what statisticians call measuring the variation of data. Let\\'s start by\\nintroducing the most basic measure of variation: the range. The range is simply the\\nmaximum value minus the minimum value, as illustrated:\\nnp.max(friends) - np.min(friends) == 1684\\nThe range tells us how far away the two most extreme values are. Now, typically the range\\nisn\\'t widely used but it does have its use in the application. Sometimes, we wish to just\\nknow how spread apart the outliers are. This is most useful in scientific measurements or\\nsafety measurements.\\nSuppose a car company wants to measure how long it takes for an airbag to deploy.\\nKnowing the average of that time is nice, but they also really want to know how spread\\napart the slowest time is versus the fastest time. This literally could be the difference\\nbetween life and death.\\nShifting back to the Facebook example, 1,684 is our range, but I\\'m not quite sure it\\'s saying\\ntoo much about our data. Now, let\\'s take a look at the most commonly used measure  of\\nvariation, the standard deviation .\\nI\\'m sure many of you have heard this term thrown around a lot and it might even incite a\\ndegree of fear, but what does it really mean? In essence, standard deviation, denoted by s\\nwhen we are working with a sample of a population, measures how much data values\\ndeviate from the arithmetic mean.\\nIt\\'s basically a way to see how spread out the data is. There is a general formula to calculate\\nthe standard deviation, which is as follows:\\n\\n\\net\\'s look at each of the elements in this formula in turn:\\ns is our sample\\'s standard deviation\\nx is each individual data point\\nis the mean of the data\\nn is the number of data points\\nBefore you freak out, let\\'s break it down. For each value in the sample, we will take that\\nvalue, subtract the arithmetic mean from it, square the difference, and, once we\\'ve added\\nup every single point this way, we will divide the entire thing by n, the number of points in\\nthe sample. Finally, we take a square root of everything.\\nWithout going into an in-depth analysis of the formula, think about it this way: it\\'s basically\\nderived from the distance formula. Essentially, what the standard deviation is calculating is\\na sort of average distance of how far the data values are from the arithmetic mean.\\nIf you take a closer look at the formula, you will see that it actually makes sense:\\nBy taking x-, you are finding the literal difference between the value and the\\nmean of the sample.\\nBy squaring the result, , we are putting a greater penalty on outliers\\nbecause squaring a large error only makes it much larger.\\nBy dividing by the number of items in the sample, we are taking (literally) the\\naverage squared distance between each point and the mean.\\nBy taking the square root of the answer, we are putting the number in terms that\\nwe can understand. For example, by squaring the number of friends minus the\\nmean, we changed our units to friends squared, which makes no sense. Taking\\nthe square root puts our units back to just \"friends.\"\\nLet\\'s go back to our Facebook example for a visualization and further explanation of this.\\nLet\\'s begin to calculate the standard deviation. So, we\\'ll start calculating a few of them.\\nRecall that the arithmetic mean of the data was just about 789, so, we\\'ll use 789 as the mean.\\nWe start by taking the difference between each data value and the mean, squaring it,\\nadding them all up, dividing it by one less than the number of values, and then taking its\\nsquare root. This would look as follows:\\n \\n\\n\\nn the other hand, we can take the Python approach and do all this programmatically\\n(which is usually preferred):\\nnp.std(friends) # == 425.2\\nWhat the number 425 represents is the spread of data. You could say that 425 is a kind of\\naverage distance the data  values are from the mean. What this means, in simple words, is\\nthat this data is pretty spread out.\\nSo, our standard deviation is about 425. This means that the number of friends that these\\npeople have on Facebook doesn\\'t seem to be close to a single number and that\\'s quite\\nevident when we plot the data in a bar graph, and also graph the mean as well as the\\nvisualizations of the standard deviation. In the following plot, every person will be\\nrepresented by a single bar in the bar chart, and the height of the bars represent the number\\nof friends that the individuals have:\\nimport matplotlib.pyplot as plt\\nfriends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\\n455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\\ny_pos = range(len(friends))\\nplt.bar(y_pos, friends)\\nplt.plot((0, 25), (789, 789), \\'b-\\')\\nplt.plot((0, 25), (789+425, 789+425), \\'g-\\')\\nplt.plot((0, 25), (789-425, 789-425), \\'r-\\')\\nHere\\'s the chart that we get:\\n\\n\\nhe blue line in the center is drawn at the mean (789), the red line near the bottom is drawn\\nat the mean minus the standard deviation (789-425 = 364), and finally the green line\\ntowards the top is drawn at the mean plus the standard deviation (789+425 = 1,214).\\nNote how most  of the data lives between the green and the red lines while the outliers live\\noutside the lines. There are three people who have friend counts below the red line and\\nthree people who have a friend count above the green line.\\nIt\\'s important to mention that the units for standard deviation are, in fact, the same units as\\nthe data\\'s units. So, in this example, we would say that the standard deviation is 425 friends\\non Facebook.\\nAnother measure of variation is the variance, as described in the previous\\nchapter. The variance is simply the standard deviation squared.\\nSo, now we know that the standard deviation and variance is good for checking how\\nspread out our data is, and that we can use it along with the mean to create a kind of range\\nthat a lot of our data lies in. But what if we want to compare the spread of two different\\ndatasets, maybe even with completely different units? That\\'s where the coefficient of\\nvariation comes into play.\\nDefinition\\nThe coefficient of variation  is defined as the ratio of the data\\'s standard deviation to its\\nmean.\\nThis ratio (which, by the way, is only helpful if we\\'re working in the ratio level of\\nmeasurement, where the division is allowed  and is meaningful) is a way to standardize the \\nstandard  deviation, which makes it easier to compare across datasets. We use this measure\\nfrequently when attempting to compare means, and it spreads across populations that exist\\nat different scales.\\n\\nxample – employee salaries\\nIf we look at the mean and standard  deviation of employees\\' salaries in the same company\\nbut among different departments, we see that, at first glance, it may be tough to compare\\nvariations:\\nThis is especially true when the mean salary of one department is $25,000 , while another\\ndepartment has a mean salary in the six-figure area.\\nHowever, if we look at the last column, which is our coefficient of variation, it becomes\\nclearer that the people in the executive department may be getting paid more but they are\\nalso getting wildly different salaries. This is probably because the CEO is earning way more\\nthan an office manager, who is still in the executive department, which makes the data very\\nspread out.\\nOn the other hand, everyone in the mailroom, while not making as much money, is making\\njust about the same as everyone else in the mailroom, which is why their coefficient of\\nvariation is only 8%.\\nWith measures of variation, we can begin to answer big questions, such as how to spread\\nout this data is or how we can come up with a good range that most of the data falls into.\\nMeasures of relative standing\\nWe can combine both  the measures of centers and variations to create measures of relative\\nstanding. Measures of variation  measure where particular data values are positioned,\\nrelative to the entire dataset.\\n\\net\\'s begin by learning a very  important value  in statistics, the z-score. The z-score  is a way\\nof telling us how far away a single data value is from the mean. The z-score of an x data\\nvalue is as follows:\\n                                                                         z = \\nLet\\'s break down this formula:\\nX is the data point\\nthe mean\\ns is the standard deviation\\nRemember that the standard deviation was (sort of) an average distance that the data is\\nfrom the mean and now the z-score is an individualized value for each particular data\\npoint. We can find the z-score of a data value by subtracting it from the mean and dividing\\nit by the standard deviation. The output will be the standardized distance a value is from a\\nmean. We use the z-score all over statistics. It is a very effective way of normalizing data\\nthat exists on very different scales, and also to put data in the context of its mean.\\nLet\\'s take our previous data on the number of friends on Facebook and standardize the data\\nto the z-score. For each data point, we will find its z-score by applying the preceding\\nformula. We will take each individual, subtract the average friends from the value, and\\ndivide that by the standard deviation, as shown:\\nz_scores = []\\nm = np.mean(friends)  # average friends on Facebook\\ns = np.std(friends)   # standard deviation friends on Facebook\\nfor friend in friends:\\n    z = (friend - m)/s  # z-score\\n    z_scores.append(z)  # make a list of the scores for plotting\\nNow, let\\'s plot these z-scores on a bar chart. The following chart shows the same\\nindividuals from our previous example using friends on Facebook, but instead of the bar\\nheight revealing the raw number of friends, now each bar is the z-score of the number of\\nfriends they have on Facebook. If we graph the z-scores, we\\'ll notice a few things:\\nplt.bar(y_pos, z_scores)\\n\\ne get this chart:\\nWe can see that we have negative values (meaning that the data point is below the mean).\\nThe bars\\' lengths no longer represent the raw number of friends, but the degree to which\\nthat friend count differs from the mean.\\nThis chart makes it very easy to pick out the individuals with much lower and higher\\nfriends on an average. For example, the individual at index 0 has fewer friends on average\\n(they had 109 friends where the average was 789).\\nWhat if we want to graph the standard deviations? Recall that we earlier graphed three\\nhorizontal lines: one at the mean, one at the mean plus the standard deviation (x+s) , and\\none at the mean minus the standard deviation (x-s) .\\nIf we plug these values into the formula for the z-score, we will get the following:\\n            \\n\\n\\nhis is no coincidence! When we standardize the data using the z-score, our standard\\ndeviations become the metric of choice. Let\\'s plot a new graph with the standard deviations\\nadded:\\nplt.bar(y_pos, z_scores)\\nplt.plot((0, 25), (1, 1), \\'g-\\')\\nplt.plot((0, 25), (0, 0), \\'b-\\')\\nplt.plot((0, 25), (-1, -1), \\'r-\\')\\nThe preceding code is adding the following three lines:\\nA blue line at y = 0  that represents zero standard deviations away from the mean\\n(which is on the x-axis)\\nA green line that represents one standard deviation above the mean\\nA red line that represents one standard deviation below the mean\\nLet\\'s look at the graph we get:\\nThe colors of the lines match  up with the lines drawn in the earlier graph of the raw friend\\ncount. If you look carefully, the same people still fall outside of the green and the red lines.\\nNamely, the same three people still fall below the red (lower) line, and the same three\\npeople fall above the green (upper) line.\\n\\n-scores are an effective way to standardize  data. This means that we can put the entire set\\non the same scale. For example, if we also measure each person\\'s general happiness scale\\n(which is between 0 and 1), we might have a dataset similar to the following dataset:\\nfriends = [109, 1017, 1127, 418, 625, 957, 89, 950, 946, 797, 981, 125,\\n455, 731, 1640, 485, 1309, 472, 1132, 1773, 906, 531, 742, 621]\\nhappiness = [.8, .6, .3, .6, .6, .4, .8, .5, .4, .3, .3, .6, .2, .8, 1, .6,\\n.2, .7, .5, .3, .1, 0, .3, 1]\\nimport pandas as pd\\ndf = pd.DataFrame({\\'friends\\':friends, \\'happiness\\':happiness})\\ndf.head()\\nWe get this table:\\nThese data points are on two different dimensions, each with a very different scale. The\\nfriend count can be in the thousands while our happiness score is stuck between 0 and 1.\\nTo remedy this (and for some statistical/machine learning modeling, this concept will\\nbecome essential), we can simply standardize the dataset using a prebuilt standardization\\npackage in scikit-learn, as follows:\\nfrom sklearn import preprocessing\\ndf_scaled = pd.DataFrame(preprocessing.scale(df), columns =\\n[\\'friends_scaled\\', \\'happiness_scaled\\'])\\ndf_scaled.head()\\n\\nhis code will scale both the friends and happiness columns simultaneously, thus revealing\\nthe z-score for each column. It is important to note that by doing this, the preprocessing\\nmodule in sklearn  is doing the following things separately for each column:\\nFinding the mean of the column\\nFinding the standard deviation of the column\\nApplying the z-score function to each element in the column\\nThe result is two columns, as shown, that exist on the same scale as each other even if they\\nwere not previously:\\nNow, we can plot friends and happiness on the same scale and the graph will at least be\\nreadable:\\ndf_scaled.plot(kind=\\'scatter\\', x = \\'friends_scaled\\', y =\\n\\'happiness_scaled\\')\\n\\nhe preceding code gives us this graph:\\nNow, our data is standardized to the z-score and this scatter plot is fairly easily\\ninterpretable! In later chapters, this idea  of standardization will not only make our data\\nmore interpretable, but it will also be essential in our model optimization. Many machine\\nlearning algorithms will require us to have standardized columns as they are reliant on the\\nnotion of scale.\\nThe insightful part – correlations in data\\nThroughout this book, we will discuss the difference between having data and having\\nactionable insights about  your data. Having data is only one step to a successful data\\nscience operation. Being able to obtain, clean, and plot data helps to tell the story that the\\ndata has to offer but cannot reveal the moral. In order to take this entire example one step\\nfurther, we will look at the relationship between having friends on Facebook and\\nhappiness.\\n\\nn subsequent chapters, we will look at a specific machine learning algorithm that attempts\\nto find relationships between quantitative features, called linear regression, but we do not\\nhave to wait until then to begin to form hypotheses. We have a sample of people, a measure\\nof their online social presence, and their reported happiness. The question of the day here is\\nthis: can we find a relationship between the number of friends on Facebook and overall\\nhappiness?\\nNow, obviously, this is a big question and should be treated respectfully. Experiments to\\nanswer this question should be conducted in a laboratory setting, but we can begin to form\\na hypothesis about this question. Given the nature of our data, we really only have the\\nfollowing three options for a hypothesis:\\nThere is a positive association between the number of online friends and\\nhappiness (as one goes up, so does the other)\\nThere is a negative association between them (as the number of friends goes up,\\nyour happiness goes down)\\nThere is no association between the variables (as one changes, the other doesn\\'t\\nreally change that much)\\nCan we use basic statistics to form a hypothesis about this question? I say we can! But first,\\nwe must introduce a concept called correlation.\\nCorrelation coefficients  are a quantitative measure  that describes the strength of\\nassociation/relationship between two variables.\\nThe correlation between the two sets of data tells us about how they move together. Would\\nchanging one help us predict the other? This concept is not only interesting in this case, but\\nit is one of the core assumptions that many machine learning models make on data. For\\nmany prediction algorithms to work, they rely on the fact that there is some sort of\\nrelationship between the variables that we are looking at. The learning algorithms then\\nexploit this relationship in order to make accurate predictions.\\nA few things to note about a correlation coefficient are as follows:\\nIt will lie between -1 and 1\\nThe greater the absolute value (closer to -1 or 1), the stronger the relationship\\nbetween the variables:\\nThe strongest correlation is a -1 or a 1\\nThe weakest correlation is a 0\\n\\n positive correlation means that as one variable increases, the other one tends to\\nincrease as well\\nA negative correlation means that as one variable increases, the other one tends\\nto decrease\\nWe can use pandas  to quickly show us correlation coefficients between every feature and\\nevery other feature in the DataFrame, as illustrated:\\n# correlation between variables\\ndf.corr()\\nWe get this table:\\nThis table shows the correlation between friends  and happiness . Note the first two things:\\nThe diagonal of the matrix is filled with positive is . This is because they represent\\nthe correlation between the variable and itself, which, of course, forms a perfect\\nline, making the correlation perfectly positive!\\nThe matrix is symmetrical across the diagonal. This is true for any correlation\\nmatrix made in pandas.\\nThere are a few caveats to trusting the correlation coefficient. One is that, in general, a\\ncorrelation will attempt to measure a linear  relationship between variables. This means that\\nif there is no visible correlation revealed by this measure, it does not mean that there is no\\nrelationship between the variables, only that there is no line of best fit that goes through the\\nlines easily. There might be a non-linear  relationship that defines the two variables.\\nIt is important to realize that causation is not implied by correlation. Just because there is a\\nweak negative correlation between these two variables does not necessarily mean that your\\noverall happiness decreases as the number of friends you keep on Facebook go up. This\\ncausation must be tested further and, in later chapters, we will attempt to do just that.\\nTo sum up, we can use correlation to make hypotheses about the relationship between\\nvariables, but we will need to use more sophisticated statistical methods and machine\\nlearning algorithms to solidify these assumptions and hypotheses.\\n\\nhe empirical rule\\nRecall that a normal distribution is defined  as having a specific probability distribution that\\nresembles a bell curve. In statistics, we love it when our data behaves normally . For\\nexample, we may have data that resembles a normal distribution, like so:\\nThe empirical rule  states that we can expect a certain amount of data to live between sets of\\nstandard deviations. Specifically, the empirical rule states the following for data that is\\ndistributed normally:\\nAbout 68% of the data falls within 1 standard deviation\\nAbout 95% of the data falls within 2 standard deviations\\nAbout 99.7% of the data falls within 3 standard deviations\\nFor example, let\\'s see if our Facebook friends\\' data holds up to this. Let\\'s use our DataFrame\\nto find the percentage of people that fall within 1, 2, and 3 standard deviations of the mean,\\nas shown:\\n# finding the percentage of people within one standard deviation of the\\nmean\\nwithin_1_std = df_scaled[(df_scaled[\\'friends_scaled\\'] <= 1) &\\n(df_scaled[\\'friends_scaled\\'] >= -1)].shape[0]\\nwithin_1_std / float(df_scaled.shape[0])\\n# 0.75\\n# finding the percentage of people within two standard deviations of the\\nmean\\nwithin_2_std = df_scaled[(df_scaled[\\'friends_scaled\\'] <= 2) &\\n(df_scaled[\\'friends_scaled\\'] >= -2)].shape[0]\\nwithin_2_std / float(df_scaled.shape[0])\\n\\n 0.916\\n# finding the percentage of people within three standard deviations of the\\nmean\\nwithin_3_std = df_scaled[(df_scaled[\\'friends_scaled\\'] <= 3) &\\n(df_scaled[\\'friends_scaled\\'] >= -3)].shape[0]\\nwithin_3_std / float(df_scaled.shape[0])\\n# 1.0\\nWe can see that our data does seem to follow the empirical rule. About 75% of the people\\nare within a single standard deviation of the mean. About 92% of the people are within two\\nstandard deviations, and all of them are within three standard deviations.\\nExample: Exam scores\\nLet\\'s say that we\\'re measuring the scores of an exam and the scores generally have a bell-\\nshaped normal distribution. The average of the exam was 84% and the standard deviation\\nwas 6%. We can say the following, with approximate certainty:\\nAbout 68% of the class scored between 78% and 90% because 78 is 6 units below\\n84, and 90 is 6 units above 84\\nIf we were asked what percentage of the class scored between 72 % and 96%, we\\nwould notice that 72 is 2 standard deviations below the mean, and 96 is 2\\nstandard deviations above the mean, so the empirical rule tells us that about 95%\\nof the class scored in that range\\nHowever, not all data is normally  distributed, so we can\\'t always use the empirical rule. We\\nhave another theorem that helps us analyze any kind of distribution. In the next chapter,\\nwe will go into depth about when we can assume the normal distribution. This is because\\nmany statistical tests and hypotheses require the underlying data to come from a normally\\ndistributed population.\\nPreviously, when we standardized our data to the z-score, we did not\\nrequire the normal distribution assumption.\\n\\nummary\\nIn this chapter, we covered much of the basic statistics required by most data scientists.\\nEverything from how we obtain/sample data to how to standardize data according to the z-\\nscore and applications of the empirical rule was covered. We also reviewed how to take\\nsamples for data analysis. In addition, we reviewed various statistical measures, such as the\\nmean and standard deviation, that help describe data.\\nIn the next chapter, we will look at much more advanced applications of statistics. One\\nthing that we will consider is how to use hypothesis tests on data that we can assume to be\\nnormal. As we use these tests, we will also quantify our errors and identify the best\\npractices to solve these errors.\\n\\n\\nAdvanced Statistics\\nWe are concerned with making inferences about entire populations based on certain\\nsamples of data. We will be using hypothesis tests along with different estimation tests in\\norder to gain a better understanding of populations, given samples of data.\\nThe key topics that we will cover in this chapter are as follows:\\nPoint estimates\\nConfidence intervals\\nThe central limit theorem\\nHypothesis testing\\nPoint estimates\\nRecall that, in the previous chapter, we mentioned how difficult it is to obtain a population\\nparameter; so, we had to use sample data to calculate a statistic that was an estimate of a\\nparameter. When we make these estimates, we call them point estimates.\\nA point estimate  is an estimate  of a population parameter based on sample data.\\nWe use point estimates to estimate population means, variances, and other statistics. To\\nobtain these estimates, we simply apply the function that we wish to measure for our\\npopulation to a sample of the data. For example, suppose there is a company of 9,000\\nemployees and we are interested in ascertaining the average length of breaks taken by\\nemployees in a single day. As we probably cannot ask every single person, we will take a\\nsample of the 9,000 people and take a mean of the sample. This sample mean will be our\\npoint estimate.\\n\\nhe following code is broken into three parts:\\nWe will use the probability distribution, known as the Poisson distribution , to\\nrandomly generate 9,000 answers to the question: for how many minutes in a day\\ndo you usually take breaks?  This will represent our population . Remember, from\\nChapter 6 , Advanced Probability , that the Poisson random variable  is used when\\nwe know the average value of an event and wish to model a distribution around\\nit.\\nNote that this average value is not usually known. I am calculating it to\\nshow the difference between our parameter and our statistic. I also set a\\nrandom seed in order to encourage reproducibility (this allows us to get\\nthe same random numbers each time).\\nWe will take a sample of 100 employees (using the Python random sample method) and\\nfind a point estimate of a mean (called a sample mean).\\nNote that this is just over 1% of our population.\\nCompare our sample mean (the mean of the sample of 100 employees) to our population\\nmean .\\nLet\\'s take a look at the following code:\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy import stats\\nfrom scipy.stats import poisson\\nnp.random.seed(1234)\\nlong_breaks = stats.poisson.rvs(loc=10, mu=60, size=3000)\\n# represents 3000 people who take about a 60 minute break\\nThe long_breaks  variable represents 3,000  answers to the question, \" how many minutes\\non an average do you take breaks for?,\"  and these answers will be on the longer side. Let\\'s\\nsee a visualization of this distribution, shown as follows:\\npd.Series(long_breaks).hist()\\n\\nWe can see that our average of 60 minutes is to the left of the distribution. Also, because we\\nonly sampled 3,000  people, our bars are at their highest at around 700–800 people.\\nNow, let\\'s model 6,000  people who take, on an average, about 15 minutes\\' worth of breaks.\\nLet\\'s again use the Poisson distribution to simulate 6,000  people, as shown:\\nshort_breaks = stats.poisson.rvs(loc=10, mu=15, size=6000)\\n# represents 6000 people who take about a 15 minute break\\npd.Series(short_breaks).hist()\\n\\n\\nkay, so, we have a distribution for the people who take longer breaks and a distribution\\nfor the people who take shorter breaks. Again, note how our average break length of 15\\nminutes falls to the left-hand side of the distribution, and note that the tallest bar is for\\nabout 1,600  people:\\nbreaks = np.concatenate((long_breaks, short_breaks))\\n# put the two arrays together to get our \"population\" of 9000 people\\nThe breaks  variable is the amalgamation of all the 9,000  employees, both long and short\\nbreak takers. Let\\'s see the entire distribution of people in a single visualization:\\npd.Series(breaks).hist()\\nWe can see we have two humps. On the left, we have our larger hump of people who take\\nabout a 15-minute break, and on the right, we have a smaller hump of people who take\\nlonger breaks. Later on, we will investigate this graph further.\\nWe can find the total average break length by running the following code:\\nbreaks.mean()\\n# 39.99 minutes is our parameter.\\n\\nur average company break length is about 40 minutes. Remember that our population is\\nthe entire company\\'s employee size of 9,000 people, and our parameter is 40 minutes . In the\\nreal world, our goal would be to estimate the population parameter because we would not\\nhave the resources to ask every single employee in a survey their average break length for\\nmany reasons. Instead, we will use a point estimate.\\nSo, to make our point, we want to simulate a world where we ask 100 random people about\\nthe length of their breaks. To do this, let\\'s take a random sample of 100 employees out of\\nthe 9,000 employees we simulated, as shown:\\nsample_breaks = np.random.choice(a = breaks, size=100)\\n# taking a sample of 100 employees\\nNow, let\\'s take the mean of the sample and subtract it from the population mean and see\\nhow far off we were:\\nbreaks.mean() - sample_breaks.mean()\\n# difference between means is 4.09 minutes, not bad!\\nThis is extremely interesting because, with only about 1% of our population (100 out of\\n9,000), we were able to get within 4 minutes of our population parameter and get a very\\naccurate estimate of our population mean. Not bad!\\nHere, we calculated a point estimate for the mean, but we can also do this for proportion\\nparameters. By proportion, I am referring to a ratio of two quantitative values.\\nLet\\'s suppose that in a company of 10,000 people, our employees are 20% white, 10% black,\\n10% Hispanic, 30% Asian, and 30% identify as other. We will take a sample of 1,000\\nemployees and see whether their race proportions are similar:\\nemployee_races = ([\"white\"]*2000) + ([\"black\"]*1000) +\\\\\\n                   ([\"hispanic\"]*1000) + ([\"asian\"]*3000) +\\\\\\n                   ([\"other\"]*3000)\\nThe employee_races  represents our employee population. For example, in our company\\nof 10,000 people, 2,000 people are white (20%) and 3,000 people are Asian (30%).\\nLet\\'s take a random sample of 1,000 people, as shown:\\nimport random\\ndemo_sample = random.sample(employee_races, 1000)   # Sample 1000 values\\n\\nor race in set(demo_sample):\\n    print( race + \" proportion estimate:\" )\\n    print( demo_sample.count(race)/1000. )\\nThe output obtained would be as follows:\\nhispanic proportion estimate:\\n0.103\\nwhite proportion estimate:\\n0.192\\nother proportion estimate:\\n0.288\\nblack proportion estimate:\\n0.1\\nasian proportion estimate:\\n0.317\\nWe can see that the race proportion estimates are very close to the underlying population\\'s\\nproportions. For example, we got 10.3% for Hispanic in our sample and the population\\nproportion for Hispanic was 10%.\\nSampling distributions\\nIn Chapter 7 , Basic Statistics , we mentioned how much we love it when data follows the\\nnormal distribution. One of the reasons for this is that many statistical tests (including the\\nones we will use in this chapter) rely on data that follows a normal pattern, and for the\\nmost part, a lot of real-world data is not normal (surprised?). Take our employee  break\\ndata, for example —you might think I was just being fancy creating data using the Poisson\\ndistribution, but I had a reason for this. I specifically wanted non-normal data, as shown:\\npd.DataFrame(breaks).hist(bins=50,range=(5,100))\\n\\nAs you can see, our data is definitely not following a normal distribution; it appears to be\\nbi-modal , which means that there are two peaks of break times, at around 25 and 70\\nminutes. As our data is not normal, many of the most popular statistics tests may not apply;\\nhowever, if we follow the given procedure, we can create normal data! Think I\\'m crazy?\\nWell, see for yourself.\\nFirst off, we will need to utilize what is known as a sampling distribution , which is a\\ndistribution of point estimates of several samples of the same size. Our procedure for\\ncreating a sampling distribution will be the following:\\nTake 500 different samples of the break times of size 100 each1.\\nTake a histogram of these 500 different point estimates (revealing their2.\\ndistribution)\\nThe number of elements in the sample (100) was arbitrary but large enough to be a\\nrepresentative sample of the population. The number of samples I took (500) was also\\narbitrary, but large enough to ensure that our data would converge to a normal\\ndistribution:\\npoint_estimates = []\\nfor x in range(500):         # Generate 500 samples\\n   sample = np.random.choice(a= breaks, size=100)\\n\\ntake a sample of 100 points\\npoint_estimates.append( sample.mean() )\\n# add the sample mean to our list of point estimates\\npd.DataFrame(point_estimates).hist()\\n# look at the distribution of our sample means\\nBehold! The sampling distribution of the sample mean appears to be normal even though\\nwe took data from an underlying bimodal population distribution. It is important to note\\nthat the bars in this histogram represent the average break length of 500 samples of\\nemployees, where each sample has 100 people in it. In other words, a sampling distribution\\nis a distribution of several point estimates.\\nOur data converged to a normal distribution because of something called the central limit\\ntheorem , which states that the sampling distribution (the distribution of point estimates)\\nwill approach a normal distribution as we increase the number of samples taken.\\n\\nhat\\'s more, as we take more and more samples, the mean of the sampling distribution\\nwill approach the true population mean, as shown:\\nbreaks.mean() - np.array(point_estimates).mean()\\n# .047 minutes difference\\nThis is actually a very exciting result because it means that we can get even closer than a\\nsingle point estimate by taking multiple point estimates and utilizing the central limit\\ntheorem!\\nIn general, as we increase the number of samples taken, our estimate will\\nget closer to the parameter (actual value).\\nConfidence intervals\\nWhile point estimates are okay, estimates of a population parameter and sampling\\ndistributions are even better. There are the following two main issues with these\\napproaches:\\nSingle point estimates are very prone to error (due to sampling bias among other\\nthings)\\nTaking multiple samples of a certain size for sampling distributions might not be\\nfeasible, and may sometimes be even more infeasible than actually finding the\\npopulation parameter\\nFor these reasons and more, we may turn to a concept known as the confidence interval to\\nfind statistics.\\nA confidence interval  is a range of values based on a point estimate that contains the true\\npopulation parameter at some confidence level.\\nConfidence  is an important concept in advanced statistics. Its meaning is sometimes\\nmisconstrued. Informally, a confidence level does not represent a probability of being correct ;\\ninstead, it represents the frequency that the obtained answer will be accurate. For example,\\nif you want to have a 95% chance of capturing the true population parameter using only a\\nsingle point estimate, you have to set your confidence level to 95%.\\n\\nigher confidence levels result in wider (larger) confidence intervals in\\norder to be more sure .\\nCalculating a confidence interval involves finding a point estimate and then incorporating a\\nmargin of error to create a range. The margin of error  is a value that represents our\\ncertainty that our point estimate is accurate and is based on our desired confidence level,\\nthe variance of the data, and how big your sample is. There are many ways to calculate\\nconfidence intervals; for the purpose of brevity and simplicity, we will look at a single way\\nof taking the confidence interval of a population mean. For this confidence interval, we\\nneed the following:\\nA point estimate. For this, we will take our sample mean of break lengths from\\nour previous example.\\nAn estimate of the population standard deviation, which represents the variance\\nin the data. This is calculated by taking the sample standard deviation (the\\nstandard deviation of the sample data) and dividing that number by the square\\nroot of the population size.\\nThe degrees of freedom (which is the -1 sample size).\\nObtaining these numbers might seem arbitrary but, trust me, there is a reason for all of\\nthem. However, again for simplicity, I will use prebuilt Python modules, as shown, to\\ncalculate our confidence interval and then demonstrate its value:\\nimport math\\nsample_size = 100\\n# the size of the sample we wish to take\\nsample = np.random.choice(a= breaks, size = sample_size)\\n# a sample of sample_size taken from the 9,000 breaks population from\\nbefore\\nsample_mean = sample.mean()\\n# the sample mean of the break lengths sample\\n\\nample_stdev = sample.std()\\n# sample standard deviation\\nsigma = sample_stdev/math.sqrt(sample_size)\\n# population standard deviation estimate\\nstats.t.interval(alpha = 0.95,              # Confidence level 95%\\n                 df= sample_size - 1,       # Degrees of freedom\\n                 loc = sample_mean,         # Sample mean\\n                 scale = sigma)             # Standard deviation\\n# (36.36, 45.44)\\nTo reiterate, this range of values (from 36.36  to 45.44 ) represents a confidence interval for\\nthe average break time with 95% confidence.\\nWe already know that our population parameter is 39.99 , and note that the interval\\nincludes the population mean of 39.99 .\\nI mentioned earlier that the confidence  level is not a percentage of accuracy of our interval\\nbut the percent chance that the interval will even contain the population parameter at all.\\nTo better understand the confidence level, let\\'s take 10,000 confidence intervals and see\\nhow often our population means falls in the interval. First, let\\'s make a function, as\\nillustrated, that makes a single confidence interval from our breaks data:\\n# function to make confidence interval\\ndef makeConfidenceInterval():\\n    sample_size = 100\\n    sample = np.random.choice(a= breaks, size = sample_size)\\n    sample_mean = sample.mean()\\n\\n   # sample mean\\n    sample_stdev = sample.std()\\n    # sample standard deviation\\n    sigma = sample_stdev/math.sqrt(sample_size)\\n    # population Standard deviation estimate\\n    return stats.t.interval(alpha = 0.95, df= sample_size - 1, loc =\\nsample_mean, scale = sigma)\\nNow that we have a function that will create a single confidence interval, let\\'s create a\\nprocedure that will test the probability that a single confidence interval will contain the true\\npopulation parameter, 39.99 :\\nTake 10,000 confidence intervals of the sample mean.1.\\nCount the number of times that the population parameter falls into our2.\\nconfidence intervals.\\nOutput the ratio of the number of times the parameter fell into the interval by3.\\n10,000:\\ntimes_in_interval = 0.\\nfor i in range(10000):\\n    interval = makeConfidenceInterval()\\n    if 39.99 >= interval[0] and 39.99 <= interval[1]:\\n    # if 39.99 falls in the interval\\n        times_in_interval += 1\\nprint(times_in_interval / 10000)\\n# 0.9455\\n\\nuccess! We see that about 95% of our confidence intervals contained our actual population\\nmean. Estimating population parameters through point estimates and confidence intervals\\nis a relatively simple and powerful form of statistical inference.\\nLet\\'s also take a quick look at how  the size of confidence intervals changes as we change\\nour confidence level. Let\\'s calculate confidence intervals for multiple confidence levels and\\nlook at how large the intervals are by looking at the difference between the two numbers.\\nOur hypothesis will be that as we make our confidence level larger, we will likely see larger\\nconfidence intervals to be surer that we catch the true population parameter:\\nfor confidence in (.5, .8, .85, .9, .95, .99):\\n    confidence_interval = stats.t.interval(alpha = confidence, df=\\nsample_size - 1, loc = sample_mean, scale = sigma)\\n    length_of_interval = round(confidence_interval[1] -\\nconfidence_interval[0], 2)\\n    # the length of the confidence interval\\n    print( \"confidence {0} has a interval of size {1}\".format(confidence,\\nlength_of_interval))\\nconfidence 0.5 has an interval of size 2.56\\nconfidence 0.8 has an interval of size 4.88\\nconfidence 0.85 has an interval of size 5.49\\nconfidence 0.9 has an interval of size 6.29\\nconfidence 0.95 has an interval of size 7.51\\nconfidence 0.99 has an interval of size 9.94\\nWe can see that as we wish to be more confident  in our interval, our interval expands in\\norder to compensate.\\nNext, we will take our concept of confidence levels and look at statistical hypothesis testing\\nin order to both expand on these topics and also create (usu ally)  even more powerful\\nstatistical inferences.\\n\\nypothesis tests\\nHypothesis tests are one of the most widely used tests in statistics. They come in many\\nforms; however, all of them have the same basic purpose.\\nA hypothesis test  is a statistical test that is used to ascertain whether we are allowed to\\nassume that a certain condition is true for the entire population, given a data sample.\\nBasically, a hypothesis test is a test for a certain hypothesis that we have about an entire\\npopulation. The result of the test then tells us whether we should believe the hypothesis or\\nreject it for an alternative one.\\nYou can think of the hypothesis test\\'s framework to determine whether the observed\\nsample data deviates from what was to be expected from the population itself. Now, this\\nsounds like a difficult task but, luckily, Python comes to the rescue and includes built-in\\nlibraries to conduct these tests easily.\\nA hypothesis test generally looks at two opposing hypotheses about a population. We call\\nthem the null hypothesis  and the alternative hypothesis . The null hypothesis is the\\nstatement being tested and is the default correct  answer; it is our starting point and our\\noriginal hypothesis. The alternative hypothesis is the statement that opposes the null\\nhypothesis. Our test will tell us which hypothesis we should trust and which we should\\nreject.\\nBased on sample data from a population, a hypothesis test determines whether or not to\\nreject  the null hypothesis. We usually use a p-value  (which is based on our significance\\nlevel) to make this conclusion.\\nA very common misconception is that statistical hypothesis tests are\\ndesigned to select the more likely  of the two hypotheses. This is incorrect.\\nA hypothesis test will default to the null hypothesis until  there is enough\\ndata to support the alternative hypothesis.\\nThe following are some examples of questions you can answer with a hypothesis test:\\nDoes the mean break time of employees differ from 40 minutes?\\nIs there a difference between people who interacted with website A and people\\nwho interacted with website B (A/B testing)?\\nDoes a sample of coffee beans vary significantly in taste from the entire\\npopulation of beans?\\n\\nonducting a hypothesis test\\nThere are multiple types of hypothesis  tests out there, and among them are dozens of\\ndifferent procedures and metrics. Nonetheless, there are five basic steps that most\\nhypothesis tests follow, which are as follows:\\nSpecify the hypotheses:1.\\nHere, we formulate our two hypotheses: the null and the alternative.\\nWe usually use the notation of H0 to represent the null hypothesis and\\nHa to represent our alternative hypothesis.\\nDetermine the sample size for the test sample:2.\\nThis calculation depends on the chosen test. Usually, we have to\\ndetermine a proper sample size in order to utilize theorems, such as\\nthe central limit theorem and assume the normality of data.\\nChoose a significance level (usually called alpha or α): 3.\\nA significance level of 0.05 is common.\\nCollect the data:4.\\nCollect a sample of data to conduct the test.\\nDecide whether to reject or fail to reject the null hypothesis:5.\\nThis step changes slightly based on the type of test being used.\\nThe final result will either yield a rejection of  the null hypothesis in\\nfavor of the alternative or fail to reject the null hypothesis.\\nIn this chapter, we will look at the following three types of hypothesis tests:\\nOne-sample t-tests\\nChi-square goodness of fit\\nChi-square test for association/independence\\nThere are many more tests. However, these three are a great combination of distinct,\\nsimple, and powerful tests. One of the biggest things to consider when choosing which test\\nwe should implement is the type of data we are working with, specifically, are we dealing\\nwith continuous or categorical data? In order to truly see the effects of a hypothesis, I\\nsuggest we dive right into an example. First, let\\'s look at the use of t-tests to deal with\\ncontinuous data.\\n\\nne sample t-tests\\nThe one sample t-test  is a statistical  test used to determine  whether a quantitative\\n(numerical) data sample differs significantly  from another dataset (the population or\\nanother sample). Suppose, in our previous employee break time example, we look,\\nspecifically, at the engineering department\\'s break times, as shown:\\nlong_breaks_in_engineering = stats.poisson.rvs(loc=10, mu=55, size=100)\\nshort_breaks_in_engineering = stats.poisson.rvs(loc=10, mu=15, size=300)\\nengineering_breaks = np.concatenate((long_breaks_in_engineering,\\nshort_breaks_in_engineering))\\nprint(breaks.mean())\\n# 39.99\\nprint(engineering_breaks.mean())\\n# 34.825\\nNote that I took the same approach as making the original break times, but with the\\nfollowing two differences:\\nI took a smaller sample from the Poisson distribution (to simulate that we took a\\nsample of 400 people from the engineering department).\\nInstead of using μ of 60 as before, I used 55 to simulate the fact that the\\nengineering department\\'s break behavior isn\\'t exactly the same as the company\\'s\\nbehavior as a whole.\\nIt is easy to see that there seems to be a difference (of over 5 minutes) between the\\nengineering department and the company as a whole. We usually don\\'t have the entire\\npopulation and the population parameters at our disposal, but I have them simulated in\\norder for the example to work. So, even though we (the omniscient readers) can see a\\ndifference, we will assume that we know nothing of these population parameters and,\\ninstead, rely on a statistical test in order to ascertain these differences.\\n\\nxample of a one-sample t-test\\nOur objective here is to ascertain whether  there is a difference between the overall\\npopulation\\'s (company employees) break times and the break times of employees in the\\nengineering department .\\nLet\\'s now conduct a t-test at a 95% confidence level in order to find a difference (or not!).\\nTechnically speaking, this test will tell us if the sample comes from the same distribution as\\nthe population.\\nAssumptions of the one-sample t-test\\nBefore diving into the five steps, we must first acknowledge that t-tests must satisfy the\\nfollowing two conditions to work properly:\\nThe population distribution should be normal, or the sample should be large  (n ≥\\n30)\\nIn order to make the assumption that the sample is independently, randomly\\nsampled, it is sufficient to enforce that the population size should be at least 10\\ntimes larger than the sample size (10n < N)\\nNote that our test requires that either the underlying data be normal (which we know is not\\ntrue for us), or that the sample size is at least 30 points large. For the t-test, this condition is\\nsufficient to assume normality. This test also requires independence, which is satisfied by\\ntaking a sufficiently small  sample. Sounds weird, right? The basic idea is that our sample\\nmust be large enough to assume normality (through conclusions similar to the central limit\\ntheorem) but small enough as to be independent  of the population.\\nNow, let\\'s follow our five steps:\\nSpecify the hypotheses.1.\\nWe will let H0 = the engineering department take breaks of the same duration as\\nthe company as a whole\\nIf we let this be the company average, we may write the following:\\nH0:\\nNote how this is our null, or default , hypothesis. It is what we would assume,\\ngiven no data. What we would like to show is the alternative hypothesis .\\n\\now that we actually have some options for our alternative, we could either say\\nthat the engineering mean (let\\'s call it that) is lower than the company average,\\nhigher than the company average, or just flat out different (higher or lower) than\\nthe company average:\\nIf we wish to answer the question, \"i s the sample mean different\\nfrom the company average?,\" then this is called a two-tailed test\\nand our alternative hypothesis would be as follows:\\nHa:\\nIf we want to find out whether  the sample mean is lower than the\\ncompany average  or if the sample mean is higher than the\\ncompany average , then we are dealing with a one-tailed test  and\\nour alternative hypothesis would be one of the following\\nhypotheses:\\nHa:(engineering takes longer breaks)\\nHa:(engineering takes shorter breaks)\\nThe difference between one and two tails is the difference of dividing a number\\nlater on by two or not. The process remains completely unchanged for both. For\\nthis example, let\\'s choose the two-tailed test. So, we are testing for whether or not\\nthis sample of the engineering department\\'s average break times is different from\\nthe company average.\\nOur test will end in one of two possible conclusions: we will either reject\\nthe null hypothesis, which means that the engineering department\\'s break\\ntimes are different from the company average, or we will fail to reject the\\nnull hypothesis, which means that there wasn\\'t enough evidence in the\\nsample to support rejecting the null.\\nDetermine the sample size for the test sample.2.\\nAs mentioned earlier, most tests (including this one) make the assumption that\\neither the underlying data is normal or that our sample is in the right range:\\nThe sample is at least 30 points (it is 400)\\nThe sample is less than 10% of the population (which would be 900\\npeople)\\n\\nhoose a significance level (usually called alpha or α). We will choose a 95% 3.\\nsignificance level, which means that our alpha would actually be 1 - .95 = .05\\nCollect the data. This is generated through the two Poisson distributions.4.\\nDecide whether to reject or fail to reject the null hypothesis. As mentioned before,5.\\nthis step varies based on the test used. For a one-sample  t-test, we must calculate\\ntwo numbers: the test statistic and our p-value . Luckily, we can do this in one line\\nin Python.\\nA test statistic is a value that is derived  from sample data during a type of hypothesis test.\\nThey are used to determine whether or not to reject the null hypothesis.\\nThe test statistic is used to compare the observed data with what is expected under the null\\nhypothesis. The test statistic is used in conjunction with the p-value.\\nThe p-value is the probability that the observed data occurred this way by chance.\\nWhen the data is showing very strong evidence against the null hypothesis, the test statistic\\nbecomes large (either positive or negative) and the p-value usually becomes very small,\\nwhich means that our test is showing powerful results and what is happening is, probably,\\nnot happening by chance.\\nIn the case of a t-test, a t value  is our test statistic, as shown:\\nt_statistic, p_value = stats.ttest_1samp(a= engineering_breaks,\\n popmean= breaks.mean())\\nWe input the engineering_breaks  variable (which holds 400 break times) and the\\npopulation mean ( popmean ), and we obtain the following numbers:\\nt_statistic == -5.742\\np_value == .00000018\\nThe test result shows that the t value  is -5.742 . This is a standardized metric that reveals\\nthe deviation of the sample mean from the null hypothesis. The p-value is what gives us our\\nfinal answer. Our p-value is telling us how often our result would appear by chance. So, for\\nexample, if our p-value was .06, then that would mean we should expect to observe this\\ndata by chance about 6% of the time. This means that about 6% of samples would yield\\nresults like this.\\n\\ne are interested in how our p-value compares to our significance level:\\nIf the p-value is less than  the significance level, then we can reject  the null\\nhypothesis\\nIf the p-value is greater than  the significance level, then we failed to reject  the null\\nhypothesis\\nOur p-value is way lower than .05 (our chosen significance level), which means that we may\\nreject our null hypothesis in favor of the alternative. This means that the engineering\\ndepartment seems to take different break lengths than the company as a whole!\\nThe use of p-values is controversial. Many journals have actually banned\\nthe use of p-values in tests for significance. This is because of the nature of\\nthe value. Suppose our p-value came out to .04. It means that 4% of the\\ntime, our data just randomly happened to appear this way and is not\\nsignificant in any way. 4% is not that small of a percentage! For this\\nreason, many people are switching to different statistical tests. However,\\nthat does not mean that p-values are useless. It merely means that we\\nmust be careful and aware of what the number is telling us.\\nThere are many other types of t-tests, including one-tailed tests (mentioned before) and\\npaired tests as well as two sample t-tests (both not mentioned yet). These procedures can be\\nreadily found in statistics literature; however, we should look at something very\\nimportant —what happens when we get it wrong.\\nType I and type II errors\\nWe\\'ve mentioned both the type I and type II errors in Chapter 5 , Impossible or Improbable –\\nA Gentle Introduction to Probability , about probability in the examples of a binary classifier,\\nbut they also apply to hypothesis tests.\\nA type I error occurs  if we reject the null hypothesis  when it is actually true. This is also\\nknown as a false positive . The type I error rate is equal to the significance level α, which\\nmeans that if we set a higher confidence level, for example, a significance level of 99%, our\\nα is .01, and therefore our false positive rate is 1%.\\nA type II error occurs if we fail to reject  the null hypothesis when it is actually false. This is\\nalso known as a false negative . The higher we set our confidence level, the more likely we\\nare to actually see a type II error.\\n\\nypothesis testing for categorical variables\\nT-tests (among other tests) are hypothesis tests  that work to compare and contrast\\nquantitative variables and underlying population distributions. In this section, we will\\nexplore two new tests, both of which serve to explore qualitative data. They are both a form\\nof test called chi-square tests. These two tests will perform the following two tasks for us:\\nDetermine whether a sample of categorical variables is taken from a specific\\npopulation (similar to the t-test)\\nDetermine whether two variables affect each other and are associated with each\\nother\\nChi-square goodness of fit test\\nThe one-sample t-test was used to check whether a sample means differed from the\\npopulation mean. The chi-square goodness of fit test is very similar to the one sample t-test\\nin that it tests  whether the distribution of the sample data matches an expected distribution,\\nwhile the big difference is that it is testing for categorical variables.\\nFor example, a chi-square goodness  of fit test would be used to see whether the race\\ndemographics of your company match that of the entire city of the U.S. population. It can\\nalso be used to see if users of your website show similar characteristics to average internet\\nusers.\\nAs we are working with categorical data, we have to be careful because categories such as\\n\"male,\" \"female,\" or \"other\" don\\'t have any mathematical meaning. Therefore, we must\\nconsider counts of the variables rather than the actual variables themselves.\\nIn general, we use the chi-square goodness of fit test in the following cases:\\nWe want to analyze one categorical variable from one population\\nWe want to determine whether a variable fits a specified or expected distribution\\nIn a chi-square test, we compare what is observed to what we expect.\\nAssumptions of the chi-square goodness of ﬁt test\\nThere are two usual  assumptions of this test, as follows:\\nAll the expected counts are at least five\\nIndividual observations are independent and the population should be at least 10\\ntimes as large as the sample ( 10n < N)\\n\\nhe second assumption should look familiar to the t-test; however, the first assumption\\nshould look foreign. Expected counts are something we haven\\'t talked about yet but are\\nabout to!\\nWhen formulating our null and alternative hypotheses for this test, we consider a default\\ndistribution of categorical variables. For example, if we have a die and we are testing\\nwhether or not the outcomes are coming from a fair die, our hypothesis might look as\\nfollows:\\nH0: The specified distribution of the categorical variable is correct.\\np1 = 1/6, p2 = 1/6, p3 = 1/6, p4 = 1/6, p5 = 1/6, p6 = 1/6\\nOur alternative hypothesis is quite simple, as shown:\\nHa : The specified distribution of the categorical variable is not correct. At least one of the pi\\nvalues is not correct.\\nIn the t-test, we used our test statistic (the t-value) to find our p-value. In a chi-square test,\\nour test statistic is, well, a chi-square:\\nTest Statistic: χ2 =\\u2028 over k categories\\nDegrees of Freedom = k − 1\\nA critical value is when we use χ2 as well as our degrees of freedom and our significance\\nlevel, and then reject the null hypothesis if the p-value is below our significance level (the\\nsame as before).\\nLet\\'s look at an example to understand this further.\\nExample of a chi-square test for goodness of ﬁt\\nThe CDC categorizes adult BMIs into four classes: Under/Normal , Over , Obesity , and\\nExtreme Obesity . A 2009 survey showed the distribution for adults in the US to be 31.2%,\\n33.1%, 29.4%, and 6.3% respectively. A total of 500 adults were randomly sampled and their\\nBMI categories were recorded. \\u2028 Is there evidence to suggest that BMI trends have changed\\nsince 2009? Let\\'s test at the 0.05 significance level:\\n\\n\\nirst, let\\'s calculate our expected values. In a sample of 500, we expect  156 to be\\nUnder/Normal  (that\\'s 31.2% of 500), and we fill in the remaining boxes in the same way:\\nFirst, check the conditions that are as follows:\\nAll of the expected counts are greater than five\\nEach observation is independent and our population is very large ( much more\\nthan 10 times of 500 people )\\nNext, carry out a goodness of fit test. We will set our null and alternative hypotheses:\\nH0: The 2009 BMI distribution is still correct.\\nHa: The 2009 BMI distribution is no longer correct (at least one of the proportions\\nis different now). We can calculate our test statistic by hand:\\nAlternatively, we can use our handy-dandy Python skills, as shown:\\nobserved = [102, 178, 186, 34]\\nexpected = [156, 165.5, 147, 31.5]\\nchi_squared, p_value = stats.chisquare(f_obs= observed, f_exp= expected)\\nchi_squared, p_value\\n#(30.1817679275599, 1.26374310311106e-06)\\n\\nur p-value is lower than .05; therefore, we may reject the null hypothesis in favor of the\\nfact that the BMI trends today are different from what they were in 2009.\\nChi-square test for association/independence\\nIndependence as a concept in probability is when knowing the value of one variable tells\\nyou nothing about  the value of another. For example, we might expect that the country and\\nthe month you were born in are independent. However, knowing which type of phone you\\nuse might indicate your creativity levels. Those variables might not be independent.\\nThe chi-square test for association/independence helps us ascertain whether two categorical\\nvariables are independent of one another. The test for independence is commonly used to\\ndetermine whether variables like education levels or tax brackets vary based on\\ndemographic factors, such as gender, race, and religion. Let\\'s look back at an example\\nposed in the preceding chapter, the A/B split test.\\nRecall that we ran a test and exposed half of our users to a certain landing page ( Website\\nA), exposed the other half to a different landing page ( Website B ), and then measured the\\nsign-up rates for both sites. We obtained the following results:\\nDid not sign up Signed up\\nWebsite A 134 54\\nWebsite B 110 48\\nResults of our A/B test\\nWe calculated website conversions but what we really want to know is whether there is a\\ndifference between the two variables: which website was the user exposed to?  and did the user\\nsign up?  For this, we will use our chi-square test.\\nAssumptions of the chi-square independence test\\nThere are the following two assumptions of this test:\\nAll expected counts are at least five\\nIndividual observations are independent and the population should be at least 10\\ntimes as large as the sample ( 10n < N)\\nNote that they are exactly the same as the last chi-square test.\\n\\net\\'s set up our hypotheses:\\nH0: There is no association between two categorical variables in the population of\\ninterest\\nH0: Two categorical variables are independent in the population of interest\\nHa: There is an association between two categorical variables in the population of\\ninterest\\nHa: Two categorical variables are not independent in the population of interest\\nYou might notice that we are missing something important here. Where are the expected\\ncounts? Earlier, we had a prior distribution to compare our observed results to but now we\\ndo not. For this reason, we will have to create some. We can use the following formula to\\ncalculate the expected values for each value. In each cell of the table, we can use the\\nfollowing:\\nExpected Count = to calculate our chi-square test statistic and our degrees of freedom\\nHere, r is the number of rows and c is the number of columns. Of course, as before, when\\nwe calculate our p-value, we will reject the null if that p-value is less than the significance\\nlevel. Let\\'s use some built-in Python methods, as shown, in order to quickly get our results:\\nobserved = np.array([[134, 54],[110, 48]])\\n# built a 2x2 matrix as seen in the table above\\nchi_squared, p_value, degrees_of_freedom, matrix =\\nstats.chi2_contingency(observed= observed)\\nchi_squared, p_value\\n# (0.04762692369491045, 0.82724528704422262)\\n\\ne can see that our p-value is quite large; therefore, we fail to reject our null hypothesis\\nand we cannot say for sure that seeing a particular website has any effect on a whether or\\nnot a user signs up . There is no association between these variables.\\nSummary\\nIn this chapter, we looked at different statistical tests, including chi-square and t-tests as\\nwell as point estimates and confidence intervals, in order to ascertain population\\nparameters based on sample data. We were able to find that even with small samples of\\ndata, we can make powerful assumptions about the underlying population as a whole.\\nUsing concepts reviewed in this chapter, data scientists will be able to make inferences\\nabout entire datasets based on certain samples of data. In addition, they will be able to use\\nhypothesis tests to gain a better understanding of full datasets, given samples of data.\\nStatistics is a very wide and expansive subject that cannot truly be covered in a single\\nchapter; however, our understanding of the subject will allow us to carry on and talk more\\nabout how we can use statistics and probability in order to communicate our ideas through\\ndata science in the next chapter.\\nIn the next chapter, we will discuss different ways of communicating results from data\\nanalysis including various presentation styles as well as visualization techniques.\\n\\n\\nCommunicating Data\\nThis chapter deals with the different ways of communicating results from our analysis.\\nHere, we will look at different presentation styles as well as visualization techniques. The\\npoint of this chapter is to take our results and be able to explain them in a coherent,\\nintelligible way so that anyone, whether they are data savvy or not, may understand and\\nuse our results.\\nMuch of what we will discuss will be how to create effective graphs through labels, keys,\\ncolors, and more. We will also look at more advanced visualization techniques, such as\\nparallel coordinate plots.\\nIn this chapter, we will look into the following topics:\\nIdentifying effective and ineffective visualizations\\nRecognizing when charts are attempting to \"trick\" the audience\\nBeing able to identify causation versus correlation\\nConstructing appealing visuals that offer valuable insight\\n\\nhy does communication matter?\\nBeing able to conduct experiments and manipulate data in a coding language is not enough\\nto conduct practical and applied data science. This is because data science is, generally, only\\nas good as how it is used in practice. For instance, a medical data scientist might be able to\\npredict the chance of a tourist contracting malaria in developing countries with >98%\\naccuracy; however, if these results are published in a poorly marketed journal and online\\nmentions of the study are minimal, their groundbreaking results that could potentially\\nprevent deaths would never truly see the light of day.\\nFor this reason, communication of results is arguably as important as the results\\nthemselves. A famous example of poor management of the distribution of results is the case\\nof Gregor Mendel. Mendel is widely recognized as one of the founders of modern genetics.\\nHowever, his results (including data and charts) were not well adopted until after his\\ndeath. Mendel even sent them to Charles Darwin, who largely ignored Mendel\\'s papers,\\nwhich were written in unknown Moravian journals.\\nGenerally, there are two ways of presenting results: verbal and visual. Of course, both the\\nverbal and visual forms of communication can be broken down into dozens of\\nsubcategories, including slide decks, charts, journal papers, and even university lectures.\\nHowever, we can find common elements of data presentation that can make anyone in the\\nfield more aware and effective in their communication skills.\\nLet\\'s dive right into effective (and ineffective) forms of communication, starting with\\nvisuals.\\nIdentifying effective and ineffective\\nvisualizations\\nThe main goal of data visualization is to have  the reader quickly digest the data, including\\npossible trends, relationships, and more. Ideally, a reader will not have to spend more than\\n5-6 seconds digesting a single visualization. For this reason, we must make  visuals very\\nseriously and ensure that we are making a visual as effective as possible. Let\\'s look at five\\nbasic types of graphs: scatter plots, line graphs, bar charts, histograms, and box plots.\\n\\ncatter plots\\nA scatter plot is probably one of the simplest graphs to create. It is made by creating two\\nquantitative  axes and using data points to represent observations. The main goal of a scatter\\nplot is to highlight relationships between two variables and, if possible, reveal a correlation.\\nFor example, we can look at two variables: the average hours of TV watched in a day and a\\n0-100 scale of work performance (0 being very poor performance and 100 being excellent\\nperformance). The goal here is to find a relationship (if it exists) between watching TV and\\naverage work performance.\\nThe following code simulates a survey of a few people, in which they revealed the amount\\nof television they watched, on average, in a day against a company-standard work\\nperformance metric. This line of code is creating 14 sample survey results of people\\nanswering the question of how many hours of TV they watch in a day:\\nimport pandas as pd\\nhours_tv_watched = [0, 0, 0, 1, 1.3, 1.4, 2, 2.1, 2.6, 3.2, 4.1, 4.4, 4.4,\\n5]\\nThis next line of code is creating 14 new sample survey results of the same people being\\nrated on their work performance on a scale from 0 to 100. For example, the first person\\nwatched 0 hours of TV a day and was rated 87/100 on their work, while the last person\\nwatched, on an average, 5 hours of TV a day and was rated 72/100:\\nwork_performance = [87, 89, 92, 90, 82, 80, 77, 80, 76, 85, 80, 75, 73, 72]\\nHere, we are creating a DataFrame in order to simplify our exploratory data analysis and\\nmake it easier to make a scatter plot:\\ndf = pd.DataFrame({\\'hours_tv_watched\\':hours_tv_watched,\\n\\'work_performance\\':work_performance})\\nNow, we are actually making our scatter plot:\\ndf.plot(x=\\'hours_tv_watched\\', y=\\'work_performance\\', kind=\\'scatter\\')\\n\\nn the following plot, we can see that our axes represent the number of hours of TV\\nwatched in a day and the person\\'s work performance metric:\\nThe scatter plot: hours of TV watched versus work performance\\nEach point on a scatter plot represents a single observation (in this case a person) and its\\nlocation is a result of where the observation stands on each variable. This scatter plot does\\nseem to show a relationship, which implies that as we watch more TV in the day, it seems\\nto affect our work performance.\\nOf course, as we are now experts in statistics from the last two chapters, so we know that\\nthis might not be causational. A scatter plot may only work to reveal a correlation or an\\nassociation, but not a causation. Advanced statistical tests, such as the ones we saw in\\nChapter 8 , Advanced Statistics , might work to reveal causation. Later on in this chapter, we\\nwill see the damaging effects that trusting correlation might have.\\nLine graphs\\nLine graphs are, perhaps, one of the most  widely used graphs in data communication. A\\nline graph simply uses lines to connect data points and usually represents time on the x-\\naxis. Line graphs are a popular way to show changes in variables over time. The line graph,\\nlike the scatter plot, is used to plot quantitative  variables.\\n\\ns a great example, many of us wonder about the possible links between what we see on\\nTV and our behavior in the world. A friend of mine once took this thought to an extreme:\\nhe wondered if he could find a relationship between the TV show The X-Files  and the\\namount of UFO sightings in the U.S. He found the number of sightings of UFOs per year\\nand plotted them over time. He then added a quick graphic to ensure that readers would be\\nable to identify the point in time when The X-Files  were released:\\nTotal reported UFO sightings since 1963 (Source: http://www.questionable-economics.com/what-do-we-know-about-aliens/)\\nIt appears to be clear that right after 1993, the year of The X-Files\\'  premiere, the number of\\nUFO sightings started to climb drastically.\\nThis graphic, albeit light-hearted, is an excellent example of a simple line graph. We are\\ntold what each axis measures, we can quickly see a general trend in the data, and we can\\nidentify with the author\\'s intent, which is to show a relationship between the number of\\nUFO sightings and The X-Files\\'  premiere.\\n\\nn the other hand, the following is a less impressive line chart:\\nThe line graph: gas price changes\\nThis line graph attempts to highlight the change in the price of gas by plotting three points\\nin time. At first glance, it is not much different than the previous graph; we have time on\\nthe bottom x-axis and a quantitative value on the vertical y-axis. The (not so) subtle\\ndifference here is that the three points are equally spaced out on the x-axis; however, if we\\nread their actual time indications, they are not equally spaced out in time. A year separates\\nthe first two points whereas a mere 7 days separate the last two points.\\nBar charts\\nWe generally turn to bar charts when  trying to compare variables across different groups.\\nFor example, we can plot the number of countries per continent using a bar chart. Note how\\nthe x-axis does not represent a quantitative variable; in fact, when using a bar chart, the x-\\naxis is generally a categorical variable, while the y-axis is quantitative.\\n\\note that, for this code, I am using the World Health Organization\\'s report on alcohol\\nconsumption around the world by country:\\nfrom matplotlib import pyplot as plt\\ndrinks =\\npd.read_csv(\\'https://raw.githubusercontent.com/sinanuozdemir/principles_of_\\ndata_science/master/data/chapter_2/drinks.csv\\')\\ndrinks.continent.value_counts().plot(kind=\\'bar\\', title=\\'Countries per\\nContinent\\')\\nplt.xlabel(\\'Continent\\')\\nplt.ylabel(\\'Count\\')\\nThe following graph shows us a count of the number of countries in each continent. We can\\nsee the continent code at the bottom of the bars and the bar height represents the number of\\ncountries we have in each continent. For example, we see that Africa has the most countries\\nrepresented in our survey, while South America has the fewest:\\nBar chart: countries in continent\\nIn addition to the count of countries, we can also plot the average beer servings per\\ncontinent using a bar chart, as shown:\\ndrinks.groupby(\\'continent\\').beer_servings.mean().plot(kind=\\'bar\\')\\n\\nhe preceding code gives us this chart:\\nBar chart: average beer served per country\\nNote how a scatter plot or a line graph would not be able to support this data because they\\ncan only handle quantitative variables; bar graphs have the ability to demonstrate\\ncategorical values.\\nWe can also use bar charts to graph variables that change over time, like a line graph.\\nHistograms\\nHistograms show the frequency distribution  of a single quantitative variable by splitting\\nthe data, by range, into equidistant bins and plotting the raw count of observations in each\\nbin. A histogram is effectively a bar chart where the x-axis is a bin (subrange) of values and\\nthe y-axis is a count. As an example, I will import a store\\'s daily number of unique\\ncustomers, as shown:\\nrossmann_sales = pd.read_csv(\\'data/ rossmann.csv \\')\\nrossmann_sales.head()\\n\\ne get the following table:\\nNote how we have multiple store data (in the first Store  column). Let\\'s subset this data for\\nonly the first store, as shown:\\nfirst_rossmann_sales = rossmann_sales[rossmann_sales[\\'Store\\']==1]\\nNow, let\\'s plot a histogram of the first store\\'s customer count:\\nfirst_rossmann_sales[\\'Customers\\'].hist(bins=20)\\nplt.xlabel(\\'Customer Bins\\')\\nplt.ylabel(\\'Count\\')\\nThis is what we get:\\nHistogram: customer counts\\n\\nhe x-axis is now categorical in that each category is a selected range of values; for example,\\n600-620 customers would potentially be a category. The y-axis, like a bar chart, is plotting\\nthe number of observations in each category. In this graph, for example, one might take\\naway the fact that most of the time, the number of customers on any given day will fall\\nbetween 500 and 700.\\nAltogether, histograms are used to visualize the distribution of values that a quantitative\\nvariable can take on.\\nIn a histogram, we do not put spaces between bars.\\nBox plots\\nBox plots are also used to show  a distribution of values. They are created by plotting the\\nfive-number summary, as follows:\\nThe minimum value\\nThe first quartile (the number that separates the 25% lowest values from the rest)\\nThe median\\nThe third quartile (the number that separates the 25% highest values from the\\nrest)\\nThe maximum value\\nIn pandas, when we create box plots, the red line denotes the median, the top of the box (or\\nthe right if it is horizontal) is the third quartile, and the bottom (left) part of the box is the\\nfirst quartile.\\nThe following is a series of box plots showing the distribution of beer consumption\\naccording to continents:\\ndrinks.boxplot(column=\\'beer_servings\\', by=\\'continent\\')\\n\\ne get this graph:\\nBox plot: beer consumption by continent\\nNow, we can clearly see the distribution of beer consumption across the seven continents\\nand how they differ. Africa and Asia have a much lower median of beer consumption than\\nEurope or North America.\\nBox plots also have the added bonus of being able to show outliers much better than a\\nhistogram. This is because the minimum and maximum are parts of the box plot.\\nGetting back to the customer data, let\\'s look at the same store customer numbers, but using\\na box plot:\\nfirst_rossmann_sales.boxplot(column=\\'Customers\\', vert=False)\\n\\nhis is the graph we get:\\nBox plot: customer sales\\nThis is the exact same data as plotted earlier  in the histogram; however, now it is shown as\\na box plot. For the purpose of comparison, I will show you both graphs, one after the other:\\nHistogram: customer counts\\n\\nBox plot: customer sales\\nNote how the x-axis for each graph is the same, ranging from 0 to 1,200. The box plot is\\nmuch quicker at giving us a center for the data, the red line is the median, while the\\nhistogram works much better in showing us how spread out the data is and where people\\'s\\nbiggest bins are. For example, the histogram reveals that there is a very large bin of zero\\npeople. This means that for a little over 150 days of data, there were zero customers.\\nNote that we can get the exact numbers to construct a box plot using the describe  feature\\nin pandas, as shown:\\nfirst_rossmann_sales[\\'Customers\\'].describe()\\nmin         0.000000\\n25%       463.000000\\n50%       529.000000\\n75%       598.750000\\nmax      1130.000000\\n\\nhen graphs and statistics lie\\nI should be clear, statistics don\\'t lie, people lie. One of the easiest ways to trick your\\naudience is to confuse  correlation with  causation.\\nCorrelation versus causation\\nI don\\'t think I would be allowed  to publish this book without taking a deeper dive into the\\ndifferences between correlation and causation. For this example, I will continue to use my\\ndata for TV consumption and work performance.\\nCorrelation  is a quantitative metric  between -1 and 1 that measures how two variables move\\nwith each other . If two variables have a correlation close to -1, it means that as one variable\\nincreases, the other decreases, and if two variables have a correlation close to +1, it means\\nthat those variables move together in the same direction; as one increases, so does the other,\\nand the same when decreasing.\\nCausation  is the idea that one variable  affects another. For example, we can look at two\\nvariables: the average hours of TV watched in a day and a 0-100 scale of work performance\\n(0 being very poor performance and 100 being excellent performance). One might expect\\nthat these two factors are negatively correlated, which means that as the number of hours of\\nTV watched increases in a 24-hour day, your overall work performance goes down. Recall\\nthe code from earlier, which is as follows. Here, I am looking at the same sample of 14\\npeople as before and their answers to the question  how many hours of TV do you watch on\\naverage per night :\\nimport pandas as pd\\nhours_tv_watched = [0, 0, 0, 1, 1.3, 1.4, 2, 2.1, 2.6, 3.2, 4.1, 4.4, 4.4,\\n5]\\nThese are the same 14 people as mentioned earlier, in the same order, but now, instead of\\nthe number of hours of TV they watched, we have their work performance as graded by the\\ncompany or a third-party system:\\nwork_performance = [87, 89, 92, 90, 82, 80, 77, 80, 76, 85, 80, 75, 73, 72]\\nThen, we produce a DataFrame:\\ndf = pd.DataFrame({\\'hours_tv_watched\\':hours_tv_watched,\\n\\'work_performance\\':work_performance})\\n\\narlier, we looked at a scatter plot of these two variables and it seemed to clearly show a\\ndownward trend between the variables: as TV consumption went up, work performance\\nseemed to go down. However, a correlation coefficient, a number between -1 and 1, is a\\ngreat way to identify relationships between variables and, at the same time, quantify them\\nand categorize their strength.\\nNow, we can introduce a new line of code that shows us the correlation between these two\\nvariables:\\ndf.corr() # -0.824\\nRecall that a correlation, if close to -1, implies a strong negative correlation, while a\\ncorrelation close to +1 implies a strong positive correlation.\\nThis number helps support the hypothesis because a correlation coefficient close to -1\\nimplies not only a negative correlation but a strong one at that. Again, we can see this via a\\nscatter plot between the two variables. So, both our visual and our numbers are aligned\\nwith each other. This is an important concept that should be true when communicating\\nresults. If your visuals and your numbers are off, people are less likely to take your analysis\\nseriously:\\nCorrelation: hours of TV watched and work performance\\n\\n cannot stress enough that correlation and causation are different  from each other.\\nCorrelation simply quantifies the degree to which variables change together, whereas\\ncausation is the idea that one variable actually determines the value of another. If you wish\\nto share the results of the findings of your correlational work, you might be met with\\nchallengers in the audience asking for more work to be done. What is more terrifying is that\\nno one might know that the analysis is incomplete and you may make actionable decisions\\nbased on simple correlational work.\\nIt is very often the case that two variables might be correlated with each other but they do\\nnot have any causation between them. This can be for a variety of reasons, some of which\\nare as follows:\\nThere might be a confounding factor  between them. This means that there is a third\\nlurking variable that is not being factored and in, that acts as a bridge between\\nthe two variables. For example, previously we showed that you might find that\\nthe amount of TV you watch is negatively correlated with work performance,\\nthat is, as the number of hours of TV you watch increases, your overall work\\nperformance may decrease. That is a correlation. It doesn\\'t seem quite right to\\nsuggest that watching TV is the actual cause of a decrease in the quality of work\\nperformed. It might seem more plausible to suggest that there is a third factor,\\nperhaps hours of sleep every night, that might answer this question. Perhaps,\\nwatching more TV decreases the amount of time you have for sleep, which in\\nturn limits your work performance. The number of hours of sleep per night is the\\nconfounding factor.\\nThey might not have anything to do with each other! It might simply be a\\ncoincidence. There are many variables that are correlated but simply do not cause\\neach other. Consider the following example:\\n\\nCorrelation analysis: cheese consumption and civil engineering doctorates\\nIt is much more likely that these two variables only happen to correlate (more strongly than\\nour previous example, I may add) than cheese consumption determines the number of civil\\nengineering doctorates in the world.\\nYou have likely heard the statement correlation does not imply causation  and the last graph is\\nexactly the reason why data scientists must believe that. Just because there exists a\\nmathematical correlation between variables does not mean they have causation between\\nthem. There might be confounding factors between them or they just might not have\\nanything to do with each other!\\nLet\\'s see what happens when we ignore confounding variables and correlations become\\nextremely misleading.\\nSimpson\\'s paradox\\nSimpson\\'s paradox is a formal reason for why we need to take confounding variables\\nseriously. The paradox states that a correlation between two variables can be completely\\nreversed when we take different factors into account. This means that even if a graph might\\nshow a positive correlation, these variables can become anti-correlated  when another factor\\n(most likely a confounding one) is taken into consideration. This can be very troublesome to\\nstatisticians.\\n\\nuppose we wish to explore the relationship between two different splash pages (recall our\\nprevious A/B testing in Chapter 7 , Basic Statistics ). We will call these pages Page A  and\\nPage B  once again. We have two splash pages that we wish to compare and contrast, and\\nour main metric for choosing will be in our conversion rates, just as earlier.\\nSuppose we run a preliminary test and find the following conversion results:\\nPage A Page B\\n75% (263/350) 83% (248/300)\\nThis means that Page  B has almost a 10% higher conversion rate than Page A . So, right off\\nthe bat, it seems like Page B  is the better choice because it has a higher rate of conversion. If\\nwe were going to communicate this data to our colleagues, it would seem that we are in the\\nclear!\\nHowever, let\\'s see what happens when we also take into account the coast that the user was\\ncloser to, as shown:\\nPage A Page B\\nWest Coast 95% (76 / 80) 93% (231/250)\\nEast Coast 72% (193/270) 34% (17/50)\\nBoth 75% (263/350) 83% (248/300)\\nThus the paradox! When we break the sample down by location, it seems that Page A  was\\nbetter in both categories but was worse overall. That\\'s the beauty and, also, the horrifying\\nnature of the paradox. This happens because of the unbalanced classes between the four\\ngroups.\\nThe Page A /East Coast  group and the Page B /West Coast  group are providing most of the\\npeople in the sample, therefore skewing the results to not be as expected. The confounding\\nvariable here might be the fact that the pages were given at different hours of the day and\\nthe West coast people were more likely to see Page B , while the East coast people were\\nmore likely to see Page A .\\nThere is a resolution to Simpson\\'s paradox (and therefore an answer); however, the proof\\nlies in a complex system of Bayesian networks and is a bit out of the scope of this book.\\n\\nhe main takeaway from Simpson\\'s paradox is that we should not unduly give causational\\npower to correlated variables. There might be confounding variables that have to be\\nexamined. Therefore, if you are able to reveal a correlation between two variables (such as\\nwebsite category and conversion rate or TV consumption and work performance), then you\\nshould absolutely try to isolate as many variables as possible that might be the reason for\\nthe correlation, or can at least help explain your case further.\\nIf correlation doesn\\'t imply causation, then what\\ndoes?\\nAs a data scientist, it is often quite  frustrating to work with correlations and not be able to\\ndraw conclusive causality. The best way to confidently obtain causality is, usually, through\\nrandomized experiments, such as the ones we saw in Chapter 8 , Advanced Statistics . One\\nwould have to split up the population groups into randomly sampled groups and run\\nhypothesis tests to conclude, with a degree of certainty, that there is a true causation\\nbetween variables.\\nVerbal communication\\nApart from visual demonstrations of data, verbal communication is just as important when\\npresenting results. If you are not merely uploading results or publishing, you are usually\\npresenting data  to a room of data scientists, executives, or to a conference hall.\\nIn any case, there are key areas to focus on when giving a verbal presentation, especially\\nwhen the presentation is regarding findings of data.\\nThere are generally two styles of oral presentation: one meant for more professional\\nsettings, including corporate  offices where the problem at hand is usually tied directly to\\ncompany performance or some other key performance indicator  (KPI), and another meant\\nmore for a room of your peers where the key idea is to motivate the audience to care about\\nyour work.\\nIt\\'s about telling a story\\nWhether it is a formal or casual presentation, people like to hear stories. When you are\\npresenting results, you are not just spitting out facts and metrics, you are attempting to \\nframe  the minds of your audience to believe in and care about what you have to say.\\n\\nhen giving a presentation, always be aware of your audience and try to gauge their\\nreactions/interest in what you are saying. If they seem unengaged, try to relate the problem\\nto them:\\n\"Just think, when popular TV shows like Game of Thrones come back, your employees will all spend\\nmore time watching TV and therefore will have lower work performance .\"\\nNow you have their attention. It\\'s about relating to your audience; whether it\\'s your boss or\\nyour mom\\'s friend, you have to find a way to make it relevant.\\nOn the more formal side of things\\nWhen presenting data findings to a more  formal audience, I like to stick to the following six\\nsteps:\\nOutline the state of the problem : In this step, we go over the current state of the 1.\\nproblem, including what the problem is and how the problem came to the\\nattention of the team of data scientists.\\nDefine the nature of the data : Here, we go into more depth about who this 2.\\nproblem affects, how the solution would change the situation, and previous work\\ndone on the problem, if any.\\nDivulge an initial hypothesis : Here, we state what we believed to be the 3.\\nsolution before doing any work. This might seem like a more novice approach to\\npresentations; however, this can be a good time to outline not just your initial\\nhypothesis but, perhaps, the hypothesis of the entire company. For example, \"we\\ntook a poll and 61% of the company believes there is no correlation between\\nhours of TV watched and work performance.\"\\nDescribe the solution and, possibly, the tools that led to the solution : Get into 4.\\nhow you solved the problem, any statistical tests used, and any assumptions that\\nwere made during the course of the problem.\\nShare the impact that your solution will have on the problem : Talk about 5.\\nwhether your solution was different from the initial hypothesis. What will this\\nmean for the future? How can we take action on this solution to improve\\nourselves and our company?\\nFuture steps : Share what future steps can be taken with the problem, such as 6.\\nhow to implement the solution and what further work this research sparked.\\n\\ny following these steps, we can hit on all of the major areas of the data science method.\\nThe first thing you want to hit on during a formal presentation is action. You want your\\nwords and solutions to be actionable. There must be a clear path to take upon the\\ncompletion of the project and the future steps should be defined.\\nThe why/how/what strategy of presenting\\nWhen speaking on a less formal level, the why/how/what strategy is a quick and easy way\\nto create a presentation worthy of praise. It is quite simple, as shown:\\nThis model is borrowed from famous  advertisements. The kind where they would not even\\ntell you what the product was until there were three seconds left. They want to catch your\\nattention and then, finally, reveal what it was that was so exciting. Consider the following\\nexample:\\n\"Hello everyone. I am here to tell you about why we seem to have a hard time focusing on our job\\nwhen the Olympics are being aired. After mining survey results and merging this data with\\ncompany-standard work performance data, I was able to find a correlation between the number of\\nhours of TV watched per day and average work performance. Knowing this, we can all be a bit more\\naware of our TV watching habits and make sure we don\\'t let it affect our work. Thank you.\"\\nThis chapter was actually formatted in this way! We started with why we should care about\\ndata communication, then we talked about how to accomplish it (through correlation,\\nvisuals, and so on), and finally I am telling you the what , which is the why/how/what\\nstrategy (insert mind-blowing sound effect here).\\nSummary\\nData communication is not an easy task. It is one thing to understand the mathematics of\\nhow data science works, but it is a completely different thing to try to convince a room of\\ndata scientists and non-data scientists alike of your results and their value to them. In this\\nchapter, we went over basic chart making, how to identify faulty causation, and how to\\nhone our oral presentation skills.\\nOur next few chapters will really begin to hit at one of the biggest talking points of data\\nscience. In the last nine chapters, we spoke about everything related to how to obtain data,\\nclean data, and visualize data in order to gain a better understanding of the environment\\nthat the data represents.\\n\\ne then turned to look at the basic and advanced probability/statistics laws in order to use\\nquantifiable theorems and tests on our data to get actionable results and answers.\\nIn subsequent chapters, we will take a look into machine learning and the situations in\\nwhich machine learning performs well and doesn\\'t perform well. As we take a journey into\\nthis material, I urge you, the reader, to keep an open mind and truly understand not just\\nhow machine learning works, but also why we need to use it.\\n\\n0\\nHow to Tell If Your Toaster Is\\nLearning – Machine Learning\\nEssentials\\nMachine learning has become quite the phrase of the decade. It seems as though every time\\nwe hear about the next great start-up or turn on the news, we hear something about a\\nrevolutionary piece of machine learning technology and how it will change the way we\\nlive.\\nThis chapter focuses on machine learning as a practical part of data science. We will cover\\nthe following topics in this chapter:\\nDefining the different types of machine learning, along with examples of each\\nkind\\nRegression and classification\\nWhat is machine learning and how is it used in data science?\\nThe differences between machine learning and statistical modeling and how\\nmachine learning is a broad category of the latter\\nOur aim will be to utilize statistics, probability, and algorithmic thinking in order to\\nunderstand and apply essential machine learning skills to practical industries, such as\\nmarketing. Examples will include predicting star ratings of restaurant reviews, predicting\\nthe presence of a disease, spam email detection, and much more. This chapter focuses on\\nmachine learning as a whole and as a single statistical model. The subsequent chapters will\\ndeal with many more models, some of which are much more complex.\\nWe will also turn our focus on metrics, which tell us how effective our models are. We will\\nuse metrics in order to conclude results and make predictions using machine learning.\\n\\nhat is machine learning?\\nIt wouldn\\'t make sense to continue without a concrete definition of what machine learning\\nis. Well, let\\'s back up for a minute. In Chapter 1 , How to Sound Like a Data Scientist , we\\ndefined machine learning as giving computers the ability to learn  from data without being\\ngiven explicit rules by a programmer. This definition still holds true. Machine learning is\\nconcerned with the ability to ascertain certain patterns (signals) out of data, even if the data\\nhas inherent errors in it (noise).\\nMachine learning models are able to learn from data without the explicit help of a human.\\nThat is the main difference between machine learning models and classical algorithms.\\nClassical algorithms are told how to find the best answer in a complex system and the\\nalgorithm then searches for these best solutions and often works faster and more efficiently\\nthan a human. However, the bottleneck here is that the human has to first come up with the\\nbest solution. In machine learning, the model is not told the best solution and, instead, is\\ngiven several examples of the problem and is told to figure out the best solution.\\nMachine learning is just another tool in the belt of a data scientist. It is on the same level as\\nstatistical tests (chi-square or t-tests) or uses base probability/statistics to estimate\\npopulation parameters. Machine learning is often regarded as the only thing data scientists\\nknow how to do, and this is simply untrue. A true data scientist is able to recognize when\\nmachine learning is applicable and, more importantly, when it is not.\\nMachine learning is a game of correlations and relationships. Most machine learning\\nalgorithms in existence are concerned with finding and/or exploiting relationships between\\ndatasets (often represented as columns in a pandas DataFrame). Once machine learning\\nalgorithms can pinpoint certain correlations, the model can either use these relationships to\\npredict future observations or generalize the data to reveal interesting patterns.\\nPerhaps a great way to explain machine learning is to offer an example of a problem\\ncoupled with two possible solutions: one using a machine learning algorithm and the other\\nutilizing a non-machine learning algorithm.\\nExample – facial recognition\\nThis problem is very well documented. Given a picture of a face, who does it belong to?\\nHowever, I argue that there  is a more important question that must be asked even before\\nthis. Suppose you wish to implement a home security system that recognizes who is\\nentering your house. Most likely, during the day, your house will be empty most of the\\ntime and the facial recognition will kick in only if there is a person in the shot. This is\\nexactly the question I propose we try and solve —given a photo, is there a face in it?\\n\\niven this problem, I propose the following two solutions:\\nA non-machine learning algorithm that will define a face as having a roundish\\nstructure, two eyes, hair, nose, and so on. The algorithm then looks for these\\nhard-coded features in the photo and returns whether or not it was able to find\\nany of these features.\\nA machine learning algorithm that will work a bit differently. The model will\\nonly be given several pictures of faces and non-faces that are labeled as such.\\nFrom the examples (called training sets), it would figure out its own definition of\\na face.\\nThe machine learning version of the solution is never told what a face is, it is merely given\\nseveral examples, some with faces, and some without. It is then up to the machine learning\\nmodel to figure out the difference between the two. Once it figures out the difference\\nbetween the two, it uses this information to take in a picture and predict whether or not\\nthere is a face in the new picture. For example, to train the model, we will give it the\\nfollowing three photographs:\\nImages for training machine learning model\\n\\nhe model will then figure out the difference between the pictures labeled as Face  and the\\npictures labeled as No Face  and be able to use that difference to find faces in future photos.\\nMachine learning isn\\'t perfect\\nThere are many caveats of machine  learning. Many are specific to different models being\\nimplemented, but there are some assumptions that are universal for any machine learning\\nmodel:\\nThe data used, for the most part, is preprocessed and cleaned using the methods\\noutlined in the earlier chapters. Almost no machine learning model will tolerate\\ndirty data with missing values or categorical values. Use dummy variables and\\nfilling/dropping techniques to handle these discrepancies.\\nEach row of a cleaned dataset represents a single observation of the environment\\nwe are trying to model.\\nIf our goal is to find relationships between variables, then there is an assumption\\nthat there is some kind of relationship between these variables.\\nThis assumption is particularly important. Many machine learning models take\\nthis assumption very seriously. These models are not able to communicate that\\nthere might not be a relationship.\\nMachine learning models are generally considered semi-automatic, which means\\nthat intelligent decisions by humans are still needed.\\nThe machine is very smart but has a hard time putting things into context. The\\noutput of most models is a series of numbers and metrics attempting to quantify\\nhow well the model did. It is up to a human to put these metrics into perspective\\nand communicate the results to an audience.\\nMost machine learning models are sensitive to noisy data. This means that the\\nmodels get confused when you include data that doesn\\'t make sense. For\\nexample, if you are attempting to find relationships between economic data\\naround the world and one of your columns is puppy adoption rates in the capital\\ncity, that information is likely not to be relevant and will confuse the model.\\nThese assumptions will come up again and again when dealing with machine learning.\\nThey are all too important and are often ignored by novice data scientists.\\n\\now does machine learning work?\\nEach flavor of machine  learning and each individual model works in very different ways,\\nexploiting different parts of mathematics and data science. However, in general, machine\\nlearning works by taking in data, finding relationships within the data, and giving as\\noutput what the model learned, as illustrated in the following diagram:\\nAn overview of machine learning models\\nAs we explore the different types of machine learning models, we will see how they\\nmanipulate data differently and come up with different outputs for different applications.\\nTypes of machine learning\\nThere are many ways to segment machine  learning and dive deeper. In Chapter 1 , How to\\nSound Like a Data Scientist , I mentioned statistical and probabilistic models. These models\\nutilize statistics and probability, which we\\'ve seen in the previous chapters, in order to find\\nrelationships between data and make predictions. In this chapter, we will implement both\\ntypes of models. In the following chapter, we will see machine learning outside the rigid\\nmathematical world of statistics/probability. You can segment machine learning models by\\ndifferent characteristics, including the following:\\nThe types of data/organic structures they utilize (tree/graph/neural network)\\nThe field of mathematics they are most related to (statistical/probabilistic)\\nThe level of computation required to train (deep learning)\\n\\nor the purpose of education, I will offer my own breakdown of machine learning models.\\nBranching off from  the top level of machine learning, there are the following three subsets:\\nSupervised learning\\nUnsupervised learning\\nReinforcement learning\\nSupervised learning\\nSimply put, supervised learning finds  associations between features of a dataset and a\\ntarget variable. For example, supervised learning models might try to find the association\\nbetween a person\\'s health features (heart rate, obesity level, and so on) and that person\\'s\\nrisk of having a heart attack (the target variable).\\nThese associations allow supervised models to make predictions based on past examples.\\nThis is often the first thing that comes to people\\'s minds when they hear the phrase,\\nmachine learning, but it in no way does it encompass the realm of machine learning.\\nSupervised machine learning models are often called predictive analytics models , named\\nfor their ability to predict the future based on the past.\\nSupervised machine learning requires  a certain type of data called labeled data . This means\\nthat we must teach our model by giving it historical examples that are labeled with the\\ncorrect answer. Recall the facial recognition example. That is a supervised learning model\\nbecause we are training our model with the previous pictures labeled as either face or not\\nface, and then asking the model to predict whether or not a new picture has a face in it.\\nSpecifically, supervised learning works using parts of the data to predict another part. First,\\nwe must separate data into two parts, as follows:\\nThe predictors, which are the columns that will be used to make our prediction.\\nThese are sometimes called features, input values, variables, and independent\\nvariables.\\nThe response, which is the column that we wish to predict. This is sometimes\\ncalled outcome, label, target, and dependent variable.\\nSupervised learning attempts to find a relationship between the predictors and the\\nresponse in order to make a prediction. The idea is that, in the future, a data observation\\nwill present itself and we will only know the predictors. The model will then have to use\\nthe predictors to make an accurate prediction of the response value.\\n\\nxample – heart attack prediction\\nSuppose we wish to predict whether  someone will have a heart attack within a year. To\\npredict this, we are given that person\\'s cholesterol level, blood pressure, height, their\\nsmoking habits, and perhaps more. From this data, we must ascertain the likelihood of a\\nheart attack. Suppose, to make this prediction, we look at previous patients and their\\nmedical history. As these are previous patients, we know not only their predictors\\n(cholesterol, blood pressure, and so on), but we also know if they actually had a heart\\nattack (because it already happened!).\\nThis is a supervised machine learning problem because we are doing the following:\\nWe are making a prediction about someone\\nWe are using historical training data to find relationships between medical\\nvariables and heart attacks:\\nAn overview of supervised models\\nThe hope here is that a patient will walk in tomorrow and our model will be able to identify\\nwhether or not the patient is at risk for a heart attack based on her/his conditions (just like a\\ndoctor would!).\\n\\ns the model sees more and more labeled data, it adjusts itself in order to match the correct\\nlabels given to us. We can use different metrics (explained later in this chapter) to pinpoint\\nexactly how well our supervised machine learning model is doing and how it can better\\nadjust itself.\\nOne of the biggest drawbacks of supervised  machine learning is that we need this labeled\\ndata, which can be very difficult to get a hold of. Suppose we wish to predict heart attacks;\\nwe might need thousands of patients along with all of their medical information and years\\'\\nworth of follow-up records for each person, which could be a nightmare to obtain.\\nIn short, supervised models use historical labeled data in order to make predictions about\\nthe future. Some possible applications for supervised learning include the following:\\nStock price predictions\\nWeather predictions\\nCrime predictions\\nNote how each of the preceding  examples uses the word prediction , which makes sense\\nseeing how I emphasized supervised learning\\'s ability to make predictions about the\\nfuture. Predictions, however, are not where the story ends.\\nHere is a visualization of how supervised models use labeled data to fit themselves and\\nprepare themselves to make predictions:\\nNote how the supervised model learns from a bunch of training data and then, when it is\\nready, it looks at unseen cases and outputs a prediction.\\n\\nt\\'s not only about predictions\\nSupervised learning exploits  the relationship between the predictors and the response to\\nmake predictions, but sometimes, it is enough just knowing that there even is a\\nrelationship. Suppose we are using a supervised machine learning model to predict\\nwhether or not a customer will purchase a given item. A possible dataset might look as\\nfollows:\\nPerson ID Age Gender Employed? Bought the product?\\n1 63 F N Y\\n2 24 M Y N\\nNote that, in this case, our predictors are Age, Gender, and Employed?, while our response\\nis Bought the product? This is because we want to see if, given someone\\'s age, gender, and\\nemployment status, they will buy the product.\\nAssume that a model is trained on this data and can make accurate predictions about\\nwhether or not someone will buy something. That, in and of itself, is exciting, but there\\'s\\nsomething else that is arguably even more exciting. The fact that we could make accurate\\npredictions implies that there is a relationship between these variables, which means that to\\nknow if someone will buy your product, you only need to know their age, gender, and\\nemployment status! This might contradict the previous market research, indicating that\\nmuch more must be known about a potential customer to make such a prediction.\\nThis speaks to supervised machine learning\\'s ability to understand which predictors affect\\nthe response and how. For example, are women more likely to buy the product, w hich age\\ngroups are prone to decline the product, is there a combination of age and gender that is a\\nbetter predictor than any one column on its own? As someone\\'s age increases, do their\\nchances of buying the product go up, down, or stay the same?\\nIt is also possible that all of the columns are not necessary. A possible output of a machine\\nlearning might suggest that only certain columns are necessary to make the prediction and\\nthat the other columns are only noise (they do not correlate to the response and therefore\\nconfuse the model).\\n\\nypes of supervised learning\\nThere are, in general, two types  of supervised learning models: regression  and\\nclassification . The difference between the two is quite  simple and lies in the response\\nvariable.\\nRegression\\nRegression models attempt to predict  a continuous response. This means that the response\\ncan take on a range of infinite values. Consider the following examples:\\nDollar amounts\\nSalary\\nBudget\\nTemperature\\nTime\\nGenerally recorded in seconds or minutes\\nClassiﬁcation\\nClassification  attempts to predict  a categorical response, which means that the response\\nonly has a finite amount of choices. Here are some  examples:\\nCancer grade (1, 2, 3, 4, 5)\\nTrue/false questions, such as the following examples:\\n\"Will this person have a heart attack within a year?\"\\n\"Will you get this job?\"\\nGiven a photo of a face, who does this face belong to? (facial recognition)\\nPredict the year someone was born:\\nNote that there are many possible answers (over 100) but still\\nfinitely many more\\nExample – regression\\nThe following graphs show a relationship between three categorical variables (age, year\\nthey were born, and education level) and a person\\'s wage:\\n\\nRegression examples (source: https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/introduction.pdf)\\nNote that, even though each predictor is categorical, this example is regressive because the\\ny axis, our dependent variable, our response, is continuous.\\nOur earlier heart attack example is classification because the response was: will this person\\nhave a heart attack within a year?  This has only two possible answers: Yes or No.\\nData is in the eyes of the beholder\\nSometimes, it can be tricky to decide  whether or not you should use classification or\\nregression. Consider that we are interested in the weather outside. We could ask the\\nquestion, how hot is it outside?  In this case, your answer is on a continuous scale, and some\\npossible answers are 60.7 degrees or 98 degrees. However, as an exercise, go and ask 10\\npeople what the temperature is outside. I guarantee you that someone (if not most people)\\nwill not answer in some exact degrees but will bucket their answer and say something like\\nit\\'s in the 60s .\\nWe might wish to consider this problem as a classification problem, where the response\\nvariable is no longer in exact degrees but is in a bucket. There would only be a finite\\nnumber of buckets in theory, making the model perhaps learn the differences between 60s\\nand 70s a bit better.\\n\\nnsupervised learning\\nThe second type of machine learning  does not deal  with predictions but has a much more\\nopen objective. Unsupervised learning takes in a set of predictors and utilizes relationships\\nbetween the predictors in order to accomplish tasks such as the following:\\nIt reduces the dimension of the data by condensing variables together. An\\nexample of this would be file compression. Compression works by utilizing\\npatterns in the data and representing the data in a smaller format.\\nIt finds groups of observations that behave similarly and groups them together.\\nThe first element on this list is called dimension reduction  and the second is called\\nclustering . Both of these are examples of unsupervised learning because they do not\\nattempt to find a relationship between predictors and a specific response and therefore are\\nnot used to make predictions of any kind. Unsupervised models, instead, are utilized to\\nfind organizations and representations of the data that were previously unknown.\\nThe following screenshot is a representation of a cluster analysis:\\nExample of cluster analysis\\n\\nhe model will recognize that each uniquely colored cluster of observations is similar to\\nanother but different from the other clusters.\\nA big advantage for unsupervised learning is that it does not require labeled data, which\\nmeans that it is much easier to get data that complies with unsupervised learning models.\\nOf course, a drawback to this is that we lose all predictive power because the response\\nvariable holds the information to make predictions and, without it, our model will be\\nhopeless in making any sort of predictions.\\nA big drawback is that it is difficult  to see how well  we are doing. In a regression or\\nclassification problem, we can easily tell how well our models are predicting by comparing\\nour models\\' answers to the actual answers. For example, if our supervised model predicts\\nrain and it is sunny outside, the model was incorrect. If our supervised model predicts the\\nprice will go up by 1 dollar and it goes up by 99 cents, our model was very close! In\\nsupervised modeling, this concept is foreign because we have no answer to compare our\\nmodels to. Unsupervised models are merely suggesting differences and similarities, which\\nthen require a human\\'s interpretation:\\nAn overview of unsupervised models\\nIn short, the main goal of unsupervised models is to find similarities and differences\\nbetween data observations. We will discuss unsupervised models in depth in later chapters.\\n\\neinforcement learning\\nIn reinforcement learning, algorithms get to choose  an action in an environment and then\\nare rewarded (positively or negatively) for choosing this action. The algorithm then  adjusts\\nitself and modifies its strategy in order to accomplish some goal, which is usually to get\\nmore rewards.\\nThis type of machine  learning is very popular in AI-assisted gameplay as agents  (the AI) are\\nallowed to explore a virtual world and collect rewards and learn the best navigation\\ntechniques. This model is also popular in robotics, especially in the field of self-automated\\nmachinery, including cars:\\nSelf-driving cars ( image source: https://www.quora.com/How-do-Googles-self-driving-cars-work)\\n\\nelf-driving cars read in sensor input, act accordingly, and are then rewarded for taking a\\ncertain action. The car then adjusts its behavior to collect more rewards. It can be thought\\nthat reinforcement is similar to supervised learning in that the agent is learning  from its \\npast actions to make better moves in the future; however, the main difference lies in the\\nreward. The reward does not have to be tied in any way to a correct  or incorrect  decision.\\nThe reward simply encourages (or discourages) different actions.\\nReinforcement learning is the least explored of the three types of machine learning and\\ntherefore is not explored in great length in this text. The remainder of this chapter will focus\\non supervised and unsupervised learning.\\nOverview of the types of machine learning\\nOf the three types of machine  learning —supervised, unsupervised, and reinforcement\\nlearning —we can imagine the world of machine learning as something like this:\\nEach of the three types of machine learning has its benefits and its drawbacks, as listed:\\nSupervised machine learning : This exploits  relationships between predictors \\nand response variables to make predictions of future data observations. The pros\\nare as follows:\\nIt can make future predictions\\nIt can quantify relationships between predictors and response\\nvariables\\nIt can show us how variables affect each other and how much\\n\\nhe cons are as follows:\\nIt requires labeled data (which can be difficult to get)\\nUnsupervised machine learning : This finds  similarities and differences between\\ndata points. The pros are as follows:\\nIt can find groups of data  points that behave similarly that a\\nhuman would never have noted.\\nIt can be a preprocessing step for supervised learning.\\nThink of clustering a bunch of data points and then using these\\nclusters as the response.\\nIt can use unlabeled data, which is much easier to find.\\nThe cons are as follows:\\nIt has zero predictive power\\nIt can be hard to determine if we are on the right track\\nIt relies much more on human interpretation\\nReinforcement learning : This is reward-based learning that encourages agents to\\ntake particular actions  in their environments. The pros are as follows:\\nVery complicated rewards systems create very complicated AI\\nsystems\\nIt can learn in almost any environment, including our own Earth\\nThe cons are as follows:\\nThe agent is erratic at first and makes many terrible choices before\\nrealizing that these choices have negative rewards\\nFor example, a car might crash into a wall and not know that that is\\nnot okay until the environment negatively rewards it\\nIt can take a while before the agent avoids decisions altogether\\nThe agent might play it safe and only choose one action and be\\n\"too afraid\" to try anything else for fear of being punished\\n\\now does statistical modeling fit into all of\\nthis?\\nUp until now, I have been using the term  machine learning, but you may ask how statistical\\nmodeling plays a role in all of this.\\nThis is still a debated topic in the field of data science. I believe that statistical modeling is\\nanother term for machine learning models that heavily relies on using mathematical rules\\nborrowed from probability and statistics to create relationships between data variables\\n(often in a predictive sense).\\nThe remainder of this chapter will focus mostly on one statistical/probabilistic\\nmodel —linear regression.\\nLinear regression\\nFinally! We will explore our first true machine learning model. Linear regression is a form\\nof regression, which means that it is a machine learning model that attempts to find a\\nrelationship between predictors and a response variable and that response variable is, you\\nguessed it, continuous! This notion is synonymous with making a line of best fit .\\nIn the case of linear regression, we will attempt to find a linear relationship between our\\npredictors and our response variable. Formally, we wish to solve a formula of the following\\nformat:\\nLet\\'s look at the constituents of this formula:\\ny is our response variable\\nxi is our ith variable ( ith column or ith predictor)\\nB0 is the intercept\\nBi is the coefficient for the xi term\\n\\net\\'s take a look at some data before we go in depth. This dataset is publically available and\\nattempts to predict  the number of bikes needed on a particular day for a bike sharing\\nprogram:\\n# read the data and set the datetime as the index\\n# taken from Kaggle: https://www.kaggle.com/c/bike-sharing-demand/data\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nurl =\\n\\'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/bikeshare.c\\nsv\\'\\nbikes = pd.read_csv(url)\\nbikes.head()\\nFollowing is the output:\\nWe can see that every row represents a single hour of bike usage. In this case, we are\\ninterested in predicting count , which represents the total number of bikes rented in the\\nperiod of that hour.\\nLet\\'s, for example, look at a scatter plot between temperature (the temp  column) and\\ncount , as shown:\\nbikes.plot(kind=\\'scatter\\', x=\\'temp\\', y=\\'count\\', alpha=0.2)\\n\\ne get the following graph as output:\\nAnd now, let\\'s use a module, called seaborn , to draw ourselves a line of best fit, as follows:\\nimport seaborn as sns #using seaborn to get a line of best fit\\nsns.lmplot(x=\\'temp\\', y=\\'count\\', data=bikes, aspect=1.5,\\nscatter_kws={\\'alpha\\':0.2})\\nFollowing is the output:\\n\\n\\nhis line in the graph attempts to visualize and quantify the relationship between temp  and\\ncount . To make a prediction, we simply find a given temperature and then see where the\\nline would predict the count . For example, if the temperature is 20 degrees (Celsius, mind\\nyou), then our line would predict that about 200 bikes will be rented. If the temperature is\\nabove 40 degrees, then more than 400 bikes will be needed!\\nIt appears that as temp  goes up, our count  also goes up. Let\\'s see if our correlation value,\\nwhich quantifies a linear relationship between variables, also matches this notion:\\nbikes[[\\'count\\', \\'temp\\']].corr()\\n# 0.3944\\nThere is a (weak) positive correlation between the two variables! Now, let\\'s go back to the\\nform of the linear regression:\\nOur model will attempt to draw a perfect  line between all of the dots in the preceding\\ngraph but, of course, we can clearly see that there is no perfect line between these dots! The\\nmodel will then find the best fit  line possible. How? We can draw infinite lines between the\\ndata points, but what makes a line the best?\\nConsider the following diagram:\\nIn our model, we are given the x and the y and the model learns  the beta coefficients, also\\nknown as model coefficients :\\nThe black dots are the observed values of x and y.\\nThe blue line is our line of best fit.\\nThe red lines between the dots and the line are called the residuals; they are the\\ndistances between the observed values and the line. They are how wrong the line\\nis.\\n\\nach data point has a residual or a distance to the line of best fit. The sum of squared\\nresiduals  is the summation of each residual squared. The best fit line has the smallest sum\\nof squared residual value. Let\\'s build this line in Python:\\n# create X and y\\nfeature_cols = [\\'temp\\'] # a list of the predictors\\nX = bikes[feature_cols] # subsetting our data to only the predictors\\ny = bikes[\\'count\\'] # our response variable\\nNote how we made an X and a y variable. These represent our predictors and our response\\nvariable. Then, we will import our machine learning module, scikit-learn , as shown:\\n# import scikit-learn, our machine learning module\\nfrom sklearn.linear_model import LinearRegression\\nFinally, we will fit our model to the predictors and the response variable, as follows:\\nlinreg = LinearRegression() #instantiate a new model\\nlinreg.fit(X, y) #fit the model to our data\\n# print the coefficients\\nprint(linreg.intercept_)\\nprint(linreg.coef_)\\n6.04621295962  # our Beta_0\\n[ 9.17054048]     # our beta parameters\\nLet\\'s interpret this:\\nB0 (6.04)  is the value of y when X = 0\\nIt is the estimation of bikes that will be rented when the temperature is 0 degrees\\nCelsius\\nSo, at 0 degrees, six bikes are predicted to be in use (it\\'s cold!)\\nSometimes, it might not make sense  to interpret the intercept at all because there might not\\nbe a concept of zero of something. Recall the levels of data. Not all levels have this notion.\\nTemperature exists at a level that has the inherent notion of no bikes ; so, we are safe. Be\\ncareful in the future though and verify results: \\nB1 (9.17)  is our temperature coefficient.\\nIt is the change in y divided by the change in x1.\\nIt represents how x and y move together.\\n\\n change in 1 degree Celsius is associated with an increase of about nine bikes\\nrented.\\nThe sign of this coefficient is important. If it were negative, that would imply that\\na rise in temperature is associated with a drop in rentals.\\nConsider the following representation of the beta coefficients in a linear regression:\\nIt is important to reiterate that these  are all statements of correlation and not a statement of\\ncausation. We have no means of stating whether or not the rental increase is caused by the\\nchange in temperature, it is just that there appears to be movement together.\\nUsing scikit-learn  to make predictions is easy:\\nlinreg.predict(20)\\n# 189.4570\\nThis means that 190 bikes will likely be rented if the temperature is 20 degrees.\\nAdding more predictors\\nAdding more predictors to the model  is as simple as telling the linear regression model in\\nscikit-learn  about them!\\nBefore we do, we should look at the data dictionary provided to us to make more sense out\\nof these predictors:\\nseason : 1 = spring, 2 = summer, 3 = fall, and 4 = winter\\nholiday : Whether the day is considered a holiday\\nworkingday : Whether the day is a weekend or holiday\\n\\neather :\\nClear , Few clouds , Partly cloudy\\nMist + Cloudy , Mist + Broken clouds , Mist + Few\\nclouds , Mist\\nLight Snow , Light Rain + Thunderstorm + Scattered\\nclouds , Light Rain + Scattered clouds\\nHeavy Rain  + Ice Pallets  + Thunderstorm  + Mist , Snow +\\nFog\\ntemp : Temperature in Celsius\\natemp : Feels like temperature in Celsius\\nhumidity : Relative humidity\\nwindspeed : Wind speed\\ncasual : Number of non-registered user rentals initiated\\nregistered : Number of registered user rentals initiated\\ncount : Number of total rentals\\nNow let\\'s actually create our linear  regression model. As before, we will first create a list\\nholding the features we wish to look at, create our features and our response datasets ( X\\nand y), and then fit our linear regression. Once we fit our regression model, we will take a\\nlook at the model\\'s coefficients in order to see how our features are interacting with our\\nresponse:\\n# create a list of features\\nfeature_cols = [\\'temp\\', \\'season\\', \\'weather\\', \\'humidity\\']\\n# create X and y\\nX = bikes[feature_cols]\\ny = bikes[\\'count\\']\\n# instantiate and fit\\nlinreg = LinearRegression()\\nlinreg.fit(X, y)\\n# pair the feature names with the coefficients\\nresult = zip(feature_cols, linreg.coef_)\\nresultSet = set(result)\\nprint(resultSet)\\n\\nhis gives us the following output:\\n[(\\'temp\\', 7.8648249924774403),\\n (\\'season\\', 22.538757532466754),\\n (\\'weather\\', 6.6703020359238048),\\n (\\'humidity\\', -3.1188733823964974)]\\nAnd this is what that means:\\nHolding all other predictors constant, a 1 unit increase in temperature is\\nassociated with a rental increase of 7.86 bikes\\nHolding all other predictors constant, a 1 unit increase in season is associated\\nwith a rental increase of 22.5 bikes\\nHolding all other predictors constant, a 1 unit increase in weather is associated\\nwith a rental increase of 6.67 bikes\\nHolding all other predictors constant, a 1 unit increase in humidity is associated\\nwith a rental decrease of 3.12 bikes\\nThis is interesting. Note that, as weather  goes up (meaning that the weather is getting\\ncloser to overcast), the bike demand goes up, as is the case when the season variables\\nincrease (meaning that we are approaching winter). This is not what I was expecting at all!\\nLet\\'s take a look at the individual scatter  plots between each predictor and the response, as\\nillustrated:\\nfeature_cols = [\\'temp\\', \\'season\\', \\'weather\\', \\'humidity\\']\\n# multiple scatter plots\\nsns.pairplot(bikes, x_vars=feature_cols, y_vars=\\'count\\', kind=\\'reg\\')\\nWe get the following output:\\n\\n\\note how the weather line is trending downwards, which is the opposite of what the last\\nlinear model was suggesting. Now, we have to worry about which of these predictors are\\nactually helping us make the prediction, and which ones are just noise. To do so, we\\'re\\ngoing to need some more advanced metrics.\\nRegression metrics\\nThere are three main metrics  when using regression  machine learning models. They are as\\nfollows:\\nThe mean absolute error\\nThe mean squared error\\nThe root mean squared error\\nEach metric attempts to describe and quantify the effectiveness of a regression model by\\ncomparing a list of predictions to a list of correct answers. Each of the following mentioned\\nmetrics is slightly different from the rest and tells a different story:\\nLet\\'s look at the coefficients:\\nn is the number of observations\\nyi is the actual value\\nŷ is the predicted value\\n\\net\\'s take a look in Python:\\n# example true and predicted response values\\ntrue = [9, 6, 7, 6]\\npred = [8, 7, 7, 12]\\n# note that each value in the last represents a single prediction for a\\nmodel\\n# So we are comparing four predictions to four actual answers\\n# calculate these metrics by hand!\\nfrom sklearn import metrics\\nimport numpy as np\\nprint(\\'MAE:\\', metrics.mean_absolute_error(true, pred))\\nprint(\\'MSE:\\', metrics.mean_squared_error(true, pred))\\nprint(\\'RMSE:\\', np.sqrt(metrics.mean_squared_error(true, pred)))\\nFollowing is the output:\\nMAE: 2.0\\nMSE: 9.5\\nRMSE: 3.08220700148\\nThe breakdown  of these  numbers is as follows:\\nMAE is probably the easiest to understand, because it\\'s just the average error. It\\ndenotes, on an average, how wrong the model is.\\nMSE is more effective than MAE, because MSE punishes larger errors, which tends\\nto be much more useful in the real world.\\nRMSE  is even more popular than MSE, because it is much more interpretable.\\nRMSE  is usually the preferred metric for regression, but no matter which one you choose,\\nthey are all loss functions and therefore are something to be minimized. Let\\'s use RMSE  to\\nascertain which columns are helping and which are hurting.\\nLet\\'s start with only using temperature. Note that our procedure will be as follows:\\nCreate our X and our y variables 1.\\nFit a linear regression model2.\\nUse the model to make a list of predictions based on X 3.\\nCalculate RMSE  between the predictions and the actual values 4.\\n\\net\\'s take a look at the code:\\nfrom sklearn import metrics\\n# import metrics from scikit-learn\\nfeature_cols = [\\'temp\\']\\n# create X and y\\nX = bikes[feature_cols]\\nlinreg = LinearRegression()\\nlinreg.fit(X, y)\\ny_pred = linreg.predict(X)\\nnp.sqrt(metrics.mean_squared_error(y, y_pred)) # RMSE\\n# Can be interpreted loosely as an average error\\n#166.45\\nNow, let\\'s try it using temperature and humidity, as shown:\\nfeature_cols = [\\'temp\\', \\'humidity\\']\\n# create X and y\\nX = bikes[feature_cols]\\nlinreg = LinearRegression()\\nlinreg.fit(X, y)\\ny_pred = linreg.predict(X)\\nnp.sqrt(metrics.mean_squared_error(y, y_pred)) # RMSE\\n# 157.79\\nIt got better! Let\\'s try using even more predictors, as illustrated:\\nfeature_cols = [\\'temp\\', \\'humidity\\', \\'season\\', \\'holiday\\', \\'workingday\\',\\n\\'windspeed\\', \\'atemp\\']\\n# create X and y\\nX = bikes[feature_cols]\\nlinreg = LinearRegression()\\nlinreg.fit(X, y)\\ny_pred = linreg.predict(X)\\nnp.sqrt(metrics.mean_squared_error(y, y_pred)) # RMSE\\n# 155.75\\nEven better! At first, this seems like a major triumph, but there is actually a hidden danger\\nhere. Note that we are training the line to fit to X and y and then asking it to predict X again!\\nThis is actually a huge mistake in machine learning because it can lead to overfitting , which\\nmeans that our model is merely memorizing  the data and regurgitating it back to us.\\n\\nmagine that you are a student, and you walk into the first day of class and the teacher says\\nthat the final exam is very difficult in this class. In order to prepare you, she gives you\\npractice test after practice test after practice test. The day of the final exam arrives and you \\nare shocked to find out that every question on the exam is exactly  the same as in the\\npractice test! Luckily, you did them so many times that you remember the answer and get a\\n100% in the exam.\\nThe same thing applies here, more or less. By fitting and predicting on the same data, the\\nmodel is memorizing the data and getting better at it. A great way to combat this\\noverfitting problem is to use the train/test approach to fit machine learning models, which\\nworks as illustrated:\\nEssentially, we will take the following steps:\\nSplit up the dataset into two parts: a training and a test set 1.\\nFit our model on the training set and then test it on the test set, just like in school,2.\\nwhere the teacher would teach from one set of notes and then test us on different\\n(but similar) questions\\nOnce our model is good enough (based on our metrics), we turn our model\\'s3.\\nattention toward the entire dataset\\nOur model awaits new data previously unseen by anyone4.\\nThe goal here is to minimize the out-of-sample errors of our model, which are the errors\\nour model has on data that it has never seen before. This is important because the main idea\\n(usually) of a supervised model is to predict outcomes for new data. If our model is unable\\nto generalize from our training data and use that to predict unseen cases, then our model\\nisn\\'t very good.\\n\\nhe preceding diagram outlines a simple way of ensuring that our model can effectively\\ningest the training data and use it to predict data points that the model itself has never seen.\\nOf course, as data scientists, we know that the test set also has answers attached to them,\\nbut the model doesn\\'t know that.\\nAll of this might sound complicated, but luckily, the scikit-learn  package has a built-in\\nmethod to do this, as shown:\\nfrom sklearn.cross_validation import train_test_split\\n# function that splits data into training and testing sets\\nfeature_cols = [\\'temp\\']\\nX = bikes[feature_cols]\\ny = bikes[\\'count\\']\\n# setting our overall data X, and y\\n# Note that in this example, we are attempting to find an association\\nbetween the temperature of the day and the number of bike rentals.\\nX_train, X_test, y_train, y_test = train_test_split(X, y) # split the data\\ninto training and testing sets\\n# X_train and y_train will be used to train the model\\n# X_test and y_test will be used to test the model\\n# Remember that all four of these variables are just subsets of the overall\\nX and y.\\nlinreg = LinearRegression()\\n# instantiate the model\\nlinreg.fit(X_train, y_train)\\n# fit the model to our training set\\ny_pred = linreg.predict(X_test)\\n# predict our testing set\\nnp.sqrt(metrics.mean_squared_error(y_test, y_pred)) # RMSE\\n# Calculate our metric: 166.91\\nWe will spend more time on the reasoning behind this train/test split in Chapter 12 , Beyond\\nthe Essentials , and look into an even more helpful method, but the main reason we must go\\nthrough this extra work is because we do not want to fall into a trap where our model is\\nsimply regurgitating our dataset back to us and will not be able to handle unseen data\\npoints.\\nIn other words, our train/test split is ensuring that the metrics we are looking at are more\\nhonest estimates of our sample performance.\\n\\now, let\\'s try again with more predictors, as follows:\\nfeature_cols = [\\'temp\\', \\'workingday\\']\\nX = bikes[feature_cols]\\ny = bikes[\\'count\\']\\nX_train, X_test, y_train, y_test = train_test_split(X, y)\\n# Pick a new random training and test set\\nlinreg = LinearRegression()\\nlinreg.fit(X_train, y_train)\\ny_pred = linreg.predict(X_test)\\n# fit and predict\\nnp.sqrt(metrics.mean_squared_error(y_test, y_pred))\\n# 166.95\\nNow our model actually got worse with that addition! This implies that workingday  might\\nnot be very predictive of our response, the bike rental count.\\nNow, all of this is good and well, but how well is our model really doing at predicting? We\\nhave our root mean squared error of around 167 bikes, but is that good ? One way to\\ndiscover this is to evaluate the null model .\\nThe null model in supervised  machine learning  represents effectively guessing the expected\\noutcome over and over, and seeing how you did. For example, in regression, if we only\\never guess the average number of hourly bike rentals, then how well would that model do?\\nFirst, let\\'s get the average hourly bike rental, as shown:\\naverage_bike_rental = bikes[\\'count\\'].mean()\\naverage_bike_rental\\n# 191.57\\nThis means that, overall, in this dataset, regardless of weather, time, day of the week,\\nhumidity, and everything else, the average number of bikes that go out every hour is about\\n192.\\nLet\\'s make a fake prediction list, wherein every single guess is 191.57. Let\\'s make this guess\\nfor every single hour, as follows:\\nnum_rows = bikes.shape[0]\\nnum_rows\\n# 10886\\nnull_model_predictions = [average_bike_rental]*num_rows\\nnull_model_predictions\\n\\nhe output is as follows:\\n[191.57413191254824,\\n 191.57413191254824,\\n 191.57413191254824,\\n 191.57413191254824,\\n...\\n 191.57413191254824,\\n 191.57413191254824,\\n 191.57413191254824,\\n 191.57413191254824]\\nSo, now we have 10886  values, all of them are the average hourly bike rental number.\\nNow, let\\'s see what RMSE  would be if our model only ever guessed the expected value of\\nthe average hourly bike rental count:\\nnp.sqrt(metrics.mean_squared_error(y, null_model_predictions))\\nThe output is as follows:\\n181.13613\\nSimply guessing, it looks like our RMSE  would be 181 bikes. So, even with one or two\\nfeatures, we can beat it! Beating the null model is a kind of baseline in machine learning. If\\nyou think about it, why go through any effort at all if your machine learning  is not even \\nbetter  than just guessing!\\nWe\\'ve spent a great deal of time on linear regression, but I\\'d like to now take some time to\\nlook at our next machine learning model, which is actually, somewhat, a cousin of linear\\nregression. They are based on very similar ideas but have one major difference —while\\nlinear regression is a regression model and can only be used to make predictions of\\ncontinuous numbers, our next machine learning model will be a classification model, which\\nmeans that it will attempt to make associations between features and a categorical response.\\nLogistic regression\\nOur first classification model is called logistic  regression. I can already hear the questions\\nyou have in your head: what makes is logistic? Why is it called regression if you claim that\\nthis is a classification algorithm? All in good time, my friend.\\n\\nogistic regression is a generalization of the linear regression model adapted to fit\\nclassification problems. In linear regression, we use a set of quantitative feature variables to\\npredict a continuous response variable. In logistic regression, we use a set of quantitative\\nfeature variables to predict the probabilities  of class membership. These probabilities can\\nthen be mapped to class labels, hence predicting a class for each observation.\\nWhen performing linear regression, we use the following function to make our line of best\\nfit:\\nHere, y is our response variable (the thing we wish to predict), our beta represents our\\nmodel parameters and x represents our input variable (a single one in this case, but it can\\ntake in more, as we have seen).\\nBriefly, let\\'s assume that one of the response options in our classification problem is class 1.\\nWhen performing logistic regression, we use the following form:\\nProbability of y=1, given x\\nHere, y represents the conditional probability that our response variable belongs to class 1,\\ngiven our data, x. Now, you might be wondering what on earth is that monstrosity of a\\nfunction on the right-hand side, and where did the e variable come from? Well, that\\nmonstrosity is called the logistic function and it is actually wonderful. And that variable, e,\\nis no variable at all. Let\\'s back up a tick.\\nThe variable e is a special number, like π. It is, approximately, 2.718, and is called Euler\\'s\\nnumber . It is used frequently in modeling environments with natural  growth and decay.\\nFor example, scientists use e to model the population growth of bacteria and buffalo alike.\\nEuler\\'s number is used to model the radioactive decay of chemicals and to calculate\\ncontinuous compound interest! Today, we will use e for a very special purpose, for machine\\nlearning.\\nWhy can\\'t we just make a linear regression directly to the probability of the data point\\nbelonging to a certain class, like this:\\n\\n\\ne can\\'t do that for a few reasons, but I will point out a big one. Linear regression, because\\nit attempts to relate to a continuous response variable, assumes that our y is continuous. In\\nour case, y would represent the probability of an event occurring. Even though our\\nprobability is, in fact, a continuous range, it is just that —a range between 0 and 1. A line\\nwould extrapolate beyond 0 and 1 and  be able  to predict a probability of -4 or 1,542! We can\\'t\\nhave that. Our graph must  be bound neatly between 0 and 1 on the y axis, just like a real\\nprobability.\\nAnother reason is a bit more philosophical. Using a linear regression, we are making a\\nserious assumption. Our big assumption here is that there is a linear relationship between\\nprobability and our features. In general, if we think about the probability of an event, we\\ntend to think of smooth curves representing them, not a single boring line. So, we need\\nsomething a bit more appropriate. For this, let\\'s go back and revisit basic probability for a\\nminute.\\nProbability, odds, and log odds\\nWe are familiar with the basic concept of probability  in that the probability of an event\\noccurring can be simply modeled as the number of ways the event can occur divided by all\\nof the possible  outcomes. For example, if, out of 3,000  people who walked into a store, 1,000\\nactually bought something, then we could say that the probability of a single person buying\\nan item is as shown here:\\nPr(buy) = 1000/3000 = 1/3 = 33.3%\\nHowever, we also have  a related concept, called odds . The odds of an outcome occurring is\\nthe ratio of the number of ways that the outcome occurs divided by every other possible\\noutcome instead of all possible outcomes. In the same example, the odds of a person buying\\nsomething would be as follows:\\nOdds(buy) = 1000/3000 = 1/3 = 33.3%\\nThis means that, for every customer you convert, you will not convert two customers. These\\nconcepts are so related, there is even a formula to get from one to the other. We have that:\\n\\n\\net\\'s check this with our example, as illustrated:\\nIt checks out!\\nLet\\'s use Python to make a table of probabilities and associated odds, as shown:\\n# create a table of probability versus odds\\ntable = pd.DataFrame({\\'probability\\':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\\ntable[\\'odds\\'] = table.probability/(1 - table.probability)\\ntable\\nFollowing is the output:\\nSo, we see that, as our probabilities increase, so do our odds, but at a much faster rate! In\\nfact, as the probability  of an event occurring nears 1, our odds will shoot off into infinity.\\nEarlier, we said that we couldn\\'t simply regress to probability  because our line would\\nshoot off into positive and negative infinities, predicting improper probabilities, but what if\\nwe regress to odds ? Well, odds  go off to positive infinity, but alas, they will merely\\napproach 0 on the bottom, but never go below 0. Therefore, we cannot simply regress to\\nprobability  or odds . It looks like we\\'ve hit rock bottom folks!\\n\\nowever, wait, natural numbers and logarithms come to the rescue! Think of logarithms as\\nfollows:\\nBasically, logarithms and exponents are one and the same. We are just so used to writing \\nexponents  in the first way that we forget there is another way to write them. How about\\nanother example? If we take the logarithm of a number, we are asking the question, what\\nexponent would we need to put on this number to make it the given number?\\nNote that np.log  automatically does  all logarithms in base e, which is what we want:\\nnp.log(10) # == 2.3025\\n# meaning that e ^ 2.302 == 10\\n# to prove that\\n2.71828**2.3025850929940459 # == 9.9999\\n# e ^ log(10) == 10\\nLet\\'s go ahead and add the logarithm of odds , or logodds , to our table , as follows:\\n# add log-odds to the table\\ntable[\\'logodds\\'] = np.log(table.odds)\\ntable\\nFollowing is the output:\\n\\n\\no, now every row has the probability  of a single event occurring, the odds  of that event\\noccurring, and now the logodds  of that event occurring. Let\\'s go ahead and ensure that our\\nnumbers are on the up and up. Let\\'s choose a probability of .25, as illustrated:\\nprob = .25\\nodds = prob / (1 - prob)\\nodds\\n# 0.33333333\\nlogodds = np.log(odds)\\nlogodds\\n# -1.09861228\\nIt checks out! Wait, look! Our logodds  variable seems to go down below zero and, in fact,\\nlogodds  is not bounded above, nor is it bounded below, which means that it is a great\\ncandidate for a response variable for linear regression. In fact, this is where our story of\\nlogistic regression really begins.\\nThe math of logistic regression\\nThe long and short of it is that logistic  regression is a linear regression between our feature,\\nx, and the log-odds of our data belonging to a certain class that we will call true for the sake\\nof generalization.\\nIf p represents the probability of a data point belonging to a particular class, then logistic\\nregression can be written as follows:\\nIf we rearrange our variables and solve this for p, we would get the logistic function, which\\ntakes on an S shape, where y is bounded by [0,1] :\\n\\n\\nThe preceding graph represents the logistic function\\'s ability to map our continuous input,\\nx, to a smooth probability curve that begins at the left, near probability 0, and as we\\nincrease x, our probability of belonging to a certain class rises naturally  and smoothly up to\\nprobability 1. Let\\'s explain that in other words:\\nLogistic regression gives an output of the probabilities of a specific class being\\ntrue\\nThose probabilities can be converted into class predictions\\nThe logistic function has some nice properties, as follows:\\nIt takes on an S shape\\nOutput is bounded by 0 and 1, as a probability should be\\n\\nn order to interpret the output value of a logistic function, we must understand the\\ndifference between probability and odds. The odds of an event are given by the ratio of the\\nprobability of the event by its complement, as shown:\\nIn linear regression, the β1 parameter represents the change in the response variable for a\\nunit change in x. In logistic  regression, β1 represents the change in the log-odds for a unit\\nchange in x. This means gives us the change in the odds for a unit change in x.\\nConsider that we are interested in mobile purchase behavior. Let y be a class label denoting\\npurchase/no purchase, and let x denote whether the phone was an iPhone. Also, suppose\\nthat we perform a logistic regression, and we get β= 0.693 . In this case, the odds ratio is\\nnp.exp(0.693) = 2 , which means that the likelihood of purchase is twice as high if the phone\\nis an iPhone.\\nOur examples have mostly been binary classification, meaning that we are\\nonly predicting one of two outcomes, but logistic regression can handle\\npredicting multiple options in our categorical response using a one-\\nversus-all approach, meaning that it will fit a probability curve for each\\ncategorical response!\\nBack to our bikes briefly to see scikit-learn\\'s logistic regression in action. I will begin by\\nmaking a new response variable that is categorical. To make things simple, I made a\\ncolumn called above_average , which is true if the hourly bike rental count is above\\naverage and false otherwise:\\n# Make a categorical response\\nbikes[\\'above_average\\'] = bikes[\\'count\\'] >= average_bike_rental\\nAs mentioned before, we should look at our null model. In regression, our null model\\nalways predicts the average response, but in classification, our null model always predicts\\nthe most common outcome. In this case, we can use a pandas value count to see that. About\\n60% of the time, the bike rental count is not above average:\\nbikes[\\'above_average\\'].value_counts(normalize=True)\\n\\now, let\\'s actually use logistic regression to try and predict whether or not the hourly bike\\nrental count will be above average, as shown:\\nfrom sklearn.linear_model import LogisticRegression\\nfeature_cols = [\\'temp\\']\\n# using only temperature\\nX = bikes[feature_cols]\\ny = bikes[\\'above_average\\']\\n# make our overall X and y variables, this time our y is\\n# out binary response variable, above_average\\nX_train, X_test, y_train, y_test = train_test_split(X, y)\\n# make our train test split\\nlogreg = LogisticRegression()\\n# instantiate our model\\nlogreg.fit(X_train, y_train)\\n# fit our model to our training set\\nlogreg.score(X_test, y_test)\\n# score it on our test set to get a better sense of out of sample\\nperformance\\n# 0.65650257\\nIt seems that, by only using temperature, we can beat the null model of guessing false all of\\nthe time! This is our first step in making our model the best it can be.\\nBetween linear and logistic  regression, I\\'d say we already have a great tool belt of machine\\nlearning forming, but I have a question —it seems that both of these algorithms are only\\nable to take in quantitative columns as features, but what if I have a categorical feature that\\nI think has an association to my response?\\nDummy variables\\nDummy variables are used when we are hoping to convert a categorical feature into a\\nquantitative one. Remember that we have two types of categorical features: nominal and\\nordinal. Ordinal features have natural order among them, while nominal data does not.\\n\\nncoding qualitative (nominal) data using separate columns is called making dummy\\nvariables and it works by turning each unique category of a nominal column into its own\\ncolumn that is either true or false.\\nFor example, if we had a column  for someone\\'s college major and we wished to plug that\\ninformation into a linear or logistic regression, we couldn\\'t because they only take in\\nnumbers! So, for each row, we had new columns that represent the single nominal column.\\nIn this case, we have four unique majors: computer science, engineering, business, and\\nliterature. We end up with three new columns (we omit computer science as it is not\\nnecessary):\\nNote that the first row has a 0 in all of the columns, which means that this person did not\\nmajor in engineering, did not major in business, and did not major in literature. The second\\nperson has a single 1 in the Engineering  column as that is the major they studied.\\nIn our bikes example, let\\'s define a new column, called when_is_it , which is going to be\\none of the following four options:\\nMorning\\nAfternoon\\nRush_hour\\nOff_hours\\nTo do this, our approach will be to make a new column that is simply the hour of the day,\\nuse that column to determine when in the day it is, and explore whether or not we think\\nthat column might help us predict the above_daily  column:\\nbikes[\\'hour\\'] = bikes[\\'datetime\\'].apply(lambda x:int(x[11]+x[12]))\\n# make a column that is just the hour of the day\\nbikes[\\'hour\\'].head()\\n0 1 2 3\\n\\nreat, now let\\'s define a function that turns these hours into strings. For this example, let\\'s\\ndefine the hours between 5 and 11 as morning, between 11 a.m. and 4 p.m. as being\\nafternoon, 4 and 6 as being rush hour, and everything else as being off hours:\\n# this function takes in an integer hour\\n# and outputs one of our four options\\ndef when_is_it(hour):\\n    if hour >= 5 and hour < 11:\\n        return \"morning\"\\n    elif hour >= 11 and hour < 16:\\n        return \"afternoon\"\\n    elif hour >= 16 and hour < 18:\\n        return \"rush_hour\"\\n    else:\\n        return \"off_hours\"\\nLet\\'s apply this function  to our new hour  column and make our brand new column,\\nwhen_is_it :\\nbikes[\\'when_is_it\\'] = bikes[\\'hour\\'].apply(when_is_it)\\nbikes[[\\'when_is_it\\', \\'above_average\\']].head()\\nFollowing is the table:\\nLet\\'s try to use only this new column to determine whether or not the hourly bike rental\\ncount will be above average. Before we do, let\\'s do the basics of exploratory data analysis\\nand make a graph to see if we can visualize a difference between the four times of the day.\\nOur graph will be a bar chart with one bar per time of the day. Each bar will represent the\\npercentage of times that this time of the day had a greater than normal bike rental:\\nbikes.groupby(\\'when_is_it\\').above_average.mean().plot(kind=\\'bar\\')\\n\\nollowing is the output:\\nWe can see that there is a pretty big difference! For example, when it is off hours, the chance\\nof having more than average bike rentals is about 25%, whereas during rush hour, the\\nchance of being above average is over 80%! Okay, this is exciting, but let\\'s use some built-in\\npandas tools to extract dummy columns, as follows:\\nwhen_dummies = pd.get_dummies(bikes[\\'when_is_it\\'], prefix=\\'when__\\')\\nwhen_dummies.head()\\n\\nollowing is the output:\\nwhen_dummies = when_dummies.iloc[:, 1:]\\n# remove the first column\\nwhen_dummies.head()\\nFollowing is the output:\\nGreat! Now we have a DataFrame full of numbers that we can plug in to our logistic\\nregression:\\nX = when_dummies\\n# our new X is our dummy variables\\ny = bikes.above_average\\nlogreg = LogisticRegression()\\n# instantiate our model\\nlogreg.fit(X_train, y_train)\\n# fit our model to our training set\\nlogreg.score(X_test, y_test)\\n# score it on our test set to get a better sense of out of sample\\n\\nerformance\\n# 0.685157\\nThis is even better than just using the temperature! What if we tacked temperature and\\nhumidity onto that? So, now we are using the temperature, humidity, and our time of day\\ndummy variables to predict whether or not we will have higher than average bike rentals:\\nnew_bike = pd.concat([bikes[[\\'temp\\', \\'humidity\\']], when_dummies], axis=1) #\\ncombine temperature, humidity, and the dummy variables\\nX = new_bike # our new X is our dummy variables\\ny = bikes.above_average\\nX_train, X_test, y_train, y_test = train_test_split(X, y)\\nlogreg = LogisticRegression() # instantiate our model\\nlogreg.fit(X_train, y_train) # fit our model to our training set\\nlogreg.score(X_test, y_test) # score it on our test set to get a better\\nsense of out of sample performance\\n# 0.75165\\nWow. Okay, let\\'s quit while we\\'re ahead.\\nSummary\\nIn this chapter, we looked at machine learning and its different subcategories. We explored\\nsupervised, unsupervised, and reinforcement learning strategies and looked at situations\\nwhere each one would come in handy.\\nLooking into linear regression, we were able to find relationships between predictors and a\\ncontinuous response variable. Through the train/test split, we were able to help avoid\\noverfitting our machine learning models and get a more generalized prediction. We were\\nable to use metrics, such as the root mean squared error, to evaluate our models as well.\\n\\ny extending our notion of linear regression into logistic regression, we were able to then\\nfind association between the same predictors, but now to categorical responses. By\\nintroducing dummy variables into the mix, we were able to add categorical features to our\\nmodels and improve our performance even further.\\nIn the next few chapters, we will be taking a much deeper dive into many more machine\\nlearning models and, along the way, we will learn new metrics, new validation techniques,\\nand more importantly, new ways of applying our data science to the world.\\n\\n1\\nPredictions Don\\'t Grow on\\nTrees – or Do They?\\nOur goal in this chapter is to see and apply concepts learned from previous chapters in\\norder to construct and use modern learning algorithms in order to glean insights and make\\npredictions on real datasets. While we explore the following algorithms, we should always\\nremember that we are constantly keeping our metrics in mind.\\nIn this chapter , we will be looking at the following machine learning algorithms:\\nDecision trees\\nNaive Bayes classification\\nk-means clustering  \\nThe first two are examples of supervised learning, while the final algorithm is an example\\nof unsupervised learning.\\nLet\\'s get to it!\\nNaive  Bayes classification\\nLet\\'s get right into it! Let\\'s begin with Naive  Bayes classification. This machine learning\\nmodel relies heavily on results from previous chapters, specifically with Bayes\\' theorem:\\n\\n\\net\\'s look a little closer at the specific features of this formula:\\nP(H)  is the probability of the hypothesis before we observe  the data, called the\\nprior probability , or just prior\\nP(H|D)  is what we want to compute, the probability of the hypothesis after we\\nobserve the data, called the posterior\\nP(D|H)  is the probability of the data under the given  hypothesis, called the\\nlikelihood\\nP(D)  is the probability of the data under  any hypothesis, called the normalizing\\nconstant\\nNaive Bayes classification is a classification model, and therefore a supervised model.\\nGiven this, what kind of data do we need?\\nLabeled data\\nUnlabeled data\\n(Insert Jeopardy music here )\\nIf you answered labeled data , then you\\'re well on your way to becoming a data scientist!\\nSuppose we have a dataset with n features, ( x1, x2, …, xn) and a class label, C. For\\nexample, let\\'s take some data involving spam text classification. Our data would consist of\\nrows of individual text samples and columns of both our features and our class labels. Our\\nfeatures would be words and phrases that are contained within the text samples and our\\nclass labels are simply spam  or not spam . In this scenario, I will replace the not spam\\nclass with an easier-to-say word, ham:\\nimport pandas as pd\\nimport sklearn\\ndf =\\npd.read_table(\\'https://raw.githubusercontent.com/sinanuozdemir/sfdat22/mast\\ner/data/sms.tsv\\',\\n                   sep=\\'\\\\t\\', header=None, names=[\\'label\\', \\'msg\\'])\\ndf\\n\\nere is a sample of text data in a row column format:\\nLet\\'s do some preliminary statistics to see what we are dealing with. Let\\'s see the difference\\nin the number of ham and spam messages at our disposal:\\ndf.label.value_counts().plot(kind=\"bar\")\\nThis gives us a bar chart, as follows:\\n\\n\\no, we have way more ham messages than we do spam. Because this is a classification\\nproblem, it will be very useful to know our null accuracy rate , which is the percentage\\nchance of predicting a single row correctly if we keep guessing the most common class,\\nham:\\ndf.label.value_counts() / df.shape[0]\\nham     0.865937\\nspam    0.134063\\nSo if we blindly guessed ham, we would be correct about 87% of the time, but we can do\\nbetter than that. If we have a set of classes, C, and features, xi, then we can use Bayes\\'\\ntheorem to predict the probability that a single row belongs to class C, using the following\\nformula:\\nLet\\'s look at this formula in a little more detail:\\nP(class C | {xi}) : The posterior probability is the probability that the row belongs\\nto class C  given the features {xi}.\\nP({xi} | class C) : This is the likelihood that we would observe these features given\\nthat the row was in class C .\\nP(class C) : This is the prior probability. It is the probability that the data point\\nbelongs to class C  before we see any data.\\nP({xi}) : This is our normalization constant.\\nFor example, imagine we have an email with three words: send cash now.  We\\'ll use\\nNaive Bayes to classify the email as either being spam or ham:\\nWe are concerned with the difference of these two numbers. We can use the following\\ncriteria to classify any single text sample:\\nIf P(spam | send cash now)  is larger than P(ham | send cash now) ,\\nthen we will classify the text as spam\\n\\nf P(ham | send cash now)  is larger than P(spam | send cash now) , then\\nwe will label the text ham\\nBecause both equations have P (send money now)  in the denominator, we can ignore them.\\nSo, now we are concerned with the following:\\nLet\\'s figure out the numbers  in this equation:\\nP(spam) = 0.134063\\nP(ham) = 0.865937\\nP(send cash now | spam)\\nP(send cash now | ham)\\nThe final two likelihoods might seem like they would not be so difficult to calculate. All we\\nhave to do is count the numbers of spam messages that include the send money\\nnow phrase and divide that by the total number of spam messages:\\ndf.msg = df.msg.apply(lambda x:x.lower())\\n# make all strings lower case so we can search easier\\ndf[df.msg.str.contains(\\'send cash now\\')] .shape\\n(0, 2)\\nOh no! There are none! There are literally zero texts with the exact phrase send cash now .\\nThe hidden problem here is that this phrase is very specific and we can\\'t assume that we\\nwill have enough data in the world to have seen this exact phrase many times before.\\nInstead, we can make a naïve assumption  in our Bayes\\' theorem. If we assume that the\\nfeatures (words) are conditionally independent (meaning that no word affects the existence\\nof another word), then we can rewrite the formula:\\nspams = df[df.label == \\'spam\\']\\nfor word in [\\'send\\', \\'cash\\', \\'now\\']:\\n    print( word, spams[spams.msg.str.contains(word)].shape[0] /\\nfloat(spams.shape[0]))\\n\\n(send|spam) = 0.096\\nP(cash |spam) = 0.091\\nP(now|spam) = 0.280\\nWith this, we can calculate the following:\\nP(send cash now| spam) ∗ P(spam)= (.096 ∗.091∗.280) ∗.134 = 0.00032\\nRepeating the same procedure for ham gives us the following:\\nP(send|ham) = 0.03\\nP(cash|ham) = 0.003\\nP(now|ham) = 0.109\\nThe fact that these numbers are both very low is not as important as the fact that the spam\\nprobability is much larger than the ham calculation. If we calculate 0.00032 / 0.0000084 =\\n38.1 we see that the send cash now  probability for spam is 38 times higher than for spam.\\nDoing this means that we can classify send cash now  as spam! Simple, right?\\nLet\\'s use Python to implement a Naive Bayes classifier without having to do all of these\\ncalculations ourselves.\\nFirst, let\\'s revisit the count vectorizer in scikit-learn, which turns text into numerical data\\nfor us. Let\\'s assume that we will train on three documents (sentences):\\n# simple count vectorizer example\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n# start with a simple example\\ntrain_simple = [\\'call you tonight\\',\\n                \\'Call me a cab\\',\\n                \\'please call me... PLEASE 44!\\']\\n# learn the \\'vocabulary\\' of the training data\\nvect = CountVectorizer()\\ntrain_simple_dtm = vect.fit_transform(train_simple)\\npd.DataFrame(train_simple_dtm.toarray(), columns=vect.get_feature_names())\\n\\n\\note that each row represents one of the three documents (sentences), each column\\nrepresents one of the words present in the documents, and each cell contains the number of\\ntimes each word appears in each document.\\nWe can then use the count vectorizer to transform new incoming test documents to\\nconform with our training set (the three sentences):\\n# transform testing data into a document-term matrix (using existing\\nvocabulary, notice don\\'t is missing)\\ntest_simple = [\"please don\\'t call me\"]\\ntest_simple_dtm = vect.transform(test_simple)\\ntest_simple_dtm.toarray()\\npd.DataFrame(test_simple_dtm.toarray(), columns=vect.get_feature_names())\\nNote how, in our test sentence, we had a new word, namely don\\'t . When we vectorized it,\\nbecause we hadn\\'t seen that word previously in our training data, the vectorizer simply\\nignored it. This is important and incentivizes data scientists to obtain as much data as\\npossible for their training sets.\\nNow, let\\'s do this for our actual data:\\n# split into training and testing sets\\nfrom sklearn.cross_validation import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(df.msg, df.label,\\nrandom_state=1)\\n# instantiate the vectorizer\\nvect = CountVectorizer()\\n# learn vocabulary and create document-term matrix in a single step\\ntrain_dtm = vect.fit_transform(X_train)\\ntrain_dtm\\nThe following is the output: \\n<4179x7456 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n with 55209 stored elements in Compressed Sparse Row format>\\n\\note that the format is in a sparse matrix, meaning the matrix is so large and full of zeroes.\\nThere is a special format to deal with objects such as this. Take a look  at the number of\\ncolumns.\\nThere are 7,456 words!\\nThis means that in our training set, there are 7,456 unique words to look at. We can now\\ntransform our test data to conform to our vocabulary:\\n# transform testing data into a document-term matrix\\ntest_dtm = vect.transform(X_test)\\ntest_dtm\\nThe output is as follows:\\n<1393x7456 sparse matrix of type \\'<class \\'numpy.int64\\'>\\'\\n        with 17604 stored elements in Compressed Sparse Row format>\\nNote that we have the same exact number of columns because it is conforming to our test\\nset to be exactly the same vocabulary as before. No more, no less.\\nNow let\\'s build a Naive Bayes model (similar to the linear regression process):\\n## MODEL BUILDING WITH NAIVE BAYES\\n# train a Naive Bayes model using train_dtm\\nfrom sklearn.naive_bayes import MultinomialNB\\n# import our model\\nnb = MultinomialNB()\\n# instantiate our model\\nnb.fit(train_dtm, y_train)\\n# fit it to our training set\\nNow the nb variable holds our fitted model. The training phase of the model involves\\ncomputing the likelihood function, which is the conditional probability of each feature\\ngiven each class:\\n# make predictions on test data using test_dtm\\npreds = nb.predict(test_dtm)\\npreds\\nThe output is as follows:\\narray([\\'ham\\', \\'ham\\', \\'ham\\', ..., \\'ham\\', \\'spam\\', \\'ham\\'],\\n      dtype=\\'|S4\\')\\n\\nhe prediction phase of the model involves computing the posterior probability of each\\nclass given the observed features, and choosing the class with the highest probability.\\nWe will use sklearn\\'s built-in accuracy and confusion matrix to look at how well our Naive\\nBayes models are performing:\\n# compare predictions to true labels\\nfrom sklearn import metrics\\nprint metrics.accuracy_score(y_test, preds)\\nprint metrics.confusion_matrix(y_test, preds)\\nThe output is as follows:\\naccuracy == 0.988513998564\\nconfusion matrix ==\\n[[1203    5]\\n [  11  174]]\\nFirst off, our accuracy is great! Compared to our null accuracy, which was 87%, 99% is a\\nfantastic improvement.\\nNow to our confusion matrix. From before, we know that each row represents actual values\\nwhile columns represent predicted values, so the top left value, 1,203, represents our true\\nnegatives. But what is negative and positive? We gave the model the spam  and ham strings\\nas our classes, not positive and negative.\\nWe can use the following:\\nnb.classes_\\nThe output is as follows:\\narray([\\'ham\\', \\'spam\\'])\\nWe can then line up the indices so that 1,203 refers to true ham predictions and 174 refers to\\ntrue spam  predictions.\\nThere were also five false spam classifications , meaning that five messages were predicted as\\nspam , but were actually ham, as well as 11 false ham classifications .\\nIn summary, Naive Bayes classification uses  Bayes\\' theorem in order to fit posterior\\nprobabilities of classes so that data points are correctly labeled as belonging to the proper\\nclass.\\n\\necision trees\\nDecision trees are supervised models  that can either perform regression or classification.\\nLet\\'s take a look at some major league baseball player data from 1986-1987. Each dot\\nrepresents a single player in the league:\\nYears ( x-axis) : Number of years played in the major leagues\\nHits ( y-axis) : Number of hits the player had in the previous year\\nSalary (color) : Low salary is blue/green, high salary is red/yellow\\nThe preceding data is our training data. The idea is to build a model that predicts the salary\\nof future players based on Years  and Hits . A decision tree aims to make splits  on our data in\\norder to segment the data points that act similarly to each other, but differently to the\\nothers. The tree makes multiples of these splits in order to make the most accurate\\nprediction possible. Let\\'s see a tree built for the preceding data:\\n\\nLet\\'s read this from top to bottom:\\nThe first split is Years < 4.5 : when a splitting rule is true , you follow the left\\nbranch. When a splitting rule is false , you follow the right branch. So for a new\\nplayer, if they have been playing for less than 4.5 years, we will go down the left\\nbranch.\\nFor players in the left branch, the mean salary is $166,000, hence you label it with\\nthat value (salary has been divided by 1,000 and log-transformed to 5.11 for ease\\nof computation).\\nFor players in the right branch, there is a further split on Hits < 117.5 , dividing\\nplayers into two more salary regions: $403,000 (transformed to 6.00) and $846,000\\n(transformed to 6.74).\\nThis tree doesn\\'t just give us predictions; it also provides some more information about our\\ndata:\\nIt seems that the number of years in the league is the most important factor in\\ndetermining salary, with a smaller number of years correlating to a lower salary.\\nIf a player has not been playing for long (< 4.5 years), the number of hits they\\nhave is not an important factor when it comes to their salary.\\n\\nor players with 5+ years under their  belt, hits are an important factor for their\\nsalary determination.\\nOur tree only made up to two decisions before spitting out an answer (two is\\ncalled our depth of the tree).\\nHow does a computer build a regression tree?\\nModern decision  tree algorithms tend to use a recursive  binary splitting approach:\\nThe process begins at the top of the tree.1.\\nFor every feature, it will examine every possible split and choose the feature and2.\\nsplit such that the resulting tree has the lowest possible Mean Squared Error\\n(MSE ). The algorithm  makes that split.\\nIt will then examine the two resulting regions and again make a single split (in3.\\none of the regions) to minimize the MSE.\\nIt will keep repeating Step 3  until a stopping criterion is met: 4.\\nMaximum tree depth (maximum number of splits required to arrive at\\na leaf)\\nMinimum number of observations in a leaf (final) node\\nFor classification trees, the algorithm is very similar with the biggest difference being the\\nmetric we optimize over. Because MSE only exists for regression problems, we cannot use\\nit. However, instead of accuracy, classification trees optimize over either the Gini index  or\\nentropy .\\nHow does a computer fit a classification tree?\\nSimilarly to a regression tree, a classification  tree is built by optimizing over  a metric (in\\nthis case, the Gini index) and choosing the best split to make this optimization. More\\nformally, at each node, the tree will take the following steps:\\nCalculate the purity of the data1.\\nSelect a candidate split2.\\nCalculate the purity of the data after the split3.\\nRepeat for all variables4.\\nChoose the variable with the greatest increase in purity5.\\nRepeat for each split until some stop criteria is met6.\\n\\net\\'s say that we are predicting the likelihood of death aboard a luxury cruise ship given\\ndemographic features. Suppose we start with 25 people, 10 of whom survived, and 15 of\\nwhom died:\\nBefore split All\\nSurvived 10\\nDied 15\\nWe first calculate the Gini index before doing anything:\\nIn this example, overall classes are survived  and died, illustrated in the following formula:\\nThis means that the purity of the dataset is 0.48.\\nNow let\\'s consider a potential split on gender. We first calculate the Gini index for each\\ngender:\\n\\n\\nhe following formula calculates Gini index for male and female as follows:\\nOnce we have the Gini index for each gender, we then calculate the overall Gini index for\\nthe split on gender, as follows:\\nSo, the gini coefficient for splitting  on gender is 0.27. We then follow this procedure  for\\nthree potential splits:\\nGender (male or female)\\nNumber of siblings on board (0 or 1+)\\nClass (first and second versus third)\\nIn this example, we would choose the gender to split on as it is the lowest Gini index!\\n\\nhe following table briefly summarizes the differences between classification and\\nregression decision trees:\\nRegression trees Classification trees\\nPredict a quantitative response Predict a qualitative response\\nPrediction is the average value in each\\nleafPrediction is the most common label in each leaf\\nSplits are chosen to minimize MSESplits are chosen to minimize Gini index\\n(usually)\\nLet\\'s use scikit-learn\\'s built-in decision tree function in order to build a decision tree:\\n# read in the data\\ntitanic = pd.read_csv(\\'short_titanic.csv\\')\\n# encode female as 0 and male as 1\\ntitanic[\\'Sex\\'] = titanic.Sex.map({\\'female\\':0, \\'male\\':1})\\n# fill in the missing values for age with the median age\\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\\n# create a DataFrame of dummy variables for Embarked\\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix=\\'Embarked\\')\\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\\n# concatenate the original DataFrame and the dummy DataFrame\\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\\n# define X and y\\nfeature_cols = [\\'Pclass\\', \\'Sex\\', \\'Age\\', \\'Embarked_Q\\', \\'Embarked_S\\']\\nX = titanic[feature_cols]\\ny = titanic.Survived\\nX.head()\\n\\n\\note that we are going to use class, sex, age, and dummy variables for city embarked as our\\nfeatures:\\n# fit a classification tree with max_depth=3 on all data from sklearn.tree\\nimport DecisionTreeClassifier treeclf = DecisionTreeClassifier(max_depth=3,\\nrandom_state=1) treeclf.fit(X, y)\\nmax_depth  is a limit to the depth of our tree. It means that, for any data point, our tree is\\nonly able to ask up to three questions and make up to three  splits. We can output our tree\\ninto a visual  format and we will obtain the following:\\nWe can notice a few things:\\nSex is the first split, meaning that sex is the most important determining factor of\\nwhether or not a person survived the crash\\nEmbarked_Q  was never used in any split\\nFor either classification or regression trees, we can also do something very interesting with\\ndecision trees, which is that we can output a number that represents each feature\\'s\\nimportance in the prediction of our data points:\\n# compute the feature importances\\npd.DataFrame({\\'feature\\':feature_cols,\\n\\'importance\\':treeclf.feature_importances_})\\n\\nThe importance scores are an average  Gini index difference for each variable, with higher \\nvalues  corresponding to higher importance  to the prediction. We can use this information to\\nselect fewer features in the future. For example, both of the embarked variables are very\\nlow in comparison to the rest of the features, so we may be able to say that they are not\\nimportant in our prediction of life or death.\\nUnsupervised learning\\nIt\\'s time to see some examples of unsupervised  learning, given that we spend a majority of\\nthis book on supervised learning models .\\nWhen to use unsupervised learning\\nThere are many times when unsupervised learning  can be appropriate. Some very common\\nexamples include the following:\\nThere is no clear response  variable. There is nothing that we are explicitly trying\\nto predict or correlate to other variables.\\nTo extract structure from data where no apparent structure/patterns exist (can be\\na supervised learning problem).\\nWhen an unsupervised concept called feature extraction is used. Feature\\nextraction is the process of creating new features from existing ones. These new\\nfeatures can be even stronger than the original features.\\n\\nhe first tends to be the most common reason that data scientists choose to use\\nunsupervised learning. This case arises frequently when we are working with data and we\\nare not explicitly trying to predict any of the columns and we merely wish to find patterns\\nof similar (and dissimilar) groups of points. The second option comes into play even if we\\nare explicitly attempting to use a supervised model to predict a response variable.\\nSometimes, simple EDA might not produce any clear patterns in the data in the few\\ndimensions that humans can imagine, whereas a machine might pick up on data points\\nbehaving similarly to each other in greater dimensions.\\nThe third common reason to use unsupervised learning is to extract new features from\\nfeatures that already exist. This process (lovingly called feature extraction ) might produce\\nfeatures that can be used in a future supervised model or that can be used for presentation\\npurposes (marketing or otherwise).\\nk-means clustering\\nk-means clustering is our first example  of an unsupervised machine learning model.\\nRemember this means that we are not making predictions; we are trying instead to extract\\nstructure from seemingly unstructured data.\\nClustering is a family of unsupervised machine learning models that attempt to group data\\npoints into clusters  with centroids .\\nDefinition\\nCluster : This is a group of data points that behave similarly.\\nCentroid : This is the center  of a cluster. It can be thought of as an average\\npoint in the cluster.\\nThe preceding definition can be quite vague, but it becomes specific when narrowed down\\nto specific domains. For example, online shoppers who behave similarly might shop for\\nsimilar things or at similar shops, whereas similar software companies might make\\ncomparable software at comparable prices.\\n\\nere is a visualization of clusters of points:\\nIn the preceding diagram, our human brains can very easily see the difference between the\\nfour clusters. We can see that the red cluster is at the bottom-left of the graph while the\\ngreen cluster lives in the bottom-right portion of the graph. This means that the red data\\npoints are similar to each other, but not similar to data points in the other clusters.\\nWe can also see the centroids of each cluster as the square in each color. Note that the\\ncentroid is not an actual data point, but is merely an abstraction of a cluster that represents\\nthe center of the cluster.\\nThe concept of similarity  is central to the definition of a cluster, and therefore to cluster\\nanalysis. \\u2028In general, greater similarity between points leads to better clustering. In most\\ncases, we turn data into points in n-dimensional space and use the distance between these\\npoints as a form of similarity. The centroid of the cluster then is usually the average of each\\ndimension (column) for each data point in each cluster. So, for example, the centroid of the\\nred cluster is the result of taking the average value of each column of each red data point.\\n\\nhe purpose of cluster analysis is to enhance our understanding of a dataset by dividing\\nthe data into groups. Clustering provides a layer of abstraction from individual data points.\\nThe goal is to extract and enhance the natural structure of the data. There are many kinds of\\nclassification procedures. For our class, we will be focusing on k-means clustering, which is\\none of the most popular clustering algorithms.\\nk-means is an iterative method that partitions  a dataset into k clusters. It works in four\\nsteps:\\nChoose k initial centroids (note that k is an input) 1.\\nFor each point, assign the point to the nearest centroid2.\\nRecalculate the centroid positions3.\\nRepeat Step 2  and Step 3  until stopping criteria is met 4.\\nIllustrative example – data points\\nImagine that we have the following  data points in a two-dimensional space:\\nEach dot is colored gray to assume no prior grouping before applying the k-means\\nalgorithm. The goal here is to eventually color in each dot and create groupings (clusters),\\nas illustrated in the following plot:\\n\\nHere, Step 1  has been applied. We have (randomly) chosen three centroids (red, blue, and\\nyellow).\\nMost k-means algorithms place random initial centroids, but there exist\\nother pre-compute methods to place initial centroids. For now, random is\\nfine.\\n\\n\\nhe first part of Step 2  has been applied. For each data point, we found the most similar\\ncentroid (closest):\\nThe second part of Step 2  has been applied here. We have colored in each data point in\\naccordance with its most similar centroid:\\nThis is Step 3  and the crux of k-means. Note that we have physically moved the centroids to\\nbe the actual center of each cluster. We have, for each color, computed the average  point\\nand made that point the new centroid. For example, suppose the three red data points had\\nthe coordinates: ( 1, 3 ), (2, 5 ), and (3, 4 ). The center ( red cross ) would be calculated as\\nfollows:\\n# centroid calculation\\nimport numpy as np\\nred_point1 = np.array([1, 3])\\nred_point2 = np.array([2, 5])\\nred_point3 = np.array([3, 4])\\n\\ned_center = (red_point1 + red_point2 + red_point3) / 3.\\nred_center\\n# array([ 2.,  4.])\\nThat is, the ( 2, 4 ) point would be the coordinates of the preceding red cross.\\nNone of the actual data points will ever move. They cannot. The only\\nentities that move are the centroids, which are not actual data points.\\nWe continue with our algorithm by repeating Step 2 . Here is the first part where we find the\\nclosest center for each point. Note a big change: the point that is circled in the following\\nfigure used to be a yellow point, but has changed to be a red cluster point because the\\nyellow cluster moved closer to its yellow constituents:\\n\\n\\nt might help to think of points as being planets in space with\\ngravitational pull. Each centroid is pulled by the planets\\' gravity.\\nHere is the second part of Step 2  again. We have assigned each point to the color of the\\nclosest cluster:\\nHere, we recalculate once more the centroids for each cluster ( Step 3 ). Note that the blue\\ncenter did not move at all, while the yellow and red centers both moved.\\nBecause we have reached a stopping  criterion (clusters do not move if we repeat Step 2  and\\nStep 3 ), we finalize our algorithm and we have our three clusters, which is the final result of\\nthe k-means algorithm:\\n\\nIllustrative example – beer!\\nEnough data  science, beer!\\nOK, OK, settle down. It\\'s a long book; let\\'s grab a beer. On that note, did you know there\\nare many types of beer? I wonder if we could possibly group beers into different categories\\nbased on different quantitative features … Let\\'s try! See the following:\\n# import the beer dataset\\nurl = \\'../data/beer.txt\\'\\nbeer = pd.read_csv(url, sep=\\' \\')\\nprint beer.shape\\nThe output is as follows:\\n(20, 5)\\nbeer.head()\\n\\n\\nere we have 20 beers with five columns: name , calories , sodium , alcohol , and cost. In\\nclustering (like almost all machine learning models), we like quantitative features, so we\\nwill ignore the name of the beer in our clustering:\\n# define X\\nX = beer.drop(\\'name\\', axis=1)\\nNow we will perform k-means using scikit-learn:\\n# K-means with 3 clusters\\nfrom sklearn.cluster import KMeans\\nkm = KMeans(n_clusters=3, random_state=1)\\nkm.fit(X)\\nn_clusters  is our k. It is our input number of clusters. random_state  as\\nalways produces reproducible results for educational purposes. Using\\nthree clusters for now is random.\\nOur k-means algorithm has run the algorithm on our data points and come up with three\\nclusters:\\n# save the cluster labels and sort by cluster\\nbeer[\\'cluster\\'] = km.labels_\\nWe can take a look at the center of each cluster by using a groupby  and mean  statement:\\n# calculate the mean of each feature for each cluster\\nbeer.groupby(\\'cluster\\').mean()\\nOn human inspection, we can see that cluster 0 has, on average, a higher calorie, sodium,\\nand alcohol content and costs more. These might be considered heavier beers. Cluster 2 has\\non average a very low alcohol content and very few calories. These are probably light beers.\\nCluster 1 is somewhere in the middle.\\n\\net\\'s use Python to make  a graph to see this in more detail:\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n# save the DataFrame of cluster centers\\ncenters = beer.groupby(\\'cluster\\').mean()\\n# create a \"colors\" array for plotting\\ncolors = np.array([\\'red\\', \\'green\\', \\'blue\\', \\'yellow\\'])\\n# scatter plot of calories versus alcohol, colored by cluster (0=red,\\n1=green, 2=blue)\\nplt.scatter(beer.calories, beer.alcohol, c=colors[list(beer.cluster)],\\ns=50)\\n# cluster centers, marked by \"+\"\\nplt.scatter(centers.calories, centers.alcohol, linewidths=3, marker=\\'+\\',\\ns=300, c=\\'black\\')\\n# add labels\\nplt.xlabel(\\'calories\\')\\nplt.ylabel(\\'alcohol\\')\\nA big part of unsupervised learning is human inspection. Clustering has\\nno context of the problem domain and can only tell us the clusters it found\\nit cannot tell us what the clusters mean.\\n\\n\\nhoosing an optimal number for K and\\ncluster validation\\nA big part of k-means clustering is knowing the optimal number of clusters. If we knew this\\nnumber ahead of time, then that might defeat the purpose of even using  unsupervised\\nlearning. So we need a way to evaluate the output of our cluster analysis.\\nThe problem here is that, because we are not performing any kind of prediction, we cannot\\ngauge how right  the algorithm is at predictions. Metrics such as accuracy and RMSE go\\nright out of the window.\\nThe Silhouette Coefficient\\nThe Silhouette Coefficient  is a common metric  for evaluating clustering performance in\\nsituations when the true cluster assignments are not known.\\nA Silhouette Coefficient is calculated for each observation as follows:\\n \\nLet\\'s look a little closer at the specific features of this formula:\\na: Mean distance to all other points in its cluster\\nb: Mean distance to all other points in the next nearest cluster\\nIt ranges from -1 (worst) to 1 (best). A global score  is calculated by taking the mean score\\nfor all observations. In general, a Silhouette Coefficient of 1 is preferred, while a score of -1\\nis not preferable:\\n# calculate Silhouette Coefficient for K=3\\nfrom sklearn import metrics\\nmetrics.silhouette_score(X, km.labels_)\\nThe output is as follows:\\n0.67317750464557957\\n\\net\\'s try calculating the coefficient for multiple values of K to find the best value:\\n# center and scale the data\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\n# calculate SC for K=2 through K=19\\nk_range = range(2, 20)\\nscores = []\\nfor k in k_range:\\n    km = KMeans(n_clusters=k, random_state=1)\\n    km.fit(X_scaled)\\n    scores.append(metrics.silhouette_score(X, km.labels_))\\n# plot the results\\nplt.plot(k_range, scores)\\nplt.xlabel(\\'Number of clusters\\')\\nplt.ylabel(\\'Silhouette Coefficient\\')\\nplt.grid(True)\\nSo, it looks like our optimal number of beer clusters is 4! This means that our k-means\\nalgorithm has determined that there seem to be four distinct types of beer.\\n\\n-means is a popular algorithm because of its computational  efficiency and simple and\\nintuitive nature. k-means, however, highly scale dependent and is not suitable for data with\\nwidely varying shapes and densities. There are ways to combat this issue by scaling data\\nusing scikit-learn\\'s standard scalar:\\n# center and scale the data\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\n# K-means with 3 clusters on scaled data\\nkm = KMeans(n_clusters=3, random_state=1)\\nkm.fit(X_scaled)\\nEasy!\\nNow let\\'s take a look at the third reason to use unsupervised methods that falls under the\\nthird option in our reasons to use unsupervised methods: feature extraction.\\nFeature extraction and principal component\\nanalysis\\nSometimes we have an overwhelming  number of columns and likely  not enough rows to\\nhandle the great quantity of columns.\\nA great example of this is when we were looking at the send cash now  example in our\\nNave Bayes example. We had literally 0 instances of texts with that exact phrase, so instead,\\nwe turned to a naïve assumption that allowed us to extrapolate a probability for both of our\\ncategories.\\nThe reason we had this problem in the first place is because of something called the curse of\\ndimensionality. The curse of dimensionality basically says that as we introduce and\\nconsider new feature columns, we need almost exponentially more rows (data points) in\\norder to fill in the empty spaces that we create.\\n\\nonsider an example where we attempt to use a learning model that utilizes the distance\\nbetween points on a corpus of text that has 4,086 pieces of text and that the whole thing has\\nbeen Countvectorized . Let\\'s assume that these texts between them have 18,884 words:\\nX.shape\\nThe output is as follows:\\n(20, 4)\\nNow, let\\'s do an experiment. I will first consider a single word as the only dimension of our\\ntext. Then, I will count how many of pieces of text are within 1 unit of each other. For\\nexample, if two sentences both contain that word, they would be 0 units away\\nand, similarly, if neither of them contain the word, they would be 0 units away from one\\nanother:\\nd = 1\\n# Let\\'s look for points within 1 unit of one another\\nX_first_word = X.iloc[:,:1]\\n# Only looking at the first column, but ALL of the rows\\nfrom sklearn.neighbors import NearestNeighbors\\n# this module will calculate for us distances between each point\\nneigh = NearestNeighbors(n_neighbors=4086)\\nneigh.fit(X_first_word)\\n# tell the module to calculate each distance between each point\\nNote that we have 16,695,396 ( 4086*4086 ) distances to scan over.\\nA = neigh.kneighbors_graph(X_first_word, mode=\\'distance\\').todense()\\n# This matrix holds all distances (over 16 million of them)\\nnum_points_within_d = (A < d).sum()\\n# Count the number of pairs of points within 1 unit of distance\\nnum_points_within_d\\n16258504\\n\\no, 16.2 million pairs of texts are within a single unit of distance. Now, let\\'s try again with\\nthe first two words:\\nX_first_two_words = X.iloc[:,:2]\\nneigh = NearestNeighbors(n_neighbors=4086)\\nneigh.fit(X_first_two_words)\\nA = neigh.kneighbors_graph(X_first_two_words, mode=\\'distance\\').todense()\\nnum_points_within_d = (A < d).sum()\\nnum_points_within_d\\n16161970\\nGreat! By adding this new column, we lost about 100,000 pairs of points that were within a\\nsingle unit of distance. This is because we are adding space in between  them for every \\ndimension  that we add. Let\\'s take this test a step further and calculate this number for the\\nfirst 100 words and then plot the results:\\nd = 1\\n# Scan for points within one unit\\nnum_columns = range(1, 100)\\n# Looking at the first 100 columns\\npoints = []\\n# We will be collecting the number of points within 1 unit for a graph\\nneigh = NearestNeighbors(n_neighbors=X.shape[0])\\nfor subset in num_columns:\\n    X_subset = X.iloc[:,:subset]\\n  # look at the first column, then first two columns, then first three\\ncolumns, etc\\n    neigh.fit(X_subset)\\n    A = neigh.kneighbors_graph(X_subset, mode=\\'distance\\').todense()\\n    num_points_within_d = (A < d).sum()\\n# calculate the number of points within 1 unit\\n    points.append(num_points_within_d)\\nNow, let\\'s plot the number of points within 1 unit versus the number of dimensions we\\nlooked at:\\n\\nWe can see clearly that the number of points within a single unit of one another goes down\\ndramatically as we introduce more and more columns. And this is only the first 100\\ncolumns! Let\\'s see how many points are within a single unit by the time we consider all\\n18,000+ words:\\nneigh = NearestNeighbors(n_neighbors=4086)\\nneigh.fit(X)\\nA = neigh.kneighbors_graph(X, mode=\\'distance\\').todense()\\nnum_points_within_d = (A < d).sum()\\nnum_points_within_d\\n4090\\nBy the end, only 4,000 sentences are within a unit of one another. All of this space that we\\nadd in by considering new columns makes it harder for the finite amount of points we have\\nto stay happily within range of each other. We would have to add in more points in order\\nto fill in this gap. And that, my friends, is why we should consider using dimension\\nreduction.\\n\\nhe curse of dimensionality is solved by either adding more data points (which is not\\nalways possible) or implementing dimension reduction. Dimension reduction is simply the\\nact of reducing the number of columns in our dataset and not the number of rows. There\\nare two ways of implementing dimension reduction:\\nFeature selection : This is the act of subsetting our column features and only\\nusing the best features\\nFeature extraction : This is the act of mathematically transforming our feature set\\ninto a new extracted coordinate system\\nWe are familiar with feature selection as the process of saying the \" Embarked_Q \" is not\\nhelping my decision tree; let\\'s get rid of it and see how it performs . It is literally when we (or the\\nmachine) make the decision to ignore certain columns.\\nFeature extraction is a bit trickier …\\nIn feature extraction, we are using usually fairly complicated mathematical formulas in\\norder to obtain new super columns  that are usually better than any single original column.\\nOur primary model for doing so is called Principal Component Analysis  (PCA ). PCA will\\nextract a set number of super columns in order  to represent our original  data with much\\nfewer columns. Let\\'s take a concrete example. Previously, I mentioned some text with 4,086\\nrows and over 18,000 columns. That dataset is actually a set of Yelp online reviews:\\nurl = \\'../data/yelp.csv\\'\\nyelp = pd.read_csv(url, encoding=\\'unicode-escape\\')\\n# create a new DataFrame that only contains the 5-star and 1-star reviews\\nyelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\\n# define X and y\\nX = yelp_best_worst.text\\ny = yelp_best_worst.stars == 5\\nOur goal is to predict whether or not a person gave a 5- or 1-star review  based on the words\\nthey used in the review. Let\\'s set a base line with logistic regression and see how well we\\ncan predict this binary category:\\nfrom sklearn.linear_model import LogisticRegression\\nlr = LogisticRegression()\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\\n# Make our training and testing sets\\nvect = CountVectorizer(stop_words=\\'english\\')\\n# Count the number of words but remove stop words like a, an, the, you, etc\\n\\n_train_dtm = vect.fit_transform(X_train)\\nX_test_dtm = vect.transform(X_test)\\n# transform our text into document term matrices\\nlr.fit(X_train_dtm, y_train)\\n# fit to our training set\\nlr.score(X_test_dtm, y_test)\\n# score on our testing set\\nThe output is as follows:\\n0.91193737\\nSo, by utilizing all of the words in our corpus, our model seems to have over a 91%\\naccuracy. Not bad!\\nLet\\'s try only using the top 100 used words:\\nvect = CountVectorizer(stop_words=\\'english\\', max_features=100)\\n# Only use the 100 most used words\\nX_train_dtm = vect.fit_transform(X_train)\\nX_test_dtm = vect.transform(X_test)\\nprint( X_test_dtm.shape) # (1022, 100)\\nlr.fit(X_train_dtm, y_train)\\nlr.score(X_test_dtm, y_test)\\nThe output is as follows:\\n0.8816\\nNote how our training and testing matrices have 100 columns. This is because I told our\\nvectorizer to only look at the top 100 words. See also that our performance took a hit and is\\nnow down to 88% accuracy. This makes sense because we are ignoring over 4,700 words in\\nour corpus.\\nNow, let\\'s take a different approach. Let\\'s import a PCA module and tell it to make us 100\\nnew super columns and see how that performs:\\nfrom sklearn import decomposition\\n# We will be creating 100 super columns\\nvect = CountVectorizer(stop_words=\\'english\\')\\n# Don\\'t ignore any words\\npca = decomposition.PCA(n_components=100)\\n# instantate a pca object\\n\\n_train_dtm = vect.fit_transform(X_train).todense()\\n# A dense matrix is required to pass into PCA, does not affect the overall\\nmessage\\nX_train_dtm = pca.fit_transform(X_train_dtm)\\nX_test_dtm = vect.transform(X_test).todense()\\nX_test_dtm = pca.transform(X_test_dtm)\\nprint( X_test_dtm.shape) # (1022, 100)\\nlr.fit(X_train_dtm, y_train)\\nlr.score(X_test_dtm, y_test)\\nThe output is as follows:\\n.89628\\nNot only do our matrices still have 100 columns, but these columns are no longer words in\\nour corpus. They are complex transformations of columns and are 100 new columns. Also\\nnote that using 100 of these new columns gives us a better predictive performance than\\nusing the 100 top words!\\nFeature extraction is a great way to use mathematical formulas to extract brand new\\ncolumns that generally perform better than just selecting the best ones beforehand.\\nBut how do we visualize these new super columns? Well, I can think of no better way than\\nto look at an example using image analysis. Specifically, let\\'s make facial recognition\\nsoftware. OK? OK. Let\\'s begin by importing some faces given to us by scikit-learn:\\nfrom sklearn.datasets import fetch_lfw_people\\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\\n# introspect the images arrays to find the shapes (for plotting)\\nn_samples, h, w = lfw_people.images.shape\\n# for machine learning we use the 2 data directly (as relative pixel #\\npositions info is ignored by this model)\\nX = lfw_people.data\\ny = lfw_people.target\\nn_features = X.shape[1]\\nX.shape (1288, 1850)\\n\\ne have gathered 1,288 images of people\\'s faces, and each one has 1,850 features (pixels)\\nthat identify  that person. Here\\'s an example:\\nplt.imshow(X[0].reshape((h, w)), cmap=plt.cm.gray)\\nlfw_people.target_names[y[0]] \\'Hugo Chavez\\'\\nplt.imshow(X[100].reshape((h, w)), cmap=plt.cm.gray)\\nlfw_people.target_names[y[100]] \\'George W Bush\\'\\n\\n\\nreat. To get a glimpse at the type of dataset we are looking at, let\\'s look at a few overall\\nmetrics:\\n# the label to predict is the id of the person\\ntarget_names = lfw_people.target_names\\nn_classes = target_names.shape[0]\\nprint(\"Total dataset size:\")\\nprint(\"n_samples: %d\" % n_samples)\\nprint(\"n_features: %d\" % n_features)\\nprint(\"n_classes: %d\" % n_classes)\\nTotal dataset size:\\nn_samples: 1288\\nn_features: 1850\\nn_classes: 7\\nSo, we have 1,288 images, 1,850 features, and 7 classes (people) to choose from. Our goal is\\nto make a classifier that will assign the person\\'s face a name based on the 1,850 pixels given\\nto us.\\nLet\\'s take a base line and see how logistic regression performs on our data without doing\\nanything:\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\nfrom time import time # for timing our work\\nX_train, X_test, y_train, y_test = train_test_split(\\nX, y, test_size=0.25, random_state=1)\\n# get our training and test set\\nt0 = time() # get the time now\\nlogreg = LogisticRegression()\\nlogreg.fit(X_train, y_train)\\n# Predicting people\\'s names on the test set\\ny_pred = logreg.predict(X_test)\\nprint( accuracy_score(y_pred, y_test), \"Accuracy\")\\nprint( (time() - t0), \"seconds\" )\\nThe output is as follows:\\n0.810559006211 Accuracy\\n6.31762504578 seconds\\nSo, within 6.3 seconds, we were able to get an 81% on our test set. Not too bad.\\n\\now let\\'s try this with our super faces :\\n# split into a training and testing set\\nfrom sklearn.cross_validation import train_test_split\\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled #\\ndataset): unsupervised feature extraction / dimensionality reduction\\nn_components = 75\\nprint(\"Extracting the top %d eigenfaces from %d faces\"\\n% (n_components, X_train.shape[0]))\\npca = decomposition.PCA(n_components=n_components,\\nwhiten=True).fit(X_train)\\n# This whiten parameter speeds up the computation of our extracted columns\\n# Projecting the input data on the eigenfaces orthonormal basis\\nX_train_pca = pca.transform(X_train)\\nX_test_pca = pca.transform(X_test)\\nThe preceding code is collecting 75 extracted columns from our 1,850 unprocessed columns.\\nThese are our super faces. Now, let\\'s plug in our newly extracted columns into our logistic\\nregression and compare:\\nt0 = time()\\n# Predicting people\\'s names on the test set WITH PCA\\nlogreg.fit(X_train_pca, y_train)\\ny_pred = logreg.predict(X_test_pca)\\nprint accuracy_score(y_pred, y_test), \"Accuracy\"\\nprint (time() - t0), \"seconds\"\\n0.82298136646 Accuracy\\n0.194181919098 seconds\\nWow! Not only was this entire calculation about 30 times faster than the unprocessed\\nimages, but the predictive performance also got better! This shows us that PCA and feature\\nextraction, in general, can help us all around when performing machine learning on\\ncomplex datasets with many columns. By searching for these patterns in the dataset and\\nextracting new feature  columns, we can speed up and enhance  our learning algorithms.\\nLet\\'s look at one more interesting thing. I mentioned before that one of the purposes of this\\nexample was to examine and visualize our eigenfaces , as they are called: our super columns.\\nI will not disappoint. Let\\'s write some code that will show us our super columns as they\\nwould look to us humans:\\ndef plot_gallery(images, titles, n_row=3, n_col=4):\\n\"\"\"Helper function to plot a gallery of portraits\"\"\"\\nplt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\\nplt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\\nfor i in range(n_row * n_col):\\n\\nlt.subplot(n_row, n_col, i + 1)\\nplt.imshow(images[i], cmap=plt.cm.gray)\\nplt.title(titles[i], size=12)\\n# plot the gallery of the most significative eigenfaces\\neigenfaces = pca.components_.reshape((n_components, h, w))\\neigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\\nplot_gallery(eigenfaces, eigenface_titles)\\nplt.show()\\n\\n\\now. A haunting and yet beautiful representation of what the data believes to be the most\\nimportance features of a face. As we move from the top-left (first super column) to the\\nbottom, it is actually somewhat easy to see what the image is trying to tell us. The first\\nsuper column looks like a very general face structure with eyes and nose and a mouth. It is\\nalmost saying \"I represent the basic qualities of a face that all faces must have.\" Our second\\nsuper column directly to its right seems to be telling us about shadows in the image. The\\nnext one might be telling us that skin tone plays a role in detecting who this is, which might\\nbe why the third face is much darker than the first two.\\nUsing feature extraction unsupervised learning methods such as PCA can give us a very\\ndeep look into our data and reveal to us what the data believes to be the most important\\nfeatures, not just what we believe them to be. Feature extraction is a great preprocessing\\ntool that can speed up our future learning methods, make them more powerful, and give us\\nmore insight into how the data believes it should be viewed. To sum up this section, we\\nwill list the pros and cons.\\nHere are the pros of using feature extraction:\\nOur models become much faster\\nOur predictive performance can become better\\nIt can give us insight into the extracted features (eigenfaces)\\nAnd here are the cons of using feature extraction:\\nWe lose some of the interpretability  of our features as they  are new\\nmathematically-derived columns, not our old ones\\nWe can lose predictive performance because we are losing information as we\\nextract fewer columns\\n\\nummary\\nBetween decision trees, Na ïve Bayes classification, feature extraction, and k-means\\nclustering, we have seen that machine learning goes way beyond the simplicity of linear\\nand logistic regression and can solve many types of complicated problems.\\nWe also saw examples of both supervised and unsupervised learning and, in doing so,\\nbecame familiar with many types of data science related problems.\\nIn the next chapter, we will be looking at even more complicated learning algorithms,\\nincluding artificial neural networks and ensembling techniques. We will also see and\\nunderstand more complicated concepts in data science, including the bias-variance\\ntradeoff, as well as the concept of overfitting.\\n\\n2\\nBeyond the Essentials\\nIn this chapter,  we will be discussing some of the more complicated parts of data science\\nthat can put some people off. The reason for this is that data science is not all fun and\\nmachine learning. Sometimes, we have to discuss and consider theoretical and\\nmathematical paradigms and evaluate our procedures.\\nThis chapter will explore many of these procedures step by step so that we completely and\\ntotally understand the topics. We will be discussing topics such as the following:\\nCross-validation\\nThe bias/variance tradeoff\\nOverfitting and underfitting\\nEnsembling techniques\\nRandom forests\\nNeural networks\\nThese are only some of the topics to be covered. At no point do I want you to be confused. I\\nwill attempt to explain each procedure/algorithm with the utmost care and with many\\nexamples and visuals.\\n\\nhe bias/variance tradeoff\\nWe have discussed the concept of bias and variance  briefly in the previous chapters. When\\nwe are discussing these two concepts, we are generally speaking of supervised learning\\nalgorithms. We are specifically talking about deriving errors from our predictive models\\ndue to bias and variance.\\nErrors due to bias\\nWhen speaking of errors due to bias, we are speaking  of the difference between the\\nexpected prediction of our model and the actual (correct) value, which we are trying to\\npredict. Bias, in effect, measures how far, in general, our model\\'s predictions are from the\\ncorrect value.\\nThink about bias as simply being the difference between a predicted value and the actual\\nvalue. For example, consider that our model, represented as F(x), predicts the value of 29 as\\nfollows:\\nHere, the value of 29 should have been predicted as 79:\\nIf a machine learning model tends to be very accurate in its prediction (regression or\\nclassification), then it is considered a low bias model, whereas if the model is more often\\nthan not wrong, it is considered to be a high bias model.\\nBias is a measure you can use to judge models on the basis of their accuracy  or just how\\ncorrect the model is on average.\\nError due to variance\\nAn error due to variance is dependent  on the variability of a model\\'s prediction for a given\\ndata point. Imagine that you repeat the machine learning model building process over and\\nover. The variance is measured by looking at how much the predictions for a fixed point\\nvary between different end results.\\n\\no imagine variance in your head, think about a population of data points. If you were to\\ntake randomized samples over and over, how drastically would your machine learning\\nmodel change or fit differently each time? If the model does not change much between\\nsamples, the model would be considered a low variance model. If your model changes\\ndrastically between samples, then that model would be considered a high variance model.\\nVariance is a great measure with which to judge our model on the basis of generalizability.\\nIf our model has a low variance, we can expect it to behave in a certain way when using it\\nin the wild and predict values without human supervision.\\nOur goal is to optimize both bias and variance. Ideally, we are looking for the lowest\\npossible variance and bias.\\nI find that this can be best explained using an example.\\nExample – comparing body and brain weight of\\nmammals\\nImagine that we are considering a relationship between  the brain weight of mammals and\\ntheir corresponding body weights. A hypothesis might read that there is a positive\\ncorrelation between the two (as one goes up, so does the other). But how strong is this\\nrelationship? Is it even linear? Perhaps, as the brain weight increases, there is a logarithmic\\nor quadratic increase in body weight.\\nLet\\'s use Python to explore this:\\n# # Exploring the Bias-Variance Tradeoff\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\n%matplotlib inline\\n\\n will be using a module, called seaborn , to visualize data points as a scatter plot and also\\nto graph linear (and higher polynomial) regression models:\\n# ## Brain and body weight\\n\\'\\'\\'\\nThis is a [dataset]) of the average\\nweight of the body and the brain for\\n62 mammal species. Let\\'s read it into pandas and\\ntake a quick look:\\n\\'\\'\\'\\ndf =\\npd.read_table(\\'http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.\\ntxt\\', sep=\\'\\\\s+\\', skiprows=33, names=[\\'id\\',\\'brain\\',\\'body\\'], index_col=\\'id\\')\\ndf.head()\\nWe are going to take a small subset of the samples  to exaggerate the visual representations\\nof bias and variance, as follows:\\n# We\\'re going to focus on a smaller subset in which the body weight is less\\nthan 200:\\ndf = df[df.body < 200]\\ndf.shape\\nThe output is as follows:\\n(51, 2)\\n\\ne\\'re actually going to pretend that there are only 51 mammal species in existence. In other\\nwords, we are pretending that this is the entire dataset of brain and body weights for every \\nknown  mammal species:\\n# Let\\'s create a scatterplot\\nsns.lmplot(x=\\'body\\', y=\\'brain\\', data=df, ci=None, fit_reg=False)\\nScatter plot of mammalian brain and body weights\\nThere appears to be a relationship between brain and body weight for mammals. So far, we\\nmight assume that it is a positive correlation.\\n\\now, let\\'s throw a linear regression  into the mix. Let\\'s use seaborn  to make and plot a\\nfirst-degree polynomial (linear) regression:\\nsns.lmplot(x=\\'body\\', y=\\'brain\\', data=df, ci=None)\\nThe same scatter plot as before with a linear regression visualization put in\\nNow, let\\'s pretend that a new mammal species is discovered. We measure the body weight\\nof every member of this species that we can find and calculate an average body weight of\\n100. We want to predict the average brain weight of this species (rather than measuring it\\ndirectly). Using this line, we might predict a brain weight of about 45.\\n\\nomething you might note is that this line isn\\'t that close to the data points in the graph, so\\nmaybe it isn\\'t the best model to use! You might argue that the bias is too high . And I would\\nagree! Linear regression models tend to have a high bias, but linear regression also has\\nsomething up its sleeve: it has a very low variance. However, what does that really mean?\\nLet\\'s say that we take our entire population of mammals  and randomly split them into two\\nsamples, as follows:\\n# set a random seed for reproducibility np.random.seed(12345) # randomly\\nassign every row to either sample 1 or sample 2 df[\\'sample\\'] =\\nnp.random.randint(1, 3, len(df)) df.head()\\nWe include a new sample column:\\n# Compare the two samples, they are fairly different!\\ndf.groupby(\\'sample\\')[[\\'brain\\', \\'body\\']].mean()\\n\\n\\ne can now tell seaborn  to create two plots, in which  the left plot only uses the data from\\nsample 1 and the right plot only uses the data from sample 2:\\n# col=\\'sample\\' subsets the data by sample and creates two\\n# separate plots\\nsns.lmplot(x=\\'body\\', y=\\'brain\\', data=df, ci=None, col=\\'sample\\')\\nSide-by-side scatter plots of linear regressions for samples 1 and 2\\nThey barely look different, right? If you look closely, you will note that not a single data\\npoint is shared between the samples and yet the line looks almost identical. To further\\nshow this point, let\\'s put both the lines of best fit in the same graph and use colors to\\nseparate the samples, as illustrated:\\n# hue=\\'sample\\' subsets the data by sample and creates a # single plot\\nsns.lmplot(x=\\'body\\', y=\\'brain\\', data=df, ci=None, hue=\\'sample\\')\\n\\nPresentation of two lines of best ﬁt in the same graph\\nThe line looks pretty similar between the two plots, despite the fact that they used separate\\nsamples of data. In both cases, we would predict a brain weight of about 45.\\nThe fact that even though the linear regression was given to completely distinct datasets\\npulled from the same population, it produced a very similar line, suggests that the model is\\nof low variance.\\n\\nhat if we increased our model\\'s complexity and allowed it to learn more? Instead of\\nfitting a line, let\\'s let seaborn  fit a fourth-degree polynomial (a quartic polynomial). By\\nadding to the degree  of the polynomial, the graph will be able to make twists and turns in\\norder to fit our data better, as shown:\\n# What would a low bias, high variance model look like? Let\\'s try\\npolynomial regression, with an fourth order polynomial:\\nsns.lmplot(x=\\'body\\', y=\\'brain\\', data=df, ci=None, col=\\'sample\\', order=4)\\nUsing a quartic polynomial for regression purposes\\nNote how, for two distinct samples from the same population, the quartic polynomial looks\\nvastly different. This is a sign of high variance.\\nThis model is low bias because it matches our data well! However, it has high variance\\nbecause the models are wildly different depending upon which points happen to be in the\\nsample. (For a body weight of 100, the brain weight prediction would either be 40 or 0,\\ndepending upon which data happened to be in the sample.)\\n\\nur polynomial is also unaware of the general relationship of the data. It seems obvious\\nthat there is a positive correlation between the brain and body weight of mammals.\\nHowever, in our quartic  polynomials, this relationship is nowhere to be found and is\\nunreliable. In our first sample (the graph on the left), the polynomial ends up shooting\\ndownwards, while in the second graph, the graph is going upwards towards the end. Our\\nmodel is unpredictable and can behave wildly differently, depending on the given training\\nset.\\nIt is our job, as data scientists, to find the middle ground.\\nPerhaps we can create a model that has less bias than the linear model, and less variance\\nthan the fourth-order polynomial:\\n# Let\\'s try a second order polynomial instead:\\nsns.lmplot(x=\\'body\\', y=\\'brain\\', data=df, ci=None, col=\\'sample\\', order=2)\\nScatter plot using a quadratic polynomial as our estimator\\nThis plot seems to have a good balance of bias and variance.\\n\\nwo extreme cases of bias/variance tradeoff\\nWhat we just saw were two extreme cases  of model fitting: one was underfitting and the\\nother was overfitting.\\nUnderfitting\\nUnderfitting occurs when our models  make little to no attempt  to fit our data. Models that\\nare high bias and low variance are prone to underfitting. In the case of the mammal\\nbrain/body weight example, the linear regression is underfitting our data. While we have a\\ngeneral shape of the relationship, we are left with a high bias.\\nIf your learning algorithm shows high bias and/or is underfitting, the following suggestions\\nmay help:\\nUse more features : Try including new features into the model if it helps with our\\npredictive power.\\nTry a more complicated model : Adding complexity to your model can help\\nimprove bias. An overly complicated model will hurt too!\\nOverfitting\\nOverfitting is the result of the model trying too hard to fit into the training set, resulting in a\\nlower bias but a much higher variance. Models that are low bias and high variance are \\nprone  to overfitting. In the case of the mammal brain/body weight example, the fourth-\\ndegree polynomial (quartic) regression is overfitting our data.\\nIf your learning algorithm  shows high variance and/or is overfitting, the following\\nsuggestions may help:\\nUse fewer features : Using fewer features can decrease our variance and prevent\\noverfitting\\nFit on more training samples : Using more training data points in our cross-\\nvalidation can reduce the effect of overfitting, and improve our high variance\\nestimator\\n\\now bias/variance play into error functions\\nError functions (which measure how incorrect  our models are) can be thought of as\\nfunctions of bias, variance, and irreducible error. Mathematically put, the error of\\npredicting a dataset using  our supervised learning model might look as follows:\\nHere, Bias2 is our bias term squared (which arises when simplifying the mentioned\\nstatement from more complicated equations), and Variance  is a measurement of how much\\nour model fitting varies between randomized samples.\\nSimply put, both bias and variance contribute to errors. As we increase our model\\ncomplexity (for example, go from a linear regression to an eighth-degree polynomial\\nregression or grow our decision trees deeper), we find that Bias2 decreases, Variance\\nincreases, and the total error of the model forms a parabolic shape, as illustrated:\\nRelationship of bias and variance\\nOur goal, as data scientists, is to find the sweet spot  that optimizes our model complexity. It\\nis easy to overfit our data. To combat overfitting in practice, we should always use cross-\\nvalidation (splitting up datasets iteratively and retraining models and averaging metrics) to\\nget the best predictor of an error.\\n\\no illustrate this point, I will introduce (quickly) a new supervised algorithm and\\ndemonstrate the bias/variance tradeoff visually.\\nWe will be using the K-Nearest Neighbors  (KNN ) algorithm, which is a supervised\\nlearning algorithm that uses a lookalike paradigm, which means  that it makes predictions\\nbased on similar data points seen in the past.\\nKNN has a complexity input, K, which represents  how many similar  data points to\\ncompare to. If K = 3 , then, for a given input, we look to the nearest three data points and use\\nthem for our prediction. In this case, K represents our model\\'s complexity:\\nfrom sklearn.neighbors import KNeighborsClassifier\\n# read in the iris data\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nX, y = iris.data, iris.target\\nSo, we have our X and our y. A great way to overfit a model is to train and predict the exact\\nsame data:\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X, y)\\nknn.score(X, y)\\nThe output is as follows:\\n1.0\\nWow, 100% accuracy?! This is too good to be true.\\nBy training and predicting the same data, we are essentially telling our data to purely\\nmemorize the training set and spit it back to us (this is called our training error). This is the\\nreason we introduced our training and test sets in Chapter 10 , How to Tell If Your Toaster Is\\nLearning – Machine Learning Essentials .\\nK folds cross-validation\\nK folds cross-validation is a much better estimator  of our model\\'s performance, even more\\nso than our train-test split. Here\\'s how it works:\\nWe will take a finite number of equal slices of our data (usually 3, 5, or 10).1.\\nAssume that this number is called k.\\nFor each \"fold\" of the cross-validation, we will treat k-1 of the sections as the 2.\\ntraining set, and the remaining section as our test set.\\n\\nor the remaining folds, a different arrangement of k-1 sections is considered for 3.\\nour training set and a different section is our training set.\\nWe compute a set metric for each fold of the cross-validation.4.\\nWe average our scores at the end.5.\\nCross-validation is effectively using multiple train-test splits being done on the same\\ndataset. This is done for a few reasons, but mainly because cross-validation is the most \\nhonest  estimate of our model\\'s out-of-sample  (OOS ) error.\\nTo explain this visually, let\\'s look at our mammal brain and body weight example for a\\nsecond. The following code manually creates a five-fold cross-validation, wherein five\\ndifferent training and test sets are made from the same population:\\nimport matplotlib.pyplot as plt\\nfrom sklearn.cross_validation import KFold\\ndf =\\npd.read_table(\\'http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.\\ntxt\\', sep=\\'\\\\s+\\', skiprows=33, names=[\\'id\\',\\'brain\\',\\'body\\'])\\ndf = df[df.brain < 300][df.body < 500]\\n# limit points for visibility\\nnfolds = 5\\nfig, axes = plt.subplots(1, nfolds, figsize=(14,4))\\nfor i, fold in enumerate(KFold(len(df), n_folds=nfolds,\\n                              shuffle=True)):\\n    training, validation = fold\\n    x, y = df.iloc[training][\\'body\\'], df.iloc[training][\\'brain\\']\\n    axes[i].plot(x, y, \\'ro\\')\\n    x, y = df.iloc[validation][\\'body\\'], df.iloc[validation][\\'brain\\']\\n    axes[i].plot(x, y, \\'bo\\')\\nplt.tight_layout()\\nFive-fold cross-validation: red = training sets, blue = test sets\\n\\nere, each graph shows the exact same population of mammals, but the dots are colored\\nred if they belong to the training set of that fold and blue if they belong to the testing set. By\\ndoing this, we are obtaining five different instances of the same machine learning model in\\norder to see if the performance remains consistent across the folds.\\nIf you stare at the dots long enough, you will note that each dot appears in a training set\\nexactly four times ( k - 1), while the same dot appears in a test set exactly once and only\\nonce.\\nSome features of k-fold cross-validation include  the following:\\nIt is a more accurate estimate of the OOS prediction error than a single train-test\\nsplit because it is taking several independent train-test splits and averaging the\\nresults together.\\nIt is a more efficient use of data than single train-test splits because the entire\\ndataset is being used for multiple train-test splits instead of just one.\\nEach record in our dataset is used for both training and testing.\\nThis method presents a clear tradeoff between efficiency and computational\\nexpense. A 10-fold CV is 10x more expensive computationally than a single\\ntrain/test split.\\nThis method can be used for parameter tuning and model selection.\\nBasically, whenever we wish to test a model on a set of data, whether we just completed\\ntuning some parameters or feature engineering, a k-fold cross-validation is an excellent\\nway to estimate the performance on our model.\\nOf course, sklearn  comes with an easy-to-use cross-validation module, called\\ncross_val_score , which automatically splits up our dataset for us, runs the model on\\neach fold, and gives us a neat and tidy output of results:\\n# Using a training set and test set is so important\\n# Just as important is cross validation. Remember cross validation\\n# is using several different train test splits and\\n# averaging your results!\\n## CROSS-VALIDATION\\n# check CV score for K=1\\nfrom sklearn.cross_validation import cross_val_score, train_test_split\\ntree = KNeighborsClassifier(n_neighbors=1)\\nscores = cross_val_score(tree, X, y, cv=5, scoring=\\'accuracy\\')\\nscores.mean()\\n0.95999999999\\n\\nhis is a much more reasonable accuracy than our previous score of 1. Remember that we\\nare not getting 100% accuracy anymore because we have a distinct training and test set. The\\ndata points that KNN has never seen are the test points and it, therefore, cannot match\\nthem exactly to themselves.\\nLet\\'s try cross-validating KNN with K=5 (increasing our model\\'s complexity), as shown:\\n# check CV score for K=5\\nknn = KNeighborsClassifier(n_neighbors=5)\\nscores = cross_val_score(knn, X, y, cv=5, scoring=\\'accuracy\\')\\nscores\\nnp.mean(scores)\\n0.97333333\\nEven better! So, now we have to find the best K? The best K is the one that maximizes  our\\naccuracy. Let\\'s try a few:\\n# search for an optimal value of K\\nk_range = range(1, 30, 2) # [1, 3, 5, 7, ..., 27, 29]\\nerrors = []\\nfor k in k_range:\\n    knn = KNeighborsClassifier(n_neighbors=k)\\n   # instantiate a KNN with k neighbors\\n   scores = cross_val_score(knn, X, y, cv=5, scoring=\\'accuracy\\')\\n # get our five accuracy scores\\n    accuracy = np.mean(scores)\\n   # average them together\\n    error = 1 - accuracy\\n   # get our error, which is 1 minus the accuracy\\n    errors.append(error)\\n   # keep track of a list of errors\\n\\ne now have an error value (1 - accuracy) for each value of K (1, 3, 5, 7, 9.., .., 29):\\n# plot the K values (x-axis) versus the 5-fold CV score (y-axis)\\nplt.figure()\\nplt.plot(k_range, errors)\\nplt.xlabel(\\'K\\')\\nplt.ylabel(\\'Error\\')\\nGraph of errors of the KNN model against KNN\\'s complexity, represented by the value of K\\nCompare this graph to the previous graph of model complexity  and bias/variance. Toward\\nthe left, our graph has a higher bias and is underfitting. As we increased our model\\'s\\ncomplexity, the error term began to go down, but after a while, our model became overly\\ncomplex, and the high variance kicked in, making our error term go back up.\\nIt seems that the optimal value of K is between 6 and 10.\\n\\nrid searching\\nsklearn  also has, up its sleeve, another useful  tool called grid searching. A grid search will\\nby brute force try many different model parameters and give us the best one based on a\\nmetric of our choosing. For example, we can choose to optimize KNN for accuracy in the\\nfollowing manner:\\nfrom sklearn.grid_search import GridSearchCV\\nfrom sklearn.neighbors import KNeighborsClassifier\\n# import our grid search module\\nknn = KNeighborsClassifier(n_jobs=-1)\\n# instantiate a blank slate KNN, no neighbors\\nk_range = list(range(1, 31, 2))\\nprint(k_range)\\n#k_range = range(1, 30)\\nparam_grid = dict(n_neighbors=k_range)\\n# param_grid = {\"n_ neighbors\": [1, 3, 5, ...]}\\nprint(param_grid)\\ngrid = GridSearchCV(knn, param_grid, cv=5, scoring=\\'accuracy\\')\\ngrid.fit(X, y)\\n\\nn the grid.fit()  line of code, what is happening is that, for each combination of features\\nin this case, we have 15 different possibilities for K, so we are cross-validating each one five\\ntimes. This means that by the end of this code, we will have 15 * 5 = 75  different KNN\\nmodels! You can see how, when applying  this technique to more complex models, we could\\nrun into difficulties with time:\\n# check the results of the grid search\\ngrid.grid_scores_\\ngrid_mean_scores = [result[1] for result in grid.grid_scores_]\\n# this is a list of the average accuracies for each parameter\\n# combination\\nplt.figure()\\nplt.ylim([0.9, 1])\\nplt.xlabel(\\'Tuning Parameter: N nearest neighbors\\')\\nplt.ylabel(\\'Classification Accuracy\\')\\nplt.plot(k_range, grid_mean_scores)\\nplt.plot(grid.best_params_[\\'n_neighbors\\'], grid.best_score_, \\'ro\\',\\nmarkersize=12, markeredgewidth=1.5,\\n         markerfacecolor=\\'None\\', markeredgecolor=\\'r\\')\\nClassiﬁcation Accuracy versus Tuning Parameters in N nearest neighbors\\nNote that the preceding graph is basically the same as the one we achieved previously with\\nour for loop, but much easier!\\n\\ne see that seven neighbors (circled in the preceding graph) seem to have the best\\naccuracy. However, we can also, very easily, get our best parameters and our best model, as\\nshown:\\ngrid.best_params_\\n# {\\'n_neighbors\\': 7}\\ngrid.best_score_\\n# 0.9799999999\\ngrid.best_estimator_\\n# actually returns the unfit model with the best parameters\\n# KNeighborsClassifier(algorithm=\\'auto\\', leaf_size=30, metric=\\'minkowski\\',\\n           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\\n           weights=\\'uniform\\')\\nI\\'ll take this one step further. Maybe you\\'ve noted that KNN has other parameters as well,\\nsuch as algorithm , p, and weights . A quick look at the scikit-learn documentation reveals\\nthat we have some options for each of these, which are as follows:\\np is an integer and represents the type of distance we wish to use. By default, we\\nuse p=2, which is our standard distance formula.\\nweights  is, by default, uniform , but can also be distance , which weighs points\\nby their distance, meaning that close neighbors have a greater impact on the\\nprediction.\\nalgorithm  is how the model finds the nearest neighbors. We can try\\nball_tree , kd_tree , or brute . The default is auto , which tries to use the best\\none automatically:\\nknn = KNeighborsClassifier()\\nk_range = range(1, 30)\\nalgorithm_options = [\\'kd_tree\\', \\'ball_tree\\', \\'auto\\', \\'brute\\']\\np_range = range(1, 8)\\nweight_range = [\\'uniform\\', \\'distance\\']\\nparam_grid = dict(n_neighbors=k_range, weights=weight_range,\\nalgorithm=algorithm_options, p=p_range)\\n# trying many more options\\ngrid = GridSearchCV(knn, param_grid, cv=5, scoring=\\'accuracy\\')\\ngrid.fit(X, y)\\n\\nhe preceding code takes about a minute to run on my laptop  because it is trying many, 1,\\n648, different combinations of parameters and cross-validating each one five times. All in\\nall, to get the best answer, it is fitting 8,400 different KNN models:\\ngrid.best_score_\\n0.98666666\\ngrid.best_params_\\n{\\'algorithm\\': \\'kd_tree\\', \\'n_neighbors\\': 6, \\'p\\': 3, \\'weights\\': \\'uniform\\'}\\nGrid searching is a simple (but inefficient) way of parameter tuning our models to get the\\nbest possible outcome. It should be noted that to get the best possible outcome, data\\nscientists should use feature manipulation (both reduction and engineering) to obtain better\\nresults in practice as well. It should not merely be up to the model to achieve the best\\nperformance.\\nVisualizing training error versus cross-validation\\nerror\\nI think it is important once again to go over and compare the cross-validation error and the\\ntraining error. This time, let\\'s put them both on the same graph to compare  how they both \\nchange  as we vary the model  complexity.\\nI will use the mammal dataset once more to show the cross-validation error and the\\ntraining error (the error on predicting the training set). Recall that we are attempting to\\nregress the body weight of a mammal to the brain weight of a mammal:\\n# This function uses a numpy polynomial fit function to\\n# calculate the RMSE of given X and y\\ndef rmse(x, y, coefs):\\n    yfit = np.polyval(coefs, x)\\n    rmse = np.sqrt(np.mean((y - yfit) ** 2))\\n    return rmse\\nxtrain, xtest, ytrain, ytest = train_test_split(df[\\'body\\'], df[\\'brain\\'])\\ntrain_err = []\\nvalidation_err = []\\ndegrees = range(1, 8)\\nfor i, d in enumerate(degrees):\\n    p = np.polyfit(xtrain, ytrain, d)\\n  # built in numpy polynomial fit function\\n\\n   train_err.append(rmse(xtrain, ytrain, p))\\n    validation_err.append(rmse(xtest, ytest, p))\\nfig, ax = plt.subplots()\\n# begin to make our graph\\nax.plot(degrees, validation_err, lw=2, label = \\'cross-validation error\\')\\nax.plot(degrees, train_err, lw=2, label = \\'training error\\')\\n# Our two curves, one for training error, the other for cross validation\\nax.legend(loc=0)\\nax.set_xlabel(\\'degree of polynomial\\')\\nax.set_ylabel(\\'RMSE\\')\\nSo, we see that as we increase our degree of fit, our training error goes down without a\\nhitch, but we are now smart enough to know that as we increase the model complexity, our\\nmodel is overfitting to our data and is merely regurgitating our data back to us, whereas\\nour cross-validation error line is much more honest and begins to perform poorly after\\nabout degree 2 or 3.\\n\\net\\'s recap:\\nUnderfitting occurs when the cross-validation error and the training error are\\nboth high\\nOverrfitting occurs when the cross-validation error is high, while the training\\nerror is low\\nWe have a good fit when the cross-validation error is low, and only slightly\\nhigher than the training error\\nBoth underfitting (high bias) and overfitting (high variance) will result in poor\\ngeneralization of the data.\\nHere are some tips if you face high  bias or variance.\\nTry the following if your model  tends to have a high bias :\\nTry adding more features to the training and test sets\\nEither add to the complexity of your model  or try a more modern sophisticated\\nmodel\\nTry the following if your model tends to have a high variance :\\nTry to include more training samples, which reduces the effect of overfitting\\nIn general, the bias/variance tradeoff is the struggle to minimize the bias and variance in\\nour learning algorithms. Many newer learning algorithms, invented in the past few\\ndecades, were made with the intention of having the best of both worlds.\\nEnsembling techniques\\nEnsemble learning , or ensembling, is the process of combining  multiple predictive models\\nto produce a supermodel that is more accurate than any individual model on its own:\\nRegression : We will take the average of the predictions for each model\\nClassification : Take a vote and use the most common prediction, or take the\\naverage of the predicted probabilities\\n\\nmagine that we are working on a binary classification problem (predicting either 0 or 1):\\n# ENSEMBLING\\nimport numpy as np\\n# set a seed for reproducibility\\nnp.random.seed(12345)\\n# generate 2000 random numbers (between 0 and 1) for each model,\\nrepresenting 2000 observations\\nmod1 = np.random.rand(2000)\\nmod2 = np.random.rand(2000)\\nmod3 = np.random.rand(2000)\\nmod4 = np.random.rand(2000)\\nmod5 = np.random.rand(2000)\\nNow, we simulate five different learning models, and each  has about a 70% accuracy, as\\nfollows:\\n# each model independently predicts 1 (the \"correct response\") if random\\nnumber was at least 0.4\\npreds1 = np.where(mod1 > 0.4, 1, 0)\\npreds2 = np.where(mod2 > 0.4, 1, 0)\\npreds3 = np.where(mod3 > 0.4, 1, 0)\\npreds4 = np.where(mod4 > 0.4, 1, 0)\\npreds5 = np.where(mod5 > 0.4, 1, 0)\\nprint(preds1.mean())\\n0.596\\nprint (preds2.mean())\\n0.6065\\nprint (preds3.mean())\\n0.591\\nprint (preds4.mean())\\n0.5965\\nprint( preds5.mean())\\n# 0.611\\n# Each model has an \"accuracy of around 60% on its own\\nNow, let\\'s apply my degrees in magic. Er... sorry, math:\\n# average the predictions and then round to 0 or 1\\nensemble_preds = np.round((preds1 + preds2 + preds3 + preds4 +\\npreds5)/5.0).astype(int)\\nensemble_preds.mean()\\n\\nhe output is as follows:\\n0.674\\nAs you add more models to a voting process, the probability of errors will decrease; this is\\nknown as Condorcet\\'s jury theorem.\\nCrazy, right?\\nFor ensembling to work well in practice, the models must  have the following\\ncharacteristics:\\nAccuracy : Each model must at least outperform the null model\\nIndependence : A model\\'s prediction process is not affected by another model\\'s\\nprediction process\\nIf you have a bunch of individually OK models, the edge case mistakes made by one model\\nare probably not going to be made by the other models, so the mistakes will be ignored\\nwhen combining the models.\\nThere are the following two basic methods for ensembling:\\nManually ensemble your individual models by writing a good deal of code\\nUse a model that ensembles for you\\nWe\\'re going to look at a model that ensembles for us. To do this, let\\'s take a look at decision\\ntrees again.\\nDecision trees tend to have low bias and high variance. Given any dataset, the tree can keep\\nasking questions (making decisions) until it is able to nitpick and distinguish between every\\nsingle  example in the dataset. It could keep asking question after question until there is only\\na single example in each leaf (terminal) node. The tree is trying too hard, growing too deep,\\nand just memorizing every single detail of our training set. However, if we started over, the\\ntree could potentially ask different questions and still grow very deep. This means that\\nthere are many possible trees that could distinguish between all elements, which means\\nthe higher variance. It is unable to generalize well.\\nIn order to reduce the variance of a single tree, we can place a restriction on the number of\\nquestions asked in a tree (the max_depth  parameter) or we can create an ensemble version\\nof decision trees, called random forests.\\n\\nandom forests\\nThe primary weakness of decision trees is that different  splits in the training  data can lead\\nto very different trees. Bagging is a general purpose procedure to reduce the variance of a\\nmachine learning method but is particularly useful for decision trees.\\nBagging is short for Bootstrap aggregation, which means  the aggregation of Bootstrap\\nsamples. What is a Bootstrap sample?\\nA Bootstrap sample is a smaller  sample that is \" bootstrapped\" from a larger  sample .\\nBootstrapping is a type of resampling where large numbers of smaller  samples of the same\\nsize are repeatedly drawn, with replacement, from a single original  sample:\\n# set a seed for reproducibility\\nnp.random.seed(1)\\n# create an array of 1 through 20\\nnums = np.arange(1, 21)\\nprint (nums)\\nThe output is as follows:\\n[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\\n# sample that array 20 times with replacement\\nnp.random.choice(a=nums, size=20, replace=True)\\nThe preceding command will create a bootstrapped sample as follows:\\n [ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\\n# This is our bootstrapped sample notice it has repeat variables!\\nSo, how does bagging work for decision trees?\\nGrow B trees using Bootstrap samples from the training data 1.\\nTrain each tree on its Bootstrap sample and make predictions2.\\nCombine the predictions:3.\\nAverage the predictions for regression trees\\nTake a vote for classification trees\\n\\nhe following are a few things to note:\\nEach Bootstrap sample should be the same size as the original training set\\nB should be a large enough value that the error seems to have stabilized\\nThe trees are grown intentionally deep so that they have low bias/high variance\\nThe reason we grow the trees intentionally deep is that the bagging  inherently increases\\npredictive accuracy by reducing the variance, similar to how cross-validation reduces the\\nvariance associated with estimating our out-of-sample error.\\nRandom forests are a variation  of bagged trees.\\nHowever, when building each tree, each time we consider a split between the features, a\\nrandom sample of m features is chosen as split candidates from the full set of p features.\\nThe split is only allowed to be one of those m features:\\nA new random sample of features is chosen for every single  tree at every single\\nsplit\\nFor classification, m is typically chosen to be the square root of p\\nFor regression, m is typically chosen to be somewhere between p/3 and p\\nWhat\\'s the point?\\nSuppose there is one very strong feature in the dataset. When using decision (or bagged)\\ntrees, most of the trees will use that feature as the top split, resulting in an ensemble of\\nsimilar trees that are highly correlated with each other.\\nIf our trees are highly correlated with each other, then averaging these quantities will not\\nsignificantly reduce variance (which is the entire goal of ensembling). Also, by randomly\\nleaving out candidate features from each split, random forests reduce the variance of the\\nresulting model.\\n\\nandom forests can be used in both classification and regression problems and can be\\neasily used in scikit-learn. Let\\'s try to predict MLB salaries based on statistics about the\\nplayer, as shown:\\n# read in the data\\nurl = \\'../data/hitters.csv\\'\\nhitters = pd.read_csv(url)\\n# remove rows with missing values\\nhitters.dropna(inplace=True)\\n# encode categorical variables as integers\\nhitters[\\'League\\'] = pd.factorize(hitters.League)[0]\\nhitters[\\'Division\\'] = pd.factorize(hitters.Division)[0]\\nhitters[\\'NewLeague\\'] = pd.factorize(hitters.NewLeague)[0]\\n# define features: exclude career statistics (which start with \"C\") and the\\nresponse (Salary)\\nfeature_cols = [h for h in hitters.columns if h[0] != \\'C\\' and h !=\\n\\'Salary\\']\\n# define X and y\\nX = hitters[feature_cols]\\ny = hitters.Salary\\nLet\\'s try and predict the salary  first using a single  decision tree, as illustrated:\\nfrom sklearn.tree import DecisionTreeRegressor\\n# list of values to try for max_depth\\nmax_depth_range = range(1, 21)\\n# list to store the average RMSE for each value of max_depth\\nRMSE_scores = []\\n# use 10-fold cross-validation with each value of max_depth\\nfrom sklearn.cross_validation import cross_val_score\\nfor depth in max_depth_range:\\n    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\\n    MSE_scores = cross_val_score(treereg, X, y, cv=10,\\nscoring=\\'mean_squared_error\\')\\n    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\\n# plot max_depth (x-axis) versus RMSE (y-axis)\\nplt.plot(max_depth_range, RMSE_scores)\\n\\nlt.xlabel(\\'max_depth\\')\\nplt.ylabel(\\'RMSE (lower is better)\\')\\nRMSE for decision tree models against the max depth of the tree (complexity)\\nLet\\'s do the same thing, but this time with a random forest:\\nfrom sklearn.ensemble import RandomForestRegressor\\n# list of values to try for n_estimators\\nestimator_range = range(10, 310, 10)\\n# list to store the average RMSE for each value of n_estimators\\nRMSE_scores = []\\n# use 5-fold cross-validation with each value of n_estimators (WARNING:\\nSLOW!)\\nfor estimator in estimator_range:\\n    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\\n    MSE_scores = cross_val_score(rfreg, X, y, cv=5,\\nscoring=\\'mean_squared_error\\')\\n    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\\n\\n plot n_estimators (x-axis) versus RMSE (y-axis)\\nplt.plot(estimator_range, RMSE_scores)\\nplt.xlabel(\\'n_estimators\\')\\nplt.ylabel(\\'RMSE (lower is better)\\')\\nRMSE for random forest models against the max depth of the tree (complexity)\\nNote already the y-axis; our RMSE is much lower on an average! See how we can obtain a\\nmajor increase in predictive power using random forests.\\n\\nn random forests, we still have the concept of important features, like we had in decision\\ntrees:\\n# n_estimators=150 is sufficiently good\\nrfreg = RandomForestRegressor(n_estimators=150, random_state=1)\\nrfreg.fit(X, y)\\n# compute feature importances\\npd.DataFrame({\\'feature\\':feature_cols,\\n\\'importance\\':rfreg.feature_importances_}).sort(\\'importance\\', ascending =\\nFalse)\\nSo, it looks like the number of years the player  has been in the league  is still the most\\nimportant feature when deciding that player\\'s salary.\\n\\nomparing random forests with decision trees\\nIt is important to realize that just using  random forests is not the solution  to your data\\nscience problems. While random forests provide many advantages, many disadvantages\\nalso come with them.\\nThe advantages of random forests are as follows:\\nTheir performance is competitive with the best-supervised learning methods\\nThey provide a more reliable estimate of feature importance\\nThey allow you to estimate out-of-sample errors without using train/test splits or\\ncross-validation\\nThe disadvantages of random forests are as follows:\\nThey are less interpretable (cannot visualize an entire forest of decision trees)\\nThey are slower to train and predict (not great for production or real-time\\npurposes)\\nNeural networks\\nProbably one of the most talked about machine  learning models, neural networks are\\ncomputational networks built to model animals\\' nervous systems. Before getting too deep\\ninto the structure, let\\'s take a look at the big advantages of neural networks.\\nThe key component of a neural network is that it is not only a complex structure, but it is\\nalso a complex and flexible structure. This means the following two things:\\nNeural networks are able to estimate any function shape (this is called being non-\\nparametric)\\nNeural networks can adapt and literally change their own internal structure\\nbased on their environment\\n\\nasic structure\\nNeural networks are made up of interconnected nodes ( perceptrons ) that each  take in input\\n(quantitative value), and output other quantitative values. Signals travel through  the\\nnetwork and eventually end up at a prediction node:\\nVisualization of neural network interconnected nodes\\nAnother huge advantage of neural networks is that they can be used for supervised\\nlearning, unsupervised learning, and reinforcement learning problems. The ability to be so\\nflexible, predict many functional shapes, and adapt to their surroundings make neural\\nnetworks highly preferable in select fields, as follows:\\nPattern recognition : This is probably the most  common application of neural\\nnetworks. Some examples are handwriting recognition and image processing\\n(facial recognition).\\nEntity movement : Examples for this include  self-driving cars, robotic animals,\\nand drone movement.\\nAnomaly detection : As neural networks are good at recognizing  patterns, they\\ncan also be used to recognize when a data point does not fit a pattern. Think of a\\nneural network monitoring a stock price movement; after a while of learning the\\ngeneral pattern of a stock price, the network can alert you when something is\\nunusual in the movement.\\n\\nhe simplest form of a neural network is a single perceptron. A perceptron, visualized as\\nfollows, takes in some input and outputs a signal:\\nThis signal is obtained by combining  the input with several weights and then is put\\nthrough some activation function . In cases of simple binary outputs, we generally use the\\nlogistic function, as shown:\\nTo create a neural network, we need to connect multiple perceptrons to each other in a\\nnetwork fashion, as illustrated in the following graph.\\n\\n multilayer perceptron  (MLP ) is a finite acyclic  graph. The nodes are neurons with\\nlogistic activation:\\nAs we train the model, we update the weights (which are random at first) of the model in\\norder to get the best predictions possible. If an observation goes through the model and is\\noutputted as false when it should have been true, the logistic functions in the single\\nperceptrons are changed slightly. This is called back-propagation . Neural networks are \\nusually  trained in batches, which means that the network is given several training  data\\npoints at once several times, and each time, the back-propagation algorithm will trigger an\\ninternal weight change in the network.\\nIt isn\\'t hard to see that we can grow the network very deep and have many hidden layers,\\nwhich are associated with the complexity of the neural network. When we grow our neural\\nnetworks very deep, we are dipping our toes into the idea of deep learning . The main\\nadvantage of deep neural networks (networks with many layers) is that they can\\napproximate almost any shape function and they can (theoretically) learn optimal\\ncombinations of features for us and use these combinations to obtain the best predictive\\npower.\\n\\net\\'s see this in action. I will be using a module called PyBrain to make my neural\\nnetworks. However, first let\\'s take a look at a new dataset, which is a dataset of \\nhandwritten  digits. We will first try to recognize digits using a random forest, as shown:\\nfrom sklearn.cross_validation import cross_val_score\\nfrom sklearn import datasets\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestClassifier\\n%matplotlib inline\\ndigits = datasets.load_digits()\\nplt.imshow(digits.images[100], cmap=plt.cm.gray_r, interpolation=\\'nearest\\')\\n# a 4 digit\\nX, y = digits.data, digits.target\\n# 64 pixels per image\\nX[0].shape\\n# Try Random Forest\\nrfclf = RandomForestClassifier(n_estimators=100, random_state=1)\\ncross_val_score(rfclf, X, y, cv=5, scoring=\\'accuracy\\').mean()\\n0.9382782\\n\\nretty good! An accuracy of 94% is nothing to laugh at, but can we do even better?\\nWarning! The PyBrain syntax can be a bit tricky.\\nfrom pybrain.datasets            import ClassificationDataSet\\nfrom pybrain.utilities           import percentError\\nfrom pybrain.tools.shortcuts     import buildNetwork\\nfrom pybrain.supervised.trainers import BackpropTrainer\\nfrom pybrain.structure.modules   import SoftmaxLayer\\nfrom numpy import ravel\\n# pybrain has its own data sample class that we must add\\n# our training and test set to\\nds = ClassificationDataSet(64, 1 , nb_classes=10)\\nfor k in xrange(len(X)):\\n    ds.addSample(ravel(X[k]),y[k])\\n# their equivalent of train test split\\ntest_data, training_data = ds.splitWithProportion( 0.25 )\\n# pybrain\\'s version of dummy variables\\ntest_data._convertToOneOfMany( )\\ntraining_data._convertToOneOfMany( )\\nprint test_data.indim # number of pixels going in\\n# 64\\nprint test_data.outdim # number of possible options (10 digits)\\n# 10\\n# instantiate the model with 64 hidden layers (standard params)\\nfnn = buildNetwork( training_data.indim, 64, training_data.outdim,\\noutclass=SoftmaxLayer )\\ntrainer = BackpropTrainer( fnn, dataset=training_data, momentum=0.1,\\nlearningrate=0.01 , verbose=True, weightdecay=0.01)\\n# change the number of epochs to try to get better results!\\ntrainer.trainEpochs (10) # 10 batches\\n\\nrint \\'Percent Error on Test dataset: \\' , \\\\\\n        percentError( trainer.testOnClassData (\\n           dataset=test_data )\\n           , test_data[\\'class\\'] )\\nThe model will output a final error on a test set:\\nPercent Error on Test dataset: 4.67706013363\\naccuracy = 1 - .0467706013363\\naccuracy\\n0.95322\\nAlready better! Both the random forests and neural networks  do very well with this\\nproblem because both of them are non-parametric, which means that they do not rely on\\nthe underlying shape of the data to make predictions. They are able to estimate any shape\\nof function.\\nTo predict the shape, we can use the following code:\\nplt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation=\\'nearest\\')\\nfnn.activate(X[0])\\narray([ 0.92183643,  0.00126609,  0.00303146,  0.00387049,  0.01067609,\\n        0.00718017,  0.00825521,  0.00917995,  0.00696929,  0.02773482])\\n\\n\\nhe array represents a probability for every single digit, which means that there is a 92%\\nchance that the digit in the preceding screenshot is a 0 (which it is). Note how the next\\nhighest probability is for a 9, which makes sense because 9 and 0 have similar shapes\\n(ovular).\\nNeural networks do have a major flaw. If left alone, they have a very high variance. To see\\nthis, let\\'s run the exact same code as the preceding one and train the exact same type of\\nneural network  on the exact same data, as illustrated:\\n# Do it again and see the difference in error\\nfnn = buildNetwork( training_data.indim, 64, training_data.outdim,\\noutclass=SoftmaxLayer )\\ntrainer = BackpropTrainer( fnn, dataset=training_data, momentum=0.1,\\nlearningrate=0.01 , verbose=True, weightdecay=0.01)\\n# change the number of eopchs to try to get better results!\\ntrainer.trainEpochs (10)\\nprint (\\'Percent Error on Test dataset: \\' , \\\\\\n        percentError( trainer.testOnClassData (\\n           dataset=test_data )\\n           , test_data[\\'class\\'] ) )\\naccuracy = 1 - .0645879732739\\naccuracy\\n0.93541\\nSee how just rerunning the model and instantiating different weights made the network\\nturn out to be different than before? This is a symptom of being a high variance model. In\\naddition, neural networks generally require many training samples in order to combat the\\nhigh variances  of the model and also require a large amount of computation power to work\\nwell in production environments.\\n\\nummary\\nThis concludes our long journey into the principles of data science. In the last 300-odd\\npages, we looked at different techniques in probability, statistics, and machine learning to\\nanswer the most difficult questions out there. I would like to personally congratulate you\\non making it through this book. I hope that it proved useful and inspired you to learn even\\nmore!\\nThis isn\\'t everything I need to know?\\nNope! There is only so much I can fit into a principles  level book. There is still so much to\\nlearn.\\nWhere can I learn more?\\nI recommend going to find open source data challenges ( https://www.kaggle.com/  is a\\ngood source) for this. I\\'d also recommend seeking out, trying, and solving your own\\nproblems at home!\\nWhen do I get to call myself a data scientist?\\nWhen you begin cultivating actionable insights from datasets, both large and small, that\\ncompanies and people can use, then you have the honor of calling yourself a true data\\nscientist.\\nIn next chapter, we will apply concepts learned in this book to real-life case studies,\\nincluding building predictive models to predict the stock market. We will also cover the\\nemerging machine learning topic of TensorFlow.\\n\\n3\\nCase Studies\\nIn this chapter, we will take a look at a few case studies to help you develop a better\\nunderstanding of the topics we\\'ve seen so far.\\nCase study 1 – Predicting stock prices\\nbased on social media\\nOur first case study will be quite  exciting! We will attempt to predict the price of the stock\\nof a publicly traded company using only social media sentiment. While this example  will\\nnot use any explicit statistical/machine learning algorithms, we will utilize exploratory data\\nanalysis  (EDA ) and use visuals in order to achieve our goal.\\nText sentiment analysis\\nWhen talking about sentiment, it should be clear  what is meant. By sentiment, I am\\nreferring to a quantitative value (at the interval level) between -1 and 1. If the sentiment\\nscore of a text piece is close to -1, it is said to have negative sentiment. If the sentiment score\\nis close to 1, then the text is said to have positive sentiment. If the sentiment score is close to\\n0, we say it has neutral sentiment. We will use a Python module called TextBlob  to\\nmeasure our text sentiment:\\nfrom textblob import TextBlob\\n# use the textblob module to make a function called stringToSentiment that\\nreturns a sentences sentiment\\ndef stringToSentiment(text):\\n    return TextBlob(text).sentiment.polarity\\n\\now, we can use this function, which calls the Textblob  module to score text out of the\\nbox:\\nstringToSentiment(\\'i hate you\\')\\n# -0.8\\nstringToSentiment(\\'i love you\\')\\n# 0.5\\nstringToSentiment(\\'i see you\\')\\n# 0.0\\nNow, let\\'s read  in our tweets  for our study :\\n# read in tweets data into a dataframe\\nfrom textblob import TextBlob\\nimport pandas as pd\\n%matplotlib inline\\n# these tweets are from last May and are about Apple (AAPL)\\ntweets = pd.read_csv(\\'../data/ so_many_tweets.csv \\')\\ntweets.head()\\nExploratory data analysis\\nSo we have four  columns, as follows:\\nText : Unstructured text at the nominal level\\nDate : Datetime (we will think of datetime in a continuous way)\\nStatus : Status unique ID at the nominal level\\nRetweet : Status ID of tweet showing that this tweet was a retweet at the nominal\\nlevel\\n\\no we have four columns, but how many rows? Also, what does each row represent? It\\nseems that each  row represents a single tweet about the company:\\ntweets.shape\\nThe output is as follows:\\n(52512, 4)\\nSo, we have four columns and 52512  tweets/rows at our disposal! Oh boy … Our goal here\\nis to eventually use the tweets\\' sentiments, so we will likely need a sentiment column in the\\nDataFrame. Using our fairly straightforward function from the previous example, let\\'s add\\nthis column!\\n# create a new column in tweets called sentiment that maps\\nstringToSentiment to the text column\\ntweets[\\'sentiment\\'] = tweets[\\'Text\\'].apply(stringToSentiment)\\ntweets.head()\\nThe preceding code will apply the stringToSentiment  function to each and every\\nelement in the Text  column of the tweets  DataFrame :\\ntweets.head()\\nSo, now we have a sense for the sentiment score for each tweet in this dataset. Let\\'s simplify\\nour problem and try to use an entire days\\' worth of tweets to predict whether or not the\\nprice of AAPL will increase within 24 hours. If this is the case, we have another issue here.\\nThe Date  column reveals that we have multiple tweets for each day. Just look at the first\\nfive tweets; they are all on the same day. We will resample this dataset in order to get a\\nsense of the average  sentiment of the stock on Twitter every day.\\n\\ne will do this in three steps:\\nWe will ensure that the Date  column is of the Python datetime  type. 1.\\nWe will replace our DataFrame\\'s index with the datetime  column (which allows 2.\\nus to use complex datetime  functions).\\nWe will resample the data so that each row, instead of representing a tweet, will3.\\nrepresent a single day with an aggregated sentiment score for each day:\\nThe index of the DataFrame is a special series used to identify rows in our\\nstructure. By default, a DataFrame will use incremental integers to\\nrepresent rows ( 0 for the first row, 1 for the second row, and so on).\\nimport pandas as pd\\ntweets.index = pd.RangeIndex(start=0, stop=52512, step=1)\\n# As a list, we can splice it\\nlist(tweets.index)[:5]\\n[0, 1, 2, 3, 4]\\nLet\\'s tackle this date  issue now! We will ensure  that the Date  column is of the 4.\\nPython datetime  type:\\n# cast the date column as a datetime\\ntweets[\\'Date\\'] = pd.to_datetime(tweets.Date)\\ntweets[\\'Date\\'].head()\\nDate\\n2015-05-24 03:46:08   2015-05-24 03:46:08\\n2015-05-24 04:17:42   2015-05-24 04:17:42\\n2015-05-24 04:13:22   2015-05-24 04:13:22\\n2015-05-24 04:08:34   2015-05-24 04:08:34\\n2015-05-24 04:04:42   2015-05-24 04:04:42\\nName: Date, dtype: datetime64[ns]\\n\\ne will replace our DataFrame\\'s index with the datetime  column (which allows 5.\\nus to use complex datetime  functions ):\\ntweets.index = tweets.Date\\ntweets.index\\nIndex([u\\'2015-05-24 03:46:08\\', u\\'2015-05-24 04:17:42\\',\\nu\\'2015-05-24 04:13:22\\',\\n       u\\'2015-05-24 04:08:34\\', u\\'2015-05-24 04:04:42\\',\\nu\\'2015-05-24 04:00:01\\',\\n       u\\'2015-05-24 03:54:07\\', u\\'2015-05-24 04:25:29\\',\\nu\\'2015-05-24 04:24:47\\',\\n       u\\'2015-05-24 04:06:42\\',\\n       ...\\n       u\\'2015-05-02 16:30:02\\', u\\'2015-05-02 16:29:35\\',\\nu\\'2015-05-02 16:28:26\\',\\n       u\\'2015-05-02 16:27:53\\', u\\'2015-05-02 16:27:02\\',\\nu\\'2015-05-02 16:26:39\\',\\n       u\\'2015-05-02 16:25:00\\', u\\'2015-05-02 16:23:39\\',\\nu\\'2015-05-02 16:23:38\\',\\n       u\\'2015-05-02 16:23:21\\'],\\n      dtype=\\'object\\', name=u\\'Date\\', length=52512)\\ntweets.head()\\n\\n\\note that the black index on the left used to be numbers, but now is the\\nexact datetime  that the tweet was sent.\\nResample the data so that each row, instead of representing a tweet, will6.\\nrepresent a single day with an aggregated sentiment score for each day:\\n# create a dataframe called daily_tweets which resamples tweets\\nby D, averaging the columns\\ndaily_tweets = tweets[[\\'sentiment\\']].resample(\\'D\\', how=\\'mean\\')\\n# I only want the sentiment column in my new Dataframe.\\ndaily_tweets.head()\\nThat\\'s looking better! Now, each row represents a single day and the sentiment score \\ncolumn  is showing us an average  sentiment for the day. Let\\'s see how many days\\' worth of\\ntweets we have:\\ndaily_tweets.shape\\n(23, 1)\\nOK, so we went from over 50,000 tweets to only 23 days! Now, let\\'s take a look at the\\nprogression of sentiment over several days :\\n# plot the sentiment as a line graph\\ndaily_tweets.sentiment.plot(kind=\\'line\\')\\n\\nAverage daily sentiment in regard to a speciﬁc company for 23 days in May 2015\\nimport pandas as pd\\nimport pandas_datareader as pdr\\nimport datetime\\nhistorical_prices = pdr.get_data_yahoo(\\'AAPL\\',\\n                          start=datetime.datetime(2015, 5, 2),\\n                          end=datetime.datetime(2015, 5, 25))\\nprices = pd.DataFrame(historical_prices)\\nprices.head()\\n\\n\\now, two things are primarily of interest to us here:\\nWe are really only interested in the Close  column, which is the final price set for\\nthe trading day.\\nWe also need to set the index  of this DataFrame to be datetimes,  so that we can\\nmerge the sentiment and the price DataFrames together:\\nprices.info() #the columns aren\\'t numbers!\\n<class \\'pandas.core.frame.DataFrame\\'>\\nDatetimeIndex: 15 entries, 2015-05-22 to 2015-05-04\\nData columns (total 8 columns):\\nAdj_Close    15 non-null object\\nClose        15 non-null object        # NOT A NUMBER\\nDate         15 non-null object\\nHigh         15 non-null object\\nLow          15 non-null object\\nOpen         15 non-null object\\nSymbol       15 non-null object\\nVolume       15 non-null object\\ndtypes: object(8)\\nLet\\'s fix that. While we\\'re at it, let\\'s also fix Volume , which represents t he number of stocks\\ntraded on that day:\\n# cast the column as numbers\\nprices.Close= not_null_close.Close.astype(\\'float\\')\\nprices.Volume = not_null_close.Volume.astype(\\'float\\')\\nNow, let\\'s try to plot both the volume and price of AAPL in the same graph:\\n# plot both volume and close as line graphs in the same graph, what do you\\nnotice is the problem?\\nprices[[\"Volume\", \\'Close\\']].plot()\\n\\nTrade volume versus date\\nWoah, what\\'s wrong here? Well, if we look carefully, Volume  and Close  are on very\\ndifferent scales!\\nprices[[\"Volume\", \\'Close\\']].describe()\\n\\n\\nnd by a lot! The Volume  column has a mean in the tens of millions, while the average\\nclosing price is merely 125!\\nfrom sklearn.preprocessing import StandardScaler\\n# scale the columns by z scores using StandardScaler\\n# Then plot the scaled data\\ns = StandardScaler()\\nonly_prices_and_volumes = prices[[\"Volume\", \\'Close\\']]\\nprice_volume_scaled = s.fit_transform(only_prices_and_volumes)\\npd.DataFrame(price_volume_scaled, columns=[\"Volume\", \\'Close\\']).plot()\\nCorrelation of volume and closing prices \\nThat looks much better! You can see how, as the price of AAPL went down somewhere in\\nthe middle, the volume of trading also went up! This is actually fairly common:\\n# concatinate prices.Close, and daily_tweets.sentiment\\nmerged = pd.concat([prices.Close, daily_tweets.sentiment], axis=1)\\nmerged.head()\\n\\n\\nmm, why are there some null Close  values? Well, if you look up May 2, 2015 on a\\ncalendar, you will see that it is a Saturday and the markets are closed on Saturdays,\\nmeaning there cannot be a closing price! So, we need to make a decision on whether or not\\nto remove these rows because we still have sentiment for that day. Eventually, we will be\\nattempting to predict the next day\\'s closing price and whether the price increased or not, so\\nlet\\'s go ahead and remove any null values in our dataset:\\n# Delete any rows with missing values in any column\\nmerged.dropna(inplace=True)\\nNow, let\\'s attempt to graph our plot:\\nmerged.plot()\\n# wow that looks awful\\nClosing price/sentiment score versus date\\n\\now, that\\'s terrible. Once again, we must scale our features in order to gain any valuable\\ninsight:\\n# scale the columns by z scores using StandardScaler\\nfrom sklearn.preprocessing import StandardScaler\\ns = StandardScaler()\\nmerged_scaled = s.fit_transform(merged)\\npd.DataFrame(merged_scaled, columns=merged.columns).plot()\\n# notice how sentiment seems to follow the closing price\\nClosing price versus sentiment score \\nMuch better! You can start to see how the closing price of the stock actually does seem to\\nmove with our sentiment. Let\\'s take this one step further and attempt to apply a supervised\\nlearning model. For this to work, we need to define our features and our response. Recall\\nthat our response is the value that we wish to predict, and our features are values that we\\nwill use to predict the response.\\nIf we look at each row of our data, we have a sentiment and closing price for that day.\\nHowever, we wish to use today\\'s sentiment to predict tomorrow\\'s stock price and whether\\nit increased or not. Think about it; it would be kind of cheating because today\\'s sentiment\\nwill include tweets from after the closing price was finalized. To simplify this, we will\\nignore any tweet as a feature for the prediction of today\\'s price.\\n\\no, for each row, our response should be today\\'s closing price, while our feature should be\\nyesterday\\'s sentiment of the stock. To do this, I will use a built-in function in Pandas called\\nshift  to shift our sentiment column one item backward:\\n# Shift the sentiment column backwards one item\\nmerged[\\'yesterday_sentiment\\'] = merged[\\'sentiment\\'].shift(1)\\nmerged.head()\\nDataframe with yesterday\\'s sentiment included\\nAh good, so now, for each  day we have  our true feature, which is yesterday_sentiment .\\nNote that in our heads (first five rows), we have a new null value! This is because, on the\\nfirst day, we don\\'t have a value from yesterday, so we will have to remove it. But before we\\ndo, let\\'s define our response column.\\nWe have two options:\\nKeep our response quantitative and use a regression analysis\\nConvert our response to a qualitative state and use classification\\nWhich route to choose is up to the data scientist and depends on the situation. If you\\nmerely wish to associate sentiment with a movement in price, then I recommend using the\\nclassification route. If you wish to associate sentiment with the amount of movement, I\\nrecommend a regression. I will do both!\\n\\negression route\\nWe are already good to go on this front! We have our response and our single feature. We\\nwill first have to remove  that one null value  before continuing:\\n# Make a new dataframe for our regression and drop the null values\\nregression_df = merged[[\\'yesterday_sentiment\\', \\'Close\\']]\\nregression_df.dropna(inplace=True)\\nregression_df.head()\\nLet\\'s use both a random forest  and a linear regression and see which performs better, using\\nroot-mean-square error  (RMSE ) as our metric:\\n# Imports for our regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.cross_validation import cross_val_score\\nimport numpy as np\\nWe will use a cross-validated RMSE in order to compare our two models:\\n# Our RMSE as a result of cross validation linear regression\\nlinreg = LinearRegression()\\nrmse_cv = np.sqrt(abs(cross_val_score(linreg,\\nregression_df[[\\'yesterday_sentiment\\']], regression_df[\\'Close\\'], cv=3,\\nscoring=\\'mean_squared_error\\').mean()))\\nrmse_cv\\n3.49837\\n# Our RMSE as a result of cross validation random forest\\nrf = RandomForestRegressor()\\n\\nmse_cv = np.sqrt(abs(cross_val_score(rf,\\nregression_df[[\\'yesterday_sentiment\\']], regression_df[\\'Close\\'], cv=3,\\nscoring=\\'mean_squared_error\\').mean()))\\nrmse_cv\\n3.30603\\nLook at our RMSE; it\\'s about 3.5 for both models, meaning that on average, our model is off\\nby about 3.5 dollars, which is actually a big deal considering our stock price likely doesn\\'t\\nmove that much:\\nregression_df[\\'Close\\'].describe()\\ncount     14.000000\\nmean     128.132858\\nstd        2.471810    # Our standard deviation is less than our RMSE (bad\\nsign)\\nmin      125.010002\\n25%      125.905003\\n50%      128.195003\\n75%      130.067505\\nmax      132.539993\\nAnother way to test the validity of our model is by comparing our RMSE to the null\\nmodel\\'s RMSE. The null model for a regression model is predicting the average value for\\neach value:\\n# null model for regression\\nmean_close = regression_df[\\'Close\\'].mean()\\npreds = [mean_close]*regression_df.shape[0]\\npreds\\nfrom sklearn.metrics import mean_squared_error\\nnull_rmse = np.sqrt(mean_squared_error(preds, regression_df[\\'Close\\']))\\nnull_rmse\\n2.381895\\nBecause our model did not beat the null model, perhaps regression isn\\'t the best way to go.\\n\\nlassification route\\nFor classification, we have a bit more  work to do because  we don\\'t have a categorical\\nresponse yet. To make one, we need to transform the closing column into some categorical\\noption. I will choose to make the following response. I will make a new column called\\nchange_close_big_deal , defined as follows:\\nSo, our response will be 1 if our response changed significantly, and 0 if the change in stock\\nwas negligible:\\n# Imports for our classification\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.cross_validation import cross_val_score\\nimport numpy as np\\n# Make a new dataframe for our classification and drop the null values\\nclassification_df = merged[[\\'yesterday_sentiment\\', \\'Close\\']]\\n# variable to represent yesterday\\'s closing price\\nclassification_df[\\'yesterday_close\\'] = classification_df[\\'Close\\'].shift(1)\\n# column that represents the precent change in price since yesterday\\nclassification_df[\\'percent_change_in_price\\'] = (classification_df[\\'Close\\']-\\nclassification_df[\\'yesterday_close\\']) /\\nclassification_df[\\'yesterday_close\\']\\n# drop any null values\\nclassification_df.dropna(inplace=True)\\nclassification_df.head()\\n# Our new classification response\\nclassification_df[\\'change_close_big_deal\\'] =\\nabs(classification_df[\\'percent_change_in_price\\'] ) > .01\\nclassification_df.head()\\n\\nOur DataFrame with a new column called change_close_big_deal  is either True  or\\nFalse .\\nLet\\'s now perform the same cross-validation as we did with our regression, but this time,\\nwe will be using the accuracy  feature of our cross-validation module and, instead of a \\nregression  module, we will be using  two classification machine learning algorithms:\\n# Our accuracy as a result of cross validation random forest\\nrf = RandomForestClassifier()\\naccuracy_cv = cross_val_score(rf,\\nclassification_df[[\\'yesterday_sentiment\\']],\\nclassification_df[\\'change_close_big_deal\\'], cv=3,\\nscoring=\\'accuracy\\').mean()\\naccuracy_cv\\n0.1777777\\nGosh! Not so good, so let\\'s try logistic regression instead:\\n# Our accuracy as a result of cross validation logistic regression\\nlogreg = LogisticRegression()\\naccuracy_cv = cross_val_score(logreg,\\nclassification_df[[\\'yesterday_sentiment\\']],\\nclassification_df[\\'change_close_big_deal\\'], cv=3,\\nscoring=\\'accuracy\\').mean()\\naccuracy_cv\\n0.5888\\n\\netter! But, of course, we should check  it with our null model\\'s accuracy:\\n# null model for classification\\nnull_accuracy = 1 - classification_df[\\'change_close_big_deal\\'].mean()\\nnull_accuracy\\n0.5833333\\nWhoa, our model can beat the null accuracy,  meaning that our machine learning algorithm\\ncan predict the movement of a stock price using social media sentiment better than just\\nrandomly guessing!\\nGoing beyond with this example\\nThere are many ways that we could have  enhanced this example to make a more robust\\nprediction. We could have included more features, including a moving average of\\nsentiment, instead of looking simply at the previous day\\'s sentiment. We could have also\\nbrought in more examples to enhance our idea of sentiment. We could have looked at\\nFacebook, the media, and so on, for more information on how we believe the stock will\\nperform in the future.\\nWe really only had 14 data points, which is far from sufficient to make a production-ready\\nalgorithm. Of course, for the purposes of this book, this is enough, but if we are serious\\nabout making a financial algorithm that can effectively predict stock price movement, we\\nwill have to obtain many more days of media coverage and prices.\\nWe could have spent more time optimizing our parameters in our models by utilizing the\\ngridsearchCV  module in the sklearn  package to get the most out of our models. There\\nare other models  that exist that deal specifically with time series data (data that changes\\nover time), including a model called AutoRegressive Integrated Moving Average\\n(ARIMA ). Models such as ARIMA and similar ones attempt to focus and zero in on specific\\ntime series features.\\n\\nase study 2 – Why do some people cheat\\non their spouses?\\nIn 1978, a survey was conducted  on housewives in order to discern factors that led them to\\npursue extramarital affairs. This study became the basis for many future studies of both\\nmen and women, all attempting to focus on features of people and marriages that led either\\npartner to seek partners elsewhere behind their spouse\\'s back.\\nSupervised learning is not always about prediction. In this case study, we will purely\\nattempt to identify a few factors of the many that we believe might be the most important\\nfactors that lead someone to pursue an affair.\\nFirst, let\\'s read in the data:\\n# Using dataset of a 1978 survey conducted to measure likliehood of women\\nto perform extramarital affairs\\n# http://statsmodels.sourceforge.net/stable/datasets/generated/fair.html\\nimport statsmodels.api as sm\\naffairs_df = sm.datasets.fair.load_pandas().data\\naffairs_df.head()\\nThe statsmodels  website provides a data dictionary, as follows:\\nrate_marriage : The rating given to the marriage (given by the wife); 1 = very\\npoor, 2 = poor , 3 = fair , 4 = good , 5 = very good ; ordinal level\\nage: Age of the wife; ratio level\\nyrs_married : Number of years married: ratio level\\n\\nhildren : Number of children between husband and wife: ratio level\\nreligious : How religious the wife is; 1 = not , 2 = mildly , 3 = fairly , 4 = strongly ;\\nordinal level\\neduc : Level of education; 9 = grade school , 12 = high school , 14 = some college , 16 =\\ncollege graduate , 17 = some graduate school , 20 = advanced degree ; ratio level\\noccupation : 1 = student;  2 = farming, agriculture; semi-skilled, or unskilled worker ; 3\\n= white-collar ; 4 = teacher, counselor, social worker, nurse; artist, writer; technician,\\nskilled worker;  5 = managerial, administrative, business;  6 = professional with advanced\\ndegree ; nominal level\\noccupation_husb : Husband\\'s occupation. Same as occupation; nominal level\\naffairs : Measure of time spent in extramarital affairs; ratio level\\nOkay, so we have a quantitative response, but my question is simply what factors cause\\nsomeone to have an affair. The exact number of minutes or hours does not really matter\\nthat much. For this reason, let\\'s make a new categorical variable called affair_binary ,\\nwhich is either true  (they had an affair for more than 0 minutes) or false  (they had an\\naffair for 0 minutes):\\n# Create a categorical variable\\naffairs_df[\\'affair_binary\\'] = (affairs_df[\\'affairs\\'] > 0)\\nAgain, this column has either a true  or a false  value. The value is true  if the person had\\nan extramarital affair for more than 0 minutes. The value is false  otherwise. From now on,\\nlet\\'s use this binary  response as our primary response. Now, we are trying to find which of\\nthese variables are associated with our response, so let\\'s begin.\\nLet\\'s start with a simple correlation matrix. Recall that this matrix shows us linear\\ncorrelations between our quantitative variables and our response. I will show the\\ncorrelation matrix as both a matrix of decimals and also as a heat map. Let\\'s see the\\nnumbers first:\\n# find linear correlations between variables and affair_binary\\naffairs_df.corr()\\n\\nCorrelation matrix for extramarital aﬀairs data from a Likert survey conducted in 1978\\nRemember that we ignore the diagonal series of 1s because they are merely telling us that\\nevery quantitative variable is correlated with itself. Note the other correlated variables,\\nwhich are the values closest to 1 and -1 in the last row or column (the matrix is always\\nsymmetrical across the diagonal).\\nWe can see a few standout variables:\\naffairs\\nage\\nyrs_married\\nchildren\\nThese are the top four variables with the largest magnitude (absolute value). However, one\\nof these variables is cheating . The affairs  variable is the largest in magnitude, but is\\nobviously correlated to affair_binary  because we made the affair_binary\\nvariable directly based on affairs. So let\\'s ignore that one.\\n\\net\\'s take a look at our correlation heat map to see whether our views can be seen there:\\nimport seaborn as sns\\nsns.heatmap(affairs_df.corr())\\nCorrelation matrix\\nThe same correlation matrix, but this time  as a heat map. Note the colors close to dark red\\nand dark blue (excluding the diagonal).\\nWe are looking for the dark red and dark blue areas of the heat map. These colors are\\nassociated with the most correlated features.\\nRemember correlations are not the only way to identify which features are associated with\\nour response. This method shows us how linearly correlated the variables are with each\\nother. We may find another variable that affects affairs by evaluating the coefficients of a\\ndecision tree classifier. These methods might reveal new variables that are associated with\\nour variables, but not in a linear fashion.\\n\\nlso notice that there are two variables here that don\\'t actually belong …\\nCan you spot them? These are the occupation  and occupation_husb\\nvariables. Recall earlier that we deemed them as nominal and therefore\\nhave no right to be included in this correlation matrix. This is because\\nPandas, unknowingly, casts them as integers and now considers them as\\nquantitative variables. Don\\'t worry, we will fix this soon.\\nFirst, let\\'s make ourselves an X and a y DataFrame:\\naffairs_X = affairs_df.drop([\\'affairs\\', \\'affair_binary\\'], axis=1)\\n# data without the affairs or affair_binary column\\naffairs_y = affairs_df[\\'affair_binary\\']\\nNow, we will instantiate a decision tree classifier and cross-validate our model in order to\\ndetermine whether or not the model is doing an okay job at fitting our data:\\nfrom sklearn.tree import DecisionTreeClassifier\\nmodel = DecisionTreeClassifier()\\n# instantiate the model\\nfrom sklearn.cross_validation import cross_val_score\\n# import our cross validation module\\n# check the accuracy on the training set\\nscores = cross_val_score(model, affairs_X, affairs_y, cv=10)\\nprint( scores.mean(), \"average accuracy\" )\\n0.659756806845 average accuracy\\nprint( scores.std(), \"standard deviation\") # very low, meaning variance of\\nthe model is low\\n0.0204081732291 standard deviation\\n# Looks ok on the cross validation side\\nBecause our standard deviation is low, we may make  the assumption that the variance of\\nour model is low (because variance is the square of standard deviation). This is good\\nbecause that means that our model is not fitting wildly differently on each fold of the cross\\nvalidation and that it is generally a reliable model.\\n\\necause we agree that our decision tree classifier is generally a reliable model, we can fit the\\ntree to our entire dataset and use the importance metric to identify which variables our tree\\ndeems the most important:\\n# Explore individual features that make the biggest impact\\n# rate_marriage, yrs_married, and occupation_husb. But one of these\\nvariables doesn\\'t quite make sense right?\\n# Its the occupation variable, because they are nominal, their\\ninterpretations\\nmodel.fit(affairs_X, affairs_y)\\npd.DataFrame({\\'feature\\':affairs_X.columns,\\n\\'importance\\':model.feature_importances_}).sort_values(\\'importance\\').tail(3)\\nSo, yrs_married  and rate_marriage  both are important, but the most important\\nvariable is occupation_husb . But that doesn\\'t make sense because that variable is\\nnominal! So, let\\'s apply our dummy variable technique wherein we create new columns\\nthat represent each option for occupation_husb  and also for occupation .\\nFirstly, for the occupation  column:\\n# Dummy Variables:\\n# Encoding qualitiative (nominal) data using separate columns (see slides\\nfor linear regression for more)\\noccuptation_dummies = pd.get_dummies(affairs_df[\\'occupation\\'],\\nprefix=\\'occ_\\').iloc[:, 1:]\\n# concatenate the dummy variable columns onto the original DataFrame\\n(axis=0 means rows, axis=1 means columns)\\naffairs_df = pd.concat([affairs_df, occuptation_dummies], axis=1)\\naffairs_df.head()\\n\\nhis new DataFrame has many new columns:\\nRemember, these new columns, occ_2.0 , occ_4.0 , and so on, represent a binary variable\\nthat represents whether  or not the wife holds job 2, or 4, and so on:\\n# Now for the husband\\'s job\\noccuptation_dummies = pd.get_dummies(affairs_df[\\'occupation_husb\\'],\\nprefix=\\'occ_husb_\\').iloc[:, 1:]\\n# concatenate the dummy variable columns onto the original DataFrame\\n(axis=0 means rows, axis=1 means columns)\\naffairs_df = pd.concat([affairs_df, occuptation_dummies], axis=1)\\naffairs_df.head()\\n(6366, 15)\\nNow we have 15 new columns! Let\\'s run our tree again and find the most important\\nvariables:\\n# remove appropiate columns for feature set affairs_X =\\naffairs_df.drop([\\'affairs\\', \\'affair_binary\\', \\'occupation\\',\\n\\'occupation_husb\\'], axis=1) affairs_y = affairs_df[\\'affair_binary\\'] model =\\nDecisionTreeClassifier() from sklearn.cross_validation import\\ncross_val_score # check the accuracy on the training set scores =\\ncross_val_score(model, affairs_X, affairs_y, cv=10) print scores.mean(),\\n\"average accuracy\"\\nprint (scores.std(), \"standard deviation\") # very low, meaning variance of\\nthe model is low # Still looks ok # Explore individual features that make\\nthe biggest impact model.fit(affairs_X, affairs_y)\\n\\nd.DataFrame({\\'feature\\':affairs_X.columns,\\n\\'importance\\':model.feature_importances_}).sort_values(\\'importance\\').tail(10\\n)\\nAnd there you have it:\\nrate_marriage : The rating of the marriage, as told by the decision tree\\nchildren : The number of children they had, as told by the decision tree and our\\ncorrelation matrix\\nyrs_married : The number of years they  had been married, as told by the\\ndecision tree and our correlation matrix\\neduc : The level of education the women had, as told by the decision tree\\nage: The age of the women, as told by the decision tree and our correlation\\nmatrix\\nThese seem to be the top five most important variables in determining whether or not a\\nwoman from the 1978 survey would be involved in an extramarital affair.\\nCase study 3 – Using TensorFlow\\nI would like to finish off our time  together by looking at a somewhat more modern machine\\nlearning module called TensorFlow.\\n\\nensorFlow is an open source machine learning module that is used primarily for its\\nsimplified deep learning and neural network abilities. I would like to take some time  to\\nintroduce the module and solve a few quick problems using TensorFlow. The syntax for\\nTensorFlow (like PyBrain in Chapter 12 , Beyond the Essentials ) is a bit different than our\\nnormal scikit-learn  syntax, so I will be going over it step by step. Let\\'s start with some\\nimports:\\nfrom sklearn import datasets, metrics\\nimport tensorflow as tf\\nimport numpy as np\\nfrom sklearn .cross_validation import train_test_split\\n%matplotlib inline\\nOur imports from sklearn  include train_test_split , datasets , and metrics . We will\\nbe utilizing our train test splits to reduce overfitting, we will use datasets in order to import\\nour iris  classification data, and we\\'ll use the metrics module in order to calculate some\\nsimple metrics for our learning models.\\nTensorFlow learns in a different way in that it is always trying to minimize an error\\nfunction. It does this by iteratively going through our entire dataset and, every so often,\\nupdates our model to better fit the data.\\nIt is important to note that TensorFlow doesn\\'t just implement neural networks, but can\\nimplement even simpler models as well. For example, let\\'s implement a classic logistic\\nregression using TensorFlow:\\n# Our data set of iris flowers\\niris = datasets.load_iris()\\n# Load datasets and split them for training and testing\\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)\\n####### TENSORFLOW #######\\n# Here is tensorflow\\'s syntax for defining features.\\n# We must specify that all features have real-value data\\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\\n# notice the dimension is set to four because we have four columns\\n# We set our \"learning rate\" which is a decimal that tells the network\\n# how quickly to learn\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\\n# A learning rate closer to 0 means the network will learn slower\\n\\n Build a linear classifier (logistic regression)\\n# note we have to tell tensorflow the number of classes we are looking for\\n# which are 3 classes of iris\\nclassifier =\\ntf.contrib.learn.LinearClassifier(feature_columns=feature_columns,\\n                                             optimizer=optimizer,\\n                                                      n_classes=3)\\n# Fit model. Uses error optimization techniques like stochastic gradient\\ndescent\\nclassifier.fit(x=X_train,\\n               y=y_train,\\n               steps=1000)  # number of iterations\\nI will point out the key lines of code from the preceding snippet to really solidify what is\\nhappening during training:\\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\",1.\\ndimension=4)] :\\nHere, I am creating four input  columns that we know correlate to the\\nflowers\\' sepal length, sepal width, petal length, and petal width.\\noptimizer =2.\\ntf.train.GradientDescentOptimizer(learning_rate=.1) :\\nHere, I am telling TensorFlow to optimize using something called\\ngradient descent , which means that we will define an error function\\n(which will happen in the next step) and, little by little, we will work\\nour way to minimize this error function.\\nOur learning rate should hover close to 0 because we want our model\\nto learn slowly. If our model learns too quickly, it might \"skip over\" the\\ncorrect answer!\\nclassifier =3.\\ntf.contrib.learn.LinearClassifier(feature_columns=feature_colum\\nns, optimizer=optimizer, n_classes=3) :\\nWhen we specify LinearClassifier , we are denoting the same error\\nfunction that logistic regression is minimizing, meaning that this\\nclassifier is attempting to work as a logistic regression classifier.\\nWe give the model our feature_columns  as defined in Step 1 .\\nThe optimizer  is the method of minimizing our error function; in this\\ncase, we chose gradient descent.\\nWe must also specify our number of classes as being 3. We know that\\nwe have three different iris flowers that the model could choose from.\\n\\nlassifier.fit(x=X_train, y=y_train, steps=1000) : 4.\\nThe training looks similar to a scikit-learn model, with an added\\nparameter called steps . Steps tell us how many times we would like\\nto go over our dataset. So, when we specify 1000 , we are iterating over\\nour dataset. The more steps we take, the more the model gets a chance\\nto learn.\\nPhew! When we run the preceding  code, a linear classifier (logistic regression) model is\\nbeing fit, and when it is done, it is ready to be tested:\\n# Evaluate accuracy.\\naccuracy_score = classifier.evaluate(x=X_test,\\n                                     y=y_test)[\"accuracy\"]\\nprint(\\'Accuracy: {0:f}\\'.format(accuracy_score))\\nAccuracy: 0.973684\\nExcellent! It is worth noting that when using TensorFlow, we may also utilize a similarly\\nsimple predict  function:\\n# Classify two new flower samples.\\nnew_samples = np.array(\\n    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\\ny = classifier.predict(new_samples)\\nprint(\\'Predictions: {}\\'.format(str(y)))\\nPredictions: [1 2]\\nNow, let\\'s compare this with a standard scikit-learn logistic regression to see who won:\\nfrom sklearn.linear_model import LogisticRegression\\n# compare our result above to a simple scikit-learn logistic regression\\nlogreg = LogisticRegression()\\n# instantiate the model\\nlogreg.fit(X_train, y_train)\\n# fit it to our training set\\ny_predicted = logreg.predict(X_test)\\n# predict on our test set, to avoid overfitting!\\naccuracy = metrics.accuracy_score(y_predicted, y_test)\\n# get our accuracy score\\n\\nccuracy\\n# It\\'s the same thing!\\nWow, so it seems that with 1,000 steps, a gradient descent-optimized TensorFlow model is\\nno better than a simple sklearn logistic regression. OK, that\\'s fine, but what if we allowed\\nthe model to iterate over the iris  dataset even more?\\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\\nclassifier =\\ntf.contrib.learn.LinearClassifier(feature_columns=feature_columns,\\n                                               optimizer=optimizer,\\n                                            n_classes=3)\\nclassifier.fit(x=X_train,\\n               y=y_train,\\n               steps=2000)  # number of iterations is 2000 now\\nOur code is exactly the same as before, but now we have 2000  steps instead of 1000 :\\n# Evaluate accuracy.\\naccuracy_score = classifier.evaluate(x=X_test,\\n                                     y=y_test)[\"accuracy\"]\\nprint(\\'Accuracy: {0:f}\\'.format(accuracy_score))\\nAccuracy: 0.973684\\nAnd now we have even better accuracy!\\nNote that you need to be very careful in choosing the number of steps. As\\nyou increase this number, you increase the number of times your model\\nsees the exact same training points over and over again. We do have a\\nchance of overfitting! To remedy this, I would recommend choosing\\nmultiple train/test splits and running the model on each one ( k-fold cross\\nvalidation ).\\nIt is also worth mentioning that TensorFlow implements very low bias, high-variance\\nmodels, meaning that running  the preceding code again for TensorFlow might result in a\\ndifferent answer! This is one of the caveats of deep learning. They might converge to a very\\ngreat low bias model, but that model will have a high variance and, therefore, amazingly\\nmight not generalize to all of the sample data. As mentioned before, cross validation would\\nbe helpful in order to mitigate this.\\n\\nensorFlow and neural networks\\nNow, let\\'s point a more powerful model  at our iris  dataset. Let\\'s create a neural network \\nwhose  goal it is to classify iris flowers (because why not?):\\n# Specify that all features have real-value data\\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\\n# Build 3 layer DNN with 10, 20, 10 units respectively.\\nclassifier =\\ntf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\\n                                        hidden_units=[10, 20, 10],\\n                                        optimizer=optimizer,\\n                                        n_classes=3)\\n# Fit model.\\nclassifier.fit(x=X_train,\\n               y=y_train,\\n               steps=2000)\\nNotice that our code  really hasn\\'t changed  from the last segment. We still have our\\nfeature_columns  from before, but now we introduce, instead of a linear classifier, a\\nDNNClassifier , which stands for Deep Neural Network Classifier .\\nThis is TensorFlow\\'s syntax for implementing a neural network. Let\\'s take a closer look:\\ntf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\\n                                     hidden_units=[10, 20, 10],\\n                                     optimizer=optimizer,\\n                                     n_classes=3)\\nWe see that we are inputting the same feature_columns , n_classes , and optimizer ,\\nbut see how we have a new parameter called hidden_units ? This list represents the\\nnumber of nodes to have in each layer between the input and the output layer.\\n\\nll in all, this neural network will have five layers:\\nThe first layer will have four nodes, one for each of the iris feature variables, and\\nthis layer is the input layer\\nA hidden layer of 10 nodes\\nA hidden layer of 20 nodes\\nA hidden layer of 10 nodes\\nThe final layer will have three nodes, one for each possible outcome of the\\nnetwork, and this is called our output layer\\nNow that we\\'ve trained  our model, let\\'s evaluate  it on our test set:\\n# Evaluate accuracy.\\naccuracy_score = classifier.evaluate(x=X_test,\\n                                     y=y_test)[\"accuracy\"]\\nprint(\\'Accuracy: {0:f}\\'.format(accuracy_score))\\nAccuracy: 0.921053\\nHmm, our neural network didn\\'t do so well on this dataset, but perhaps it is because the\\nnetwork is a bit too complicated for such a simple dataset. Let\\'s introduce a new dataset\\nthat has a bit more to it...\\nThe mnist  dataset consists of over 50,000 handwritten digits (0-9) and the goal is to\\nrecognize the handwritten digits and output which letter they are writing. TensorFlow has\\na built-in mechanism for downloading and loading these images. We\\'ve seen these images\\nbefore, but on a much smaller scale in Chapter 12 , Beyond the Essentials :\\nfrom tensorflow.examples.tutorials.mnist import input_data\\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\\nExtracting MNIST_data/train-images-idx3-ubyte.gz\\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\\nNotice that one of our inputs for downloading mnist  is called one_hot . This parameter\\neither brings in the dataset\\'s target variable (which is the digit itself) as a single number, or\\nhas a dummy variable.\\n\\nor example, if the first digit were a 7, the target would be either of these:\\n7: If one_hot  was false\\n0 0 0 0 0 0 0 1 0 0 : If one_hot  was true  (notice that starting from 0, the seventh\\nindex is a 1)\\nWe will encode our target the former way, as this is what our TensorFlow neural network\\nand our sklearn logistic regression will expect.\\nThe dataset is split up already into a training and test set, so let\\'s create new variables to\\nhold them:\\nx_mnist = mnist.train.images\\ny_mnist = mnist.train.labels.astype(int)\\nFor the y_mnist  variable, I specifically cast every target as an integer (by default, they\\ncome in as floats) because otherwise, TensorFlow would throw an error at us.\\nOut of curiosity, let\\'s take a look  at a single image:\\nimport matplotlib.pyplot as plt\\nplt.imshow(x_mnist[10].reshape(28, 28))\\nThe number 0 in the MNIST dataset\\n\\nnd, hopefully, our target variable matches at the 10th index as well:\\ny_mnist[10]\\n0\\nExcellent! Let\\'s now take a peek at how big our dataset is:\\nx_mnist.shape\\n(55000, 784)\\ny_mnist.shape\\n(55000,)\\nOur training size then is 55000  images and target variables.\\nLet\\'s fit a deep neural network to our images and see whether it will be able to pick up on\\nthe patterns in our inputs:\\n# Specify that all features have real-value data feature_columns =\\n[tf.contrib.layers.real_valued_column(\"\", dimension=784)] optimizer =\\ntf.train.GradientDescentOptimizer(learning_rate=.1) # Build 3 layer DNN\\nwith 10, 20, 10 units respectively. classifier =\\ntf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\\nhidden_units=[10, 20, 10], optimizer=optimizer, n_classes=10) # Fit model.\\nclassifier.fit(x=x_mnist, y=y_mnist, steps=1000) # Warning this is\\nveryyyyyyyy slow\\nThis code is very similar to our previous  segment using DNNClassifier ; however, look\\nhow, in our first line of code, I have changed the number of columns to be 784 while, in the\\nclassifier itself, I changed the number of output classes to be 10. These are manual  inputs\\nthat TensorFlow must be given to work.\\nThe preceding code runs very slowly. It is adjusting itself, little by little,  in order to get the\\nbest possible performance from our training set. Of course, we know that the ultimate test\\nhere is testing our network on an unknown test set, which is also given to us by\\nTensorFlow:\\nx_mnist_test = mnist.test.images\\ny_mnist_test = mnist.test.labels.astype(int)\\nx_mnist_test.shape\\n(10000, 784)\\ny_mnist_test.shape\\n(10000,)\\n\\no we have 10,000 images to test on; let\\'s see how our network was able to adapt to the\\ndataset:\\n# Evaluate accuracy.\\naccuracy_score = classifier.evaluate(x=x_mnist_test,\\n                                     y=y_mnist_test)[\"accuracy\"]\\nprint(\\'Accuracy: {0:f}\\'.format(accuracy_score))\\nAccuracy: 0.920600\\nNot bad, 92% accuracy on our dataset. Let\\'s take a second and compare this\\nperformance to a standard sklearn logistic regression now:\\nlogreg = LogisticRegression()\\nlogreg.fit(x_mnist, y_mnist)\\n# Warning this is slow\\ny_predicted = logreg.predict(x_mnist_test)\\nfrom sklearn.metrics import accuracy_score\\n# predict on our test set, to avoid overfitting!\\naccuracy = accuracy_score(y_predicted, y_mnist_test)\\n# get our accuracy score\\naccuracy\\n0.91969\\nSuccess! Our neural network performed better  than the standard logistic regression. This is\\nlikely because the network is attempting to find relationships between the pixels\\nthemselves and using these relationships to map them to what digit we are writing down.\\nIn logistic regression, the model assumes that every single input is independent of one\\nanother, and therefore has a tough time finding relationships between them.\\n\\nhere are ways of making our neural network learn differently:\\nWe could make our network wider, that is, increase the number of nodes in the\\nhidden layers instead of having several layers of a smaller number of nodes:\\nArtiﬁcial neural network (Source: http://electronicdesign.com/site-ﬁles/electronicdesign.com/ﬁles/uploads/2015/02/0816_Development_Tools_F1_0.gif)\\n# A wider network\\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\",\\ndimension=784)]\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\\n# Build 3 layer DNN with 10, 20, 10 units respectively.\\nclassifier =\\ntf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\\n                                     hidden_units=[1500],\\n                                     optimizer=optimizer,\\n                                            n_classes=10)\\n# Fit model.\\nclassifier.fit(x=x_mnist,\\n               y=y_mnist,\\n               steps=100)\\n# Warning this is veryyyyyyyy slow\\n# Evaluate accuracy.\\naccuracy_score = classifier.evaluate(x=x_mnist_test,\\n                                     y=y_mnist_test)[\"accuracy\"]\\nprint(\\'Accuracy: {0:f}\\'.format(accuracy_score))\\n      Accuracy: 0.898400\\n\\ne could increase our learning rate, forcing the network to attempt to converge\\non an answer faster. As I mentioned before, we run the risk of the model\\nskipping the answer entirely if we go down this route. It is usually better  to stick\\nwith a smaller learning rate.\\nWe can change the method  of optimization. Gradient descent is very popular;\\nhowever, there are other algorithms for doing this. One example is called the\\nAdam Optimizer . The difference is in the way they traverse the error function,\\nand therefore the way that they approach the optimization point. Different\\nproblems in different domains call for different optimizers.\\nThere is no replacement for a good old fashioned feature selection phase, instead\\nof attempting to let the network figure everything out for us. We can take the\\ntime to find relevant and meaningful features that actually will allow our\\nnetwork to find an answer quicker!\\nSummary\\nIn this chapter, we\\'ve seen three different case studies from three different domains using\\nmany different statistical and machine learning methods. However, what all of them have\\nin common is that in order to solve them properly, we had to implement a data science\\nmindset. We had to solve problems in an interesting way, obtain data, clean the data,\\nvisualize the data, and finally model the data and evaluate our thinking process.\\nI do hope that you have found the contents of this book to be interesting and not just the\\nfinal chapter! I leave it unto you to keep exploring the world of data science. Keep learning\\nPython. Keep learning statistics and probability. Keep your minds open. It is my hope that\\nthis book has been a catalyst for you to go out and find even more on the subject.\\nFor further reading beyond this book, I highly recommend looking into well-known data\\nscience books and blogs, such as:\\nDataschool.io, a blog by Kevin Markham\\nPython for Data Scientists,  by Packt\\nIf you would like to contact me for any reason, please feel free to reach out to\\nsinan.u.ozdemir@gmail.com .\\n\\n4\\nMicrosoft Azure Databricks\\nThe main purpose of this chapter is to highlight the Microsoft Data Environment and how\\nwe can utilize the many tools provided to us, especially Azure Databricks: a fullyfledged,\\npowerful analytics platform powered by Apache Spark.\\nWe have three main case studies in this chapter that join together the principles of data\\nscience and machine learning that we have learned about in this book with the ease and\\npower of the Microsoft Data Environment, specifically Azure Databricks. Each case study\\nwill highlight different features of using Azure Databricks, as well as aspects of machine\\nlearning that we have learned in this text.\\nThe Microsoft data science environment\\nMicrosoft offers many tools and environments to the data scientist, and this offering\\nconsists of many parts. The three main components are as follows:\\nMicrosoft Azure : This is an  enterprise-grade cloud computing platform that\\nprovides a great  deal of access and support for production-ready scalable\\nsystems.\\nMicrosoft Azure Machine Learning Studio : This is a GUI-based environment for\\ncreating  and operationalizing a machine learning workflow that is optimized for\\nAzure.\\nAzure Databricks : This is an Apache Spark-based analytics and machine\\nlearning platform  optimized for the Microsoft Azure platform.\\n\\nhis text will focus heavily  on the Azure  Databricks  environment as it provides access to\\nmany tools, including these:\\nSpark DataFrames : Spark DataFrames are distributed collections of data\\norganized into rows and columns. They are conceptually equivalent to a data\\nframe in Python.\\nNotebooks : Lik e Jupyter, the Azure Databricks notebook tool provides a cell-\\nbased code environment for writing logically separated code that is easy to read\\nand replicate.\\nClusters/workers : Using Microsoft Azure (or another cloud computing\\nplatform), we can spin  up resources in order to massively optimize and\\nparallelize our machine learning and data analysis to allow for faster iteration.\\nMLib : This is a scalable machine  learning library consisting of common learning\\nalgorithms and utilities, including classification, regression, and more.\\nAzure Databricks provides many more capabilities other than what we have listed here. We\\nwill focus on the tools that are relevant to us as data scientists and machine learning\\nengineers.\\nFor more information on Azure Databricks, check out https:/ \\u200b/\\u200bdocs.\\nmicrosoft. \\u200bcom/ \\u200ben-\\u200bus/\\u200bazure/ \\u200bazure- \\u200bdatabricks/ \\u200bwhat- \\u200bis-\\u200bazure-\\ndatabricks .\\nWhat exactly are Spark and PySpark?\\nThe backbone of Azure Databricks  is Apache Spark. Spark  is an analytics engine for big\\ndata processing. It has built-in modules for data streaming, SQL, machine learning, and\\nmore.  PySpark  is the Python API for Spark. It allows us to invoke the power  of Spark using\\nPython code. Azure Databricks brings all of this at our fingertips and makes setting up\\nclusters running Spark and PySpark within seconds possible. Let\\'s see it in action!\\nBasic Azure Databricks use\\nBefore we begin with our case studies, it is important to get our bearings in  Azure\\nDatabricks. We will begin by setting up our first cluster and some notebooks to write our\\nPython code in.\\n\\netting up our first cluster\\nTo get started in Azure Databricks, we have to set up our first cluster. This will spin up\\n(initialize) resources running the Azure Databricks Runtime Environment (including\\nSpark). This is where all of the action takes place. Whenever we run code in our notebooks,\\nthe code is sent to our cluster to actually run it. This means that no code will ever actually\\nrun on our local machine. This is great for many reasons, one of the main ones being that it\\nprovides data scientists with sub-optimal  equipment at home/work a chance to use\\nproduction-quality resources for a fraction of the cost!\\nTo set up a cluster, navigate to the Clusters  option on the left-hand pane and click on\\nCreate cluster . There, you will see a form with three basic pieces of information: Cluster\\nName , Azure Databricks Runtime Version , and Python Version . Make your cluster name\\nwhatever your heart desires. Try to use the most recent Azure Databricks version (this is\\nthe default) and select Python version 3. Once that\\'s finished, click Create cluster  again and\\nyou are done.\\nWhen spinning up a cluster, there are many optional advanced settings\\nthat we have the ability to set. For our purposes, we will not need to. \\nThe page should look something like this:\\n\\n\\nnce we have a cluster, it is time to set up a notebook. This can be done from the home\\npage. When creating a new notebook, all we have to do is select the language we wish to\\nuse (Python for now, but more are  available). The notebooks in Azure Databricks are nearly\\nidentical to Jupyter notebooks in functionality and use (nifty!). This comes in handy for\\ndata scientists who are used to this environment :\\nCreating a notebook is even easier than spinning up a new cluster. All of the code that we write in here will be run on our cluster and not on our local machine\\nOnce we have our cluster up and running and we are able to create notebooks, it is time to\\njump right into using the Azure Databricks environment and seeing first-hand how the\\npower of Apache Spark and the Microsoft Data Environment will affect the way that we\\nwrite data-driven code.\\nCase study 1 – bike-sharing usage prediction\\nusing parallelization in Azure Databricks\\nOur first case study will focus on setting up a simple  notebook in Azure Databricks and\\nrunning some basic data visualization and machine learning code in order to get used to\\nthe Azure Databricks environment. Azure Databricks comes with a built-in filesystem that\\nis preloaded with data for us to use. We can upload our own files to the system (which we\\nwill do in the second case study),  but for now, we will import a dataset that came pre-\\nloaded with Azure Databricks. We will also make heavy use of the built-in Spark-based\\nvisualization tools to analyze our data to the fullest extent that we can.\\n\\nhe aspects of Azure Databricks that we will be highlighting in this case study include the\\nfollowing:\\nThe collection of open data that is easily accessible by the Azure Databricks\\nfilesystem\\nConverting our pandas  DataFrames to Spark equivalents and generating\\nvisualizations\\nParallelizing some simple hyperparameter tuning\\nBroadcasting variables to workers to enhance parallelization further\\nThe data that we will be using involves predicting the amount of bikes being rented via a\\nbike-share system. Our goal  is to predict the usage of the system based on daily/hourly\\ncorresponding weather, time-based, and seasonal information. Let\\'s get right to it and see\\nour first Azure Databricks-specific programming. We can access this through the dbutils\\nmodule:\\n# display is a reserved function in Databricks that allows us to view and\\nmanipulate Dataframes inline\\n# dbutils is a library full of ready-to-use datasets for us to use\\n# Let\\'s start by displaying all of the directories in the main data folder\\n# \"databricks-datasets\"\\ndisplay(dbutils.fs.ls(\"/databricks-datasets\"))\\nThe output is a DataFrame of available folders to look in for data. Here is a snippet:\\npath                                         name                 size\\ndbfs:/databricks-datasets/README.md          README.md            976\\ndbfs:/databricks-datasets/Rdatasets/         Rdatasets/           0\\ndbfs:/databricks-datasets/SPARK_README.md    SPARK_README.md      3359\\ndbfs:/databricks-datasets/adult/             adult/               0\\ndbfs:/databricks-datasets/airlines/          airlines/            0\\ndbfs:/databricks-datasets/amazon/            amazon/              0\\ndbfs:/databricks-datasets/asa/               asa/                 0\\ndbfs:/databricks-datasets/atlas_higgs/       atlas_higgs/         0\\ndbfs:/databricks-datasets/bikeSharing/       bikeSharing/         0\\ndbfs:/databricks-datasets/cctvVideos/        cctvVideos/          0\\ndbfs:/databricks-datasets/credit-card-fraud/ credit-card-fraud/   0\\ndbfs:/databricks-datasets/cs100/             cs100/               0\\ndbfs:/databricks-datasets/cs110x/            cs110x/              0\\n...\\n\\nvery time we run a command in our notebook, the time that it took our cluster to execute\\nthat code is shown at the bottom. For example, this code block took my cluster 0.88 seconds.\\nIn general, the format for this statement is as follows:\\nCommand took  <<elapsed_time>>  -- by <<username/email>>  at <<date>>,\\n<<time>>  on <<cluster_name>>\\nWe can view the contents of a particular file:\\n# Let\\'s check out the general README of the date folder by opening the\\nmarkdown folder and printing out the result of \"readlines\"\\n# which is a way to print out the contents of a file\\nwith open(\"/dbfs/databricks-datasets/README.md\") as f:\\n x = \\'\\'.join(f.readlines())\\nprint(x)\\nDatabricks Hosted Datasets\\n==========================\\nThe data contained within this directory is hosted for users to build data\\npipelines using Apache Spark and Databricks.\\nLicense\\n-------\\n....\\nLet\\'s take a deeper look at the bikeSharing  directory:\\n# Let\\'s list the contents of the directory corresponding to the data that\\nwe want to import\\ndisplay(dbutils.fs.ls(\"dbfs:/databricks-datasets/bikeSharing/\"))\\nRunning the preceding code will list out the contents of the bikeSharing  directory:\\npath                                                name       size\\ndbfs:/databricks-datasets/bikeSharing/README.md     README.md  5016\\ndbfs:/databricks-datasets/bikeSharing/data-001/     data-001/  0\\nWe have a README file in markdown format and another sub-directory. In the  data-001\\nfolder, we can list out the contents and see that there are two CSV files:\\n# the data is given to use in both an hourly and a daily format.\\ndisplay(dbutils.fs.ls(\"dbfs:/databricks-datasets/bikeSharing/data-001/\"))\\n\\nhe output is as follows:\\npath                                                     name       size\\ndbfs:/databricks-datasets/bikeSharing/data-001/day.csv   day.csv    57569\\ndbfs:/databricks-datasets/bikeSharing/data-001/hour.csv  hour.csv   1156736\\nWe will use the hourly format for our study. Before we do, note that there is a README in\\nthe directory that will give us context and information about the data. We can view the file\\nby opening it and printing our the line contents as we did before:\\n# Note that we had to change the format from dbfs:/ to /dbfs/\\nwith open(\"/dbfs/databricks-datasets/bikeSharing/README.md\") as f:\\n x = \\'\\'.join(f.readlines())\\nprint(x)\\n## Dataset\\nBike-sharing rental process is highly correlated to the environmental and\\nseasonal settings. For instance, weather conditions, precipitation, day of\\nweek, season, hour of the day, etc. can affect the rental behaviors. The\\ncore data set is related to the two-year historical log corresponding to\\nyears 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA\\nwhich is publicly available in http://capitalbikeshare.com/system-data. We\\naggregated the data on two hourly and daily basis and then extracted and\\nadded the corresponding weather and seasonal information. Weather\\ninformation are extracted from http://www.freemeteo.com.\\n...\\nThe README goes on to say that this dataset is primarily used to do regression (predicting\\na continuous response). We will treat the problem as a classification by bucketing our\\nresponse to classes. More on this later. Let\\'s import our data into a good old-fashioned\\npandas DataFrame:\\n# Load data into a Pandas dataframe (note that pandas, sklearn, etc come\\nwith the environment. That\\'s pretty neat!)\\nimport pandas\\nbike_data = pandas.read_csv(\"/dbfs/databricks-\\ndatasets/bikeSharing/data-001/hour.csv\").iloc[:,1:] # remove line number\\n# view the dataframe\\n#look at the first row\\nbike_data.loc[0]\\n\\nhe output is as follows:\\ndteday 2011-01-01\\nseason 1\\nyr 0 mnth 1 hr 0 holiday 0 weekday 6 workingday 0 weathersit 1 temp 0.24\\natemp 0.2879 hum 0.81 windspeed 0 casual 3 registered 13 cnt 16\\nDescriptions for each variable are in the README:\\nholiday : A Boolean that  is 1 (True ) means the  day is a holiday and 0 (False )\\nmeans it is not  (extracted from http:/ \\u200b/\\u200bdchr. \\u200bdc.\\u200bgov/ \\u200bpage/ \\u200bholiday- \\u200bschedule )\\nworkingday : A Boolean that is 1 (True ) means  the day is neither a weekend nor\\na holiday, otherwise 0 (False )\\ncnt: Count of total rental bikes, including both casual and registered\\nThe README included in the Azure Databricks filesystem is very helpful for context and\\ninformation about the datasets. Let\\'s see how many observations we have:\\n# 17,379 observations\\nbike_data.shape\\n(17379, 16)\\nIt is nice to stop and notice that everything that we have done so far (except for the Azure\\nDatabricks filesystem module) would be exactly the same if we were to do it in a normal\\nJupyter environment. Let\\'s see whether our dataset has any missing data that we would\\nhave to deal with:\\n# no missing data, great!\\nbike_data.isnull().sum()\\ndteday 0\\nseason 0\\nyr 0\\nmnth 0 hr 0\\nholiday 0\\nweekday 0\\nworkingday 0\\nweathersit 0\\ntemp 0\\natemp 0\\nhum 0\\nwindspeed 0\\ncasual 0\\nregistered 0\\ncnt 0\\n\\no missing data. Excellent. Let\\'s see a quick histogram of the cnt column, as it represents\\nthe total count of bikes reserved in that hour:\\n# not everything will carry over, but that is ok\\n%matplotlib inline\\nbike_data.hist(\"cnt\")\\n# matplotlib inline is not supported in Databricks.\\n# You can display matplotlib figures using display(). For an example, see #\\nhttps://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot\\n.html\\nWe get an error when we run this code, even though we have run this command  in this\\nbook before. Unfortunately, matplotlib  does not work exactly the same in this\\nenvironment as it does in Jupyter. That is fine, though, because Azure Databricks provides\\na visualization tool built on top of Spark\\'s version of a DataFrame that puts even more\\ncapabilities at our fingertips. To get access to these visualizations, we will have to first\\nconvert our pandas DataFrame to its Spark equivalent:\\n# converting to a Spark DataFrame allows us to make multiple types of plots\\nusing either a sample of data or the entire data population as the source\\nsparkDataframe = spark.createDataFrame(bike_data)\\n# display is a simple way to view your data either via a table or through\\ngraphs\\ndisplay(sparkDataframe)\\nRunning the preceding code yields a snippet of the Dataframe for us to inspect:\\n\\n\\nhe display  command shows the Spark DataFrame inline with our notebook.\\nAzure Databricks allows for extremely powerful graphing capabilities thanks to\\nSpark. When using the display  command, on the bottom-left of the cell, a widget will\\nappear, allowing us to graph using the data. Let\\'s click on the Histogram :\\nAzure Databricks oﬀers a multitude of graphing options to get the best picture of our data\\nAnother button will appear, called Plot Options... , which allows us to customize our graph.\\nWe can drag and drop our columns into one of three fields:\\nKeys  will, in general, represent our x-axis.\\nValues  will, in general, represent our y-axis.\\nSeries groupings  will separate our data into groupings in order to get a bigger\\npicture.\\n\\ny default, display and visualizations will only aggregate over the first\\n1,000 rows for convenience. We can force Azure Databricks to utilize the\\nentire dataset in our graph by setting the appropriate option.\\nCreating a simple histogram in Azure Databricks couldn\\'t be simpler!\\nWe can see how easy to use the system is and that our distribution is right-skewed.\\nInteresting! Let\\'s try something else now. Let\\'s say we also want to visualize the hourly\\nusage, separated by the binary variable workingday ; the idea being that we are curious to\\nsee how the total amount of bikes reserved changes by the hour and if that distribution\\nchanges depending on whether it\\'s a working day or not. We can achieve this by selecting\\ncnt as our value, hr as our key, and workingday  as our grouping.\\n\\nt should look something like this:\\nHitting Apply  will apply this to the entire dataset, instead of the first-1,000-row sample that\\nit shows us on the right:\\n\\n\\nt is extremely easy to use Azure Databricks to generate beautiful and interpretable graphs\\nbased on our data. From this, we can see that bike sharing appears to be somewhat\\nnormally distributed on weekends while workdays have large spikes in the morning and\\nevening (which makes sense).\\nIn this dataset, there are three possible  regression response candidates: casual ,\\nregistered , and cnt. We will turn our problem into a classification problem by bucketing\\nthe cnt column into one of two buckets. Our response will either be 100 or fewer bikes\\nreserved that hour ( False ) or over 100 ( True ):\\n# Seperate into X, and y (features and label)\\n# we will turn our regression problem into a classification problem by\\nbucketing the column \"cnt\" as being either over 100 / 100 or under.\\nfeatures, labels = bike_data[[\"season\", \"yr\", \"mnth\", \"hr\", \"holiday\",\\n\"weekday\", \"workingday\", \"weathersit\", \"atemp\", \"hum\", \"windspeed\"]],\\nbike_data[\"cnt\"] > 100\\n# See the distribution of our labels\\nlabels.value_counts()\\nTrue 10344\\nFalse 7035\\nLet\\'s now create a simple function that will do a few things: take in a param choice for a\\nrandom forest classifier, fit and test our model on our data, and return the results:\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n# Create a function that will\\n# 0. Take in a parameter for our Random Forest\\'s n_estimators param\\n# 1. Instantiate a Random Forest algorithm\\n# 2. Split our data into a training and testing split\\n# 3. Fit on our training data\\n# 4. evaluate on our testing data\\n# 5. return a tuple with the n_estimators param and corresponding accuracy\\ndef runRandomForest(c):\\n rf = RandomForestClassifier(n_estimators=c)\\n # Split into train and test using\\nsklearn.cross_validation.train_test_split\\n X_train, X_test, y_train, y_test = train_test_split(features, labels,\\ntest_size=0.2, random_state=1)\\n\\n# Build the model\\n rf.fit(X_train, y_train)\\n # Calculate predictions and accuracy\\n predictions = rf.predict(X_test)\\n accuracy = accuracy_score(predictions, y_test)\\n # return param and accuracy score\\n return (c, accuracy)\\nThis function takes in a single number as an input, sets that number as a random forest\\'s\\nn_estimator  parameter, and returns the accuracy associated with that param choice on a\\ntrain-test split:\\nrunRandomForest(1)\\n(1, 0.90218642117376291)\\nCommand took  0.13 seconds\\nThis took a very short amount of time because we were only training a single decision tree\\nin our forest. Let\\'s now iteratively try a varying number of estimators and see how this\\naffects our accuracy:\\nfor i in [1, 10, 100, 1000, 10000]:\\n  print(runRandomForest(i))\\n(1, 0.89700805523590332)\\n(10, 0.939873417721519)\\n(100, 0.94677790563866515)\\n(1000, 0.94677790563866515)\\n(10000, 0.94792865362485612)\\nCommand took  4.16 minutes\\nIt took my cluster over 4 minutes to try these five n_estimator  options. Most of the time\\nwas taken up by the final two options as it took a very long time to train thousands of\\ndecision trees.\\nAs we add more combinations of parameters  and more parameter\\noptions, the time it will take to iteratively go through these options will\\nexplode. We will see how we can utilize Databrick\\'s environment to\\noptimize this in the third case study.\\n\\net\\'s make use of Spark to parallelize our for loop. Every notebook has a special variable\\ncalled sc that represents Spark:\\n# every notebook has a variable called \"sc\" that represents the Spark\\ncontext in our cluster\\nsc\\nThere are a few ways of performing this parallelization, but in general, it will look like this:\\nCreate a dataset that will be sent to our cluster (in this case, parameter options).1.\\nMap a function to each element of the dataset (our runRandomForest  function). 2.\\nCollect the results:3.\\n# 1. set up 5 tasks in our Spark Cluster by parallelizing a\\ndataset (list) of five elements (n_estimator options)\\nk = sc.parallelize([1, 10, 100, 1000, 10000])\\n# 2. map our function to our 5 tasks\\n# The code will not be sent to our cluster until we run the\\nnext command\\nresults = k.map(runRandomForest)\\nCommand took  0.13 seconds\\nHere, we are introduced to our first Apache Spark-specific syntax. Step 1  will return a\\ndistributed dataset that is optimized for parallel computation, and we will call that dataset\\nk. The values of k represent different possible arguments for our function,\\nrunRandomForest . Step 2  tells our cluster to run the function across our distributed\\ndataset.\\nIt is important to note that while Step 1  and Step 2  are Spark-specific commands, up until\\nnow, our function has not actually been sent to our cluster for execution. We have just set\\nthe stage to do so by setting up the appropriate variables. Step 3  will collect our results by\\nrunning the function in parallel across the different values in k:\\n# 3. the collect method actually sends the five tasks to our cluster for\\nexecution\\n# Faster (1.5x) because we aren\\'t doing each task one after the other. We\\nare doing them in parallel\\n# This becomes much more noticeable when doing more params (we will get to\\nthis in a later case study)\\nresults.collect()\\nCommand took  2.73 minutes\\n\\nmmediately, we can see the value of parallelizing functions using Spark. By doing nothing\\nmore than relying on Azure Databricks and Spark, we are able to perform our for loop 1.5x\\nfaster. If we use a variable in a function (like our dataset), Spark will automatically send the\\ndataset to the workers. This is usually fine. We can send it to workers  more efficiently by\\nbroadcasting  it. By broadcasting data, a copy of the data is sent to our workers, which are\\nused when running tasks . This is much more efficient when dealing with extremely large\\ndatasets with large values in them. We can rewrite our previous function using broadcast\\nvariables:\\n# Broadcast dataset\\n# If we use a variable in a function, Spark will automatically send the\\ndataset to the workers. This is usually fine.\\n# We can send it to workers more efficiently by broadcasting it. By\\nbroadasting data, a copy is sent to our workers which are used when running\\ntasks.\\n# For more info on broadcast variables, see the Spark programming guide.\\nYou can create a Broadcast variable using sc.broadcast().\\n# To access the value of a broadcast variable, you need to use .value\\n# broadcast the variables to our workers\\nfeaturesBroadcast = sc.broadcast(features)\\nlabelsBroadcast = sc.broadcast(labels)\\n# reboot of the previous function\\ndef runRandomForestBroadcast(c):\\n  rf = RandomForestClassifier(n_estimators=c)\\n  # Split into train and test using\\nsklearn.cross_validation.train_test_split\\n  # ** This part of the function is the only difference from the previous\\nversion **\\n  X_train, X_test, y_train, y_test =\\ntrain_test_split(featuresBroadcast.value, labelsBroadcast.value,\\ntest_size=0.2, random_state=1)\\n  # Build the model\\n  rf.fit(X_train, y_train)\\n  # Calculate predictions and accuracy\\n  predictions = rf.predict(X_test)\\n  accuracy = accuracy_score(predictions, y_test)\\n  return (c, accuracy)\\n\\nnce our new function using broadcast variables is complete, running it in parallel is no\\ndifferent:\\n# set up 5 tasks in our Spark Cluster\\nk = sc.parallelize([1, 10, 100, 1000, 10000])\\n# map our function to our five tasks\\nresults = k.map(runRandomForestBroadcast)\\n# the real work begins here.\\nresults.collect()\\nThe timing is not very different from  our last run with non-broadcast data. This is because\\nour data and logic are not large enough to see a noticeable difference yet. Once done with\\nour variables, we can unpersist (unbroadcast) them like so:\\n# Since we are done with our broadcast variables, we can clean them up.\\n# (This will happen automatically, but we can make it happen earlier by\\nexplicitly unpersisting the broadcast variables.\\nfeaturesBroadcast.unpersist()\\nlabelsBroadcast.unpersist()\\nHyperparameter tuning would be very difficult to do if you wanted to do a grid search\\nacross multiple parameters and multiple options. We could enhance our function in order\\nto accommodate this; however, it would be better to rely on existing frameworks in scikit-\\nlearn to do so. We will see how to remedy this conundrum in a later case study.\\nWe can see how we can utilize Databrick\\'s easy-to-use environment, clusters, and\\nnotebooks in order to enhance our data analysis and machine learning with minimal\\nchanges to our coding style. In the next case study, we will examine Spark\\'s scalable\\nmachine learning library, MLlib, for optimized machine learning speed.\\nCase study 2 – Using MLlib in Azure Databricks\\nto predict credit card fraud\\nOur second case study will focus  on predicting credit card fraud and will make use of\\nMLlib, Apache Spark\\'s scalable machine learning library. MLlib comes standard with our\\nAzure Databricks environment and allows us to write scalable machine learning code. This\\ncase study will focus on MLlib syntax while we draw parallels (no pun intended) to its\\nscikit-learn cousins.\\n\\nhe aspects of Azure Databricks that we will be highlighting in this case study include\\nthese:\\nImporting a CSV that is uploaded to the Azure Databricks filesystem\\nUsing MLlib\\'s pipeline, feature pre-processing, and machine learning library to\\nwrite scalable machine learning code\\nUsing MLlib metric evaluation modules that mirror scikit-learn\\'s components\\nThe data in question is predicting credit card fraud. The data represents credit card\\ntransactions and contains 28 anonymized continuous features (called V1 .. V28) plus the\\namount that the transaction was for and the time at which the transaction occurred. We will\\nnot dive too deeply into feature engineering in this case study, but will focus  mainly on\\nusing the MLlib modules that come with Azure Databricks to predict the response label\\n(which is a binary variable). Let\\'s get right into it and start by importing a CSV that we have\\nuploaded using the Azure Databricks data import feature:\\n# File location and type\\n# This dataset also exists in the DBFS of Databricks. This is just to show\\nhow to import a CSV that has been previously uploaded so that\\n# you can upload your own!\\nfile_location = \"/FileStore/tables/creditcard.csv\"\\nfile_type = \"csv\"\\n# CSV options\\n# will automatically cast columns as appropiate types (float, string, etc)\\ninfer_schema = \"true\"\\nfirst_row_is_header = \"true\"\\ndelimiter = \",\"\\n# read a csv file and convert to a Spark dataframe\\ndf = spark.read.format(file_type) \\\\\\n .option(\"inferSchema\", infer_schema) \\\\\\n .option(\"header\", first_row_is_header) \\\\\\n .option(\"sep\", delimiter) \\\\\\n .load(file_location)\\n# show us the Spark Dataframe\\ndisplay(df)\\n\\nur goal is to build a pipeline that will do this:\\nAssemble the columns that we wish to use as features1.\\nScale our features using a standard z-score function2.\\nEncode our label as 0 or 1 (it already is but it is good to see this functionality)3.\\nRun a logistic regression across the training data to fit coefficients4.\\nEvaluate binary metrics on the testing set5.\\nUse an MLlib cross-validating grid searching module to find the best parameters6.\\nfor our logistic regression model\\nPhew. That\\'s a lot, so let\\'s go one step at a time. The following code block will handle Step 1 ,\\nStep 2 , and Step 3  by setting up a pipeline with three steps in it:\\n# import the pipeline module from pyspark\\nfrom pyspark.ml import Pipeline\\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler,\\nStringIndexer\\n# will hold the steps of our pipeline\\nstages = []\\nThe first stage of our pipeline will assemble the features that we want to use in our machine\\nlearning procedure. Let\\'s use all 28 entries from the anonymized data (V1 - V28) plus the\\namount  column. For this case study, we will not use the time  column:\\n# Transform all features into a vector using VectorAssembler\\n# create list of [\"V1\", \"V2\", \"V3\", ...]\\nnumericCols = [\"V{}\".format(i) for i in range(1, 29)]\\n# Add \"Amount\" to the list of features\\nassemblerInputs = numericCols + [\"Amount\"]\\n# VectorAssembler acts like scikit-learn\\'s \"FeatureUnion\" to put together\\nthe feature columns and adding the label \"features\"\\nassembler = VectorAssembler(inputCols=assemblerInputs,\\noutputCol=\"features\")\\n# add the VectorAssembler to the stages of our MLLib Pipeline\\nstages.append(assembler)\\nThe second stage of our pipeline will take the assembled features and scale them:\\n# MLLib\\'s StandardScaler acts like scikit-learn\\'s StandardScaler\\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\\n withStd=True, withMean=True)\\n# add StandardScaler to our pipeline\\nstages.append(scaler)\\n\\nnce we\\'ve gathered our features and scaled them, the final step of our pipeline encodes\\nour class label  to ensure consistency:\\n# Convert label into label indices using the StringIndexer (like scikit-\\nlearn\\'s LabelEncoder)\\nlabel_stringIdx = StringIndexer(inputCol=\"Class\", outputCol=\"label\")\\n# add the StringIndexer to our pipeline\\nstages.append(label_stringIdx)\\nThat was a lot, but note that every step of the pipeline has a scikit-learn equivalent. As a\\ndata scientist, the most important thing to remember is that as long as you know the theory \\nand the proper steps of data science and machine learning, you can transfer your skills\\nacross many languages, platforms, and technologies. Now that we have created our list of\\nthree stages, let\\'s instantiate a pipeline  object:\\n# create our pipeline with three stages\\npipeline = Pipeline(stages=stages)\\nTo make sure that we are training a model that is able to predict unseen cases, we should\\nsplit up our data into a training and testing set, and fit our pipeline to the training set while\\nusing the trained pipeline to transform the testing set:\\n# We need to split our data into training and test sets. This is like\\nscikit-learn\\'s train_test_split function\\n# We will also set a random number seed for reproducibility\\n(trainingData, testData) = df.randomSplit([0.7, 0.3], seed=1)\\nprint(trainingData.count())\\nprint(testData.count())\\n199470  # elements in the training set\\n85337  # elements in the testing set\\nLet\\'s now fit our pipeline to the training set. This will learn the features as well as the\\nparameters to scale future unseen testing data:\\n# fit and transform to our training data\\npipelineModel = pipeline.fit(trainingData)\\ntrainingDataTransformed = pipelineModel.transform(trainingData)\\n\\nf we take a look at our DataFrame, we will notice that we have three new columns:\\nfeatures , scaledFeatures , and label . These three columns were added by our pipeline\\nand those names can be found exactly in the preceding code where we set the three stages\\nof the pipeline. Note that the data types of the features and scaledFeatures  column\\nare vector . This indicates that they represent observations to be learned by our machine\\nlearning model in the future:\\n# note the new columns \"features\", \"scaledFeatures\", and \"label\" at the end\\ntrainingDataTransformed\\nTime:decimal(10,0)\\nV1:double\\nV2:double\\nV3:double\\nV4:double\\nV5:double\\nV6:double\\nV7:double\\nV8:double\\nV9:double\\nV10:double\\nV11:double\\nV12:double\\nV13:double\\nV14:double\\nV15:double\\nV16:double\\nV17:double\\nV18:double\\nV19:double\\nV20:double\\nV21:double\\nV22:double\\nV23:double\\nV24:double\\nV25:double\\nV26:double\\nV27:double\\nV28:double\\nAmount:double\\nClass:integer\\nfeatures: udt\\nscaledFeatures: udt\\nlabel:double\\n\\nust like we do in scikit-learn, we will have to import out logistic regression model,\\ninstantiate it, and fit it to our training data:\\n# Import the logistic regression module from pyspark\\nfrom pyspark.ml.classification import LogisticRegression\\n# Create initial LogisticRegression model\\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaledFeatures\")\\n# Train model with Training Data\\nlrModel = lr.fit(trainingDataTransformed)\\nThis process should look very familiar to us as it is nearly identical to scikit-learn. The main\\ndifference is that when we instantiate our model, we have to tell the object the names of the\\nlabel and features, instead of feeding the features and label separately into the fit method\\n(like we do in scikit-learn).\\nOnce we fit our model, we can transform and gather predictions from our testing data:\\n# transform (not fit) to the testing data\\ntestDataTransformed = pipelineModel.transform(testData)\\n# run our logistic regression over the transformed testing set\\npredictions = lrModel.transform(testDataTransformed)\\nTransforming our testing data using logistic regression will actually add three new columns\\n(like the pipeline did). We can see this by running this:\\npredictions:\\nTime: decimal(10,0)\\nV1: double\\nV2: double\\nV3: double\\nV4: double\\nV5: double\\nV6: double\\nV7: double\\nV8: double\\nV9: double\\nV10: double\\nV11: double\\nV12: double\\nV13: double\\nV14: double\\nV15: double\\nV16: double\\nV17: double\\nV18: double\\n\\n19: double\\nV20: double\\nV21: double\\nV22: double\\nV23: double\\nV24: double\\nV25: double\\nV26: double\\nV27: double\\nV28: double\\nAmount: double\\nClass: integer\\nfeatures: udt\\nscaledFeatures: udt\\nlabel: double\\nrawPrediction: udt\\nprobability: udt\\nprediction: double\\nLet\\'s take a look at the label  column as well  as two of the new columns,  probability  and\\nprediction . We can do this by invoking the filter method of the Spark DataFrame:\\nselected = predictions.select(\"label\", \"prediction\", \"probability\")\\ndisplay(selected)\\nThe output is as follows:\\nlabel prediction probability\\n0     0          [1,2,[],[0.9998645390717447,0.000135460928255428]]\\n0     0          [1,2,[],[0.9998821292706751,0.00011787072932487872]]\\n0     0          [1,2,[],[0.9994714991454193,0.000528500854580697]]\\n0     0          [1,2,[],[0.9991193503498385,0.0008806496501614437]]\\n0     0          [1,2,[],[0.9997043818469743,0.00029561815302580084]]\\n0     0          [1,2,[],[0.9998106820888389,0.00018931791116114655]]\\n0     0          [1,2,[],[0.9995735877526569,0.0004264122473429876]]\\n...\\nThe label  and prediction  columns show each observation\\'s ground truth and our\\nmodel\\'s estimate, while the probability  column holds a vector that contains the\\npredicted probability (think scikit-learn\\'s predict_proba  functionality). Let\\'s now bring in\\nPySpark\\'s metric evaluation module in order to get some basic metrics. we will start with\\nBinaryClassificationEvaluator , which can tell us the testing AUC (area under the\\nROC curve):\\n# like scikit-learn\\'s metric module\\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\\n\\n Evaluate model using either area under Precision Recall curev or the area\\nunder the ROC\\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\\nmetricName=\"areaUnderROC\")\\nevaluator.evaluate(predictions)\\n0.984986959421\\nThis is helpful, but we also want some of the more familiar and interpretable metrics such\\nas accuracy , precision , recall , and more. We can get these using\\nPySpark\\'s MulticlassMetrics  module:\\n# Get some deeper metrics out of our predictions\\nfrom pyspark.mllib.evaluation import MulticlassMetrics\\n# must turn DF into RDD (Resilient Distributed Dataset)\\npredictionAndLabels = predictions.select(\"label\",\\n\"prediction\").rdd.map(tuple)\\n# instantiate a MulticlassMetrics object\\nmetrics = MulticlassMetrics(predictionAndLabels)\\n# Overall statistics\\naccuracy = metrics.accuracy\\nprecision = metrics.precision()\\nrecall = metrics.recall()\\nf1Score = metrics.fMeasure()\\nprint(\"Summary Stats\")\\nprint(\"Accuracy = %s\" % accuracy)\\nprint(\"Precision = %s\" % precision)\\nprint(\"Recall = %s\" % recall)\\nprint(\"F1 Score = %s\" % f1Score)\\nThe output is as follows:\\nSummary Stats\\nAccuracy = 0.999203159239\\nPrecision = 0.999203159239\\nRecall = 0.999203159239\\nF1 Score = 0.999203159239\\nNote that accuracy is an attribute of the MulticlassMetrics  object and\\nnot a method, so we do not need the parentheses.\\n\\nreat! We can also calculate our true positive rate, false negative rate, and other by using\\nthe predictions vector. This will give us a better sense of how our machine learning model\\nperforms on particular examples of positive and negative fraud cases:\\ntp = predictions[(predictions.label == 1) & (predictions.prediction ==\\n1)].count()\\ntn = predictions[(predictions.label == 0) & (predictions.prediction ==\\n0)].count()\\nfp = predictions[(predictions.label == 0) & (predictions.prediction ==\\n1)].count()\\nfn = predictions[(predictions.label == 1) & (predictions.prediction ==\\n0)].count()\\nprint (\"True Positives:\", tp)\\nprint (\"True Negatives:\", tn)\\nprint (\"False Positives:\", fp)\\nprint (\"False Negatives:\", fn)\\nThe output is as follows:\\nTrue Positives: 93\\nTrue Negatives: 85176\\nFalse Positives: 18\\nFalse Negatives: 50\\nLet\\'s consider this our baseline logistic regression model and try to optimize our results by\\ntweaking our logistic regression parameters.\\nUsing the MLlib Grid Search module to tune\\nhyperparameters\\nTo optimize our parameters, it would help  to know exactly what they were and how to use\\nthem. Luckily, we can see the README in each model by running the explainParams\\nmethod. This will generate a list of the available parameters, a description as to what they\\nrepresent, and usually a guide to the acceptable values:\\n# explain the parameters that are included in MLLib\\'s Logistic Regression\\nprint(lr.explainParams())\\nThe output is as follows:\\naggregationDepth : suggested depth for treeAggregate (>= 2). (default: 2)\\nelasticNetParam : the ElasticNet mixing parameter, in range [0, 1]. For\\nalpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1\\n\\nenalty. (default: 0.0)\\nfamily: The name of family which is a description of the label distribution\\nto be used in the model. Supported options: auto, binomial, multinomial\\n(default: auto) featuresCol : features column name. (default: features,\\ncurrent: scaledFeatures) fitIntercept : whether to fit an intercept term.\\n(default: True)\\nlabelCol : label column name. (default: label, current: label)\\n....\\nLet\\'s build a parameter grid for Spark to gridsearch  across. Let\\'s choose three parameters\\nto start with: maxIter , regParam,  and elasticNetParam . We first need to build a\\nparameter grid using PySpark\\'s version of scikit-learn\\'s GridSearchCV :\\n# pyspark\\'s version of GridSearchCV\\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\\n# Create ParamGrid for Cross Validation\\nparamGrid = (ParamGridBuilder()\\n .addGrid(lr.regParam, [0.0, 0.01, 0.5, 2.0])\\n .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\n .addGrid(lr.maxIter, [5, 10])\\n .build())\\n# Create 5-fold CrossValidator that can also test multiple parameters (like\\nscikit-learn\\'s GridSearchCV)\\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,\\nevaluator=evaluator, numFolds=5)\\nLet\\'s send our grid search to the cluster for the efficient parallelization of tasks:\\n# Run cross validations on our cluster\\ncvModel = cv.fit(trainingDataTransformed)\\n# this will likely take a fair amount of time because of the amount of\\nmodels that we\\'re creating and testing\\nThe variable, cvModel , holds the logistic regression model with the optimized parameters.\\nFrom our optimized model, we can extract the learned coefficients to gain a deeper\\nunderstanding as to how the features correlate to the label:\\n# extract the weights from our model\\nweights = cvModel.bestModel.coefficients\\n# convert from numpy type to float\\nweights = [[float(w)] for w in weights]\\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\\ndisplay(weightsDF)\\n\\nunning the preceding code yields a single-column DataFrame with the logistic regression\\ncoefficients. As we\\'ve seen before, the weights represent the importance and correlation\\nbetween the features and the response:\\nFeature Weight\\n-0.009310428799214699\\n0.024089617982041803\\n-0.07660344812402071\\n0.13879375587420806\\n0.03389644146602658\\n-0.03197804822203382\\n-0.026671134727093863\\n-0.05963860645699601\\n-0.07157334685249503\\n-0.12739634200985744\\n0.11271203988538568\\n-0.16991941687681994\\n-0.022382161065846975\\n-0.2967372927422323\\n-0.002525484797586701\\n-0.08661888759753078\\n-0.09428351861530046\\n-0.010145267312697291\\n0.000474661023205239\\n0.003985147929831393\\n0.031230406955806467\\n0.011362000753207976\\n-0.014548646956536248\\n-0.011270335506048019\\n-0.004390342109545349\\n0.008722583938741943\\n0.01390573346423987\\n0.014176539525542918\\n0.021489763526114244\\nWe can grab our predictions in the same way we did previously to compare our results:\\n# Use test set to get the best params\\npredictions = cvModel.transform(testDataTransformed)\\n# must turn DF into RDD (Resilient Distributed Dataset)\\npredictionAndLabels = predictions.select(\"label\",\\n\"prediction\").rdd.map(tuple)\\nmetrics = MulticlassMetrics(predictionAndLabels)\\n# Overall statistics\\n\\nccuracy = metrics.accuracy\\nprecision = metrics.precision()\\nrecall = metrics.recall()\\nf1Score = metrics.fMeasure()\\nprint(\"Summary Stats\")\\nprint(\"Accuracy = %s\" % accuracy)\\nprint(\"Precision = %s\" % precision)\\nprint(\"Recall = %s\" % recall)\\nprint(\"F1 Score = %s\" % f1Score)\\nThe output is as follows:\\nSummary Stats\\nAccuracy = 0.998839893598\\nPrecision = 0.998839893598\\nRecall = 0.998839893598\\nF1 Score = 0.998839893598\\nRun the following code:\\ntp = predictions[(predictions.label == 1) & (predictions.prediction ==\\n1)].count()\\ntn = predictions[(predictions.label == 0) & (predictions.prediction ==\\n0)].count()\\nfp = predictions[(predictions.label == 0) & (predictions.prediction ==\\n1)].count()\\nfn = predictions[(predictions.label == 1) & (predictions.prediction ==\\n0)].count()\\nprint \"True Positives:\", tp\\nprint \"True Negatives:\", tn\\nprint \"False Positives:\", fp\\nprint \"False Negatives:\", fn\\n# False positive went from 18 to 16 (win) but False Negative jumped to 83\\n(opposite of win)\\nThe output is as follows:\\nTrue Positives: 67\\nTrue Negatives: 85178\\nFalse Positives: 16\\nFalse Negatives: 76\\nIt\\'s easy to see how Azure Databricks\\' environment makes it easy to utilize Spark and\\nMLlib to create scalable machine  learning pipelines that are similar in construction and\\nusage to scikit-learn.\\n\\nase study 3 – Using Azure Databricks to\\noptimize our hyperparameter tuning\\nOur final case study is the shortest and will showcase how we can combine the best of\\nscikit-learn, Spark, and Azure Databricks to build simple yet powerful machine learning\\nmodels. We will be using the MNIST dataset, which we used earlier, and we will be fitting\\na fairly simple RandomForestClassifier  to the data. The interesting bit will come when\\nwe import a third-party tool called spark_sklearn  to help us out.\\nThe aspects of Azure Databricks that we will be highlighting in this case study include\\nthese:\\nImporting third-party packages into our Azure Databricks environment\\nEnabling Spark\\'s parallelization within scikit-learn\\'s easy-to-use syntax\\nHow to add Python libraries to your cluster\\nUp until now, all of the packages that we have used come with the Azure Databricks\\nenvironment. Now we need to add a new package, called spark_sklearn . To add a third-\\nparty package to our Azure Databricks cluster, we simply click on Import Library  from our\\nmain dashboard, and we will see a window like the following:\\nType in any package that you wish to use in the PyPi Name ﬁeld\\n\\ne simply type in spark_sklearn , hit Install Library , and we are done! The cluster will\\nnow let us import the library from any existing or new notebook. Now, spark_sklearn  is\\na handy tool that allows the use of some  scikit-learn packages with the backend swapped\\nout for PySpark. This means that we can use existing code that we have already written,\\nand only have to tweak it slightly to make it compatible with Spark.\\nUsing spark_sklearn to build an MNIST classifier\\nWe have already seen that MNIST is a handwritten digit detection dataset. Without\\nspending too much time  on the data itself, let\\'s jump right into how we can use\\nspark_sklearn  for our benefit. Let\\'s bring in our data using the standard scikit-learn\\ndataset module:\\nfrom sklearn import datasets\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import GridSearchCV as original_grid_search\\ndigits = datasets.load_digits()\\nX, y = digits.data, digits.target\\nFrom here, we can import our handy GridSearchCV  module to do some hyperparameter\\ntuning:\\nparam_grid = {\"max_depth\": [3, None],\\n \"max_features\": [1, 3, 10],\\n \"min_samples_leaf\": [1, 3, 10],\\n \"bootstrap\": [True, False],\\n \"criterion\": [\"gini\", \"entropy\"],\\n \"n_estimators\": [10, 100, 1000]}\\ngs = original_grid_search(RandomForestClassifier(), param_grid=param_grid)\\ngs.fit(X, y)\\ngs.best_params_, gs.best score\\n({\\'bootstrap\\': False,\\n\\'criterion\\': \\'gini\\',\\n\\'max_depth\\': None,\\n\\'max_features\\': 3,\\n\\'min_samples_leaf\\': 1,\\n\\'n_estimators\\': 1000},\\n0.95436839176405119)\\nCommand took 24.69 minutes\\n\\nhe preceding code is a standard grid search that takes nearly 25 minutes to run. As in our\\nfirst case study, we could write a custom function to run this in parallel, but that would\\ntake a lot of custom code. As in our second case study, we use could MLlib to write a\\nscalable grid search using the MLlib standard models, but that would  take a while as well.\\nspark_sklearn  provides a third option. We can import their GridSearchCV  and swap\\nout our module for theirs to get extreme gains in speed with minimal code intervention:\\n# the new gridsearch module\\nfrom spark_sklearn import GridSearchCV as spark_grid_search\\n# the only difference is passing in the SparkContext objecr as the first\\nparameter of the grid search\\ngs = spark_grid_search(sc, RandomForestClassifier(), param_grid=param_grid)\\ngs.fit(X, y)\\ngs.best_params_, gs.bestscore\\n({\\'bootstrap\\': False,\\n\\'criterion\\': \\'gini\\',\\n\\'max_depth\\': None,\\n\\'max_features\\': 3,\\n\\'min_samples_leaf\\': 1,\\n\\'n_estimators\\': 100},\\n0.95436839176405119)\\nCommand took  5.29 minute\\nBy doing nothing more than importing a new grid search module and passing the\\nSparkContext  variable into the new  module, we can get a 5x speed boost. Always be on\\nthe lookout for third-party modules that can be used to enhance the already easy-to-use\\nAzure Databricks environment.\\nMore about spark_sklearn  can be found on their GitHub page: https:/\\n/\\u200bgithub. \\u200bcom/ \\u200bdatabricks/ \\u200bspark- \\u200bsklearn .\\n\\nummary\\nAzure Databricks provides an environment that allows us to create scalable machine\\nlearning pipelines with ease. By using the notebooks in Azure Databricks, we can run our\\ndata analytics and machine learning code in the cloud with powerful Azure resources in\\nthe backend. By implementing MLlib, we can create scalable machine learning pipelines\\nbehind Apache Spark. Utilizing third-party tools is a great way to bring everything together\\nto build extremely powerful learning algorithms and train them much faster than on our\\nlocal machines.\\n\\nther Books You May Enjoy\\nIf you enjoyed this book, you may be interested in these other books by Packt:\\nData Science Algorithms in a Week, Second Edition\\nDávid Natingga\\nISBN:  978-1-78980-607-6\\nUnderstand how to identify a data science problem correctly\\nImplement well-known machine learning algorithms efficiently using Python\\nClassify your datasets using Naive Bayes, decision trees, and random forest with\\naccuracy\\nDevise an appropriate prediction solution using regression\\nWork with time series data to identify relevant data events and trends\\nCluster your data using the k-means algorithm\\n\\nPractical Data Science Cookbook, Second Edition\\nTony Ojeda, Sean Patrick Murphy, Et al\\nISBN: 978-1-78712-962-7\\nLearn and understand the installation procedure and environment required for R\\nand Python on various platforms\\nPrepare data for analysis by implement various data science concepts such as\\nacquisition, cleaning and munging through R and Python\\nBuild a predictive model and an exploratory model\\nAnalyze the results of your model and create reports on the acquired data\\nBuild various tree-based methods and Build random forest\\n\\neave a review - let other readers know what\\nyou think\\nPlease share your thoughts on this book with others by leaving a review on the site that you\\nbought it from. If you purchased the book from Amazon, please leave us an honest review\\non this book\\'s Amazon page.  This is vital so that other potential readers can see and use\\nyour unbiased opinion to make purchasing decisions, we can understand what our\\ncustomers think about our products, and our authors can see your feedback on the title that\\nthey have worked with Packt to create. It will only take a few minutes of your time, but is\\nvaluable to other potential customers, our authors, and Packt. Thank you!\\n\\nndex\\nA\\nAdam Optimizer  374\\narithmetic mean  48, 144\\narithmetic symbols\\n   about  78\\n   dot product  79, 81\\n   proportional  79\\n   summation  78\\nAutoRegressive Integrated Moving Average\\n(ARIMA)  355\\naverage observation  44\\nB\\nback-propagation  332\\nbar charts  193\\nBayes\\' formula  79\\nBayes\\' theorem  112, 113, 115\\n   applications  117\\n   example  117, 119\\nBayesian approach\\n   about  97\\n   versus Frequentist approach  97\\nBayesian\\n   about  113\\nbias/variance tradeoff\\n   about  298\\n   error functions  309, 310\\n   errors, due to bias  298\\n   errors, due to variance  298, 299\\n   example  299, 300, 301, 302, 303, 304, 306,\\n307\\n   overfitting  308\\n   underfitting  308\\nbig data  21\\nbinary classifier  110\\nbinomial random variable  127, 129Bootstrap aggregation (bagging)  323\\nbox plots  197, 199\\nC\\nCartesian graph  82\\ncausation  201\\nchi-square goodness of fit test\\n   about  182\\n   assumptions  182\\n   example  183\\n   for association/independence  185\\nchi-square independence test\\n   assumptions  185\\nclassification  219\\nclassification route  353, 354, 355\\nclassification tree\\n   fitting  266, 268, 270, 271\\ncluster validation\\n   optimal number, selecting for  282\\nclustering  221\\ncoefficient of variation  149\\ncollectively exhaustive\\n   about  112\\n   events  112\\ncomma separated value (CSV)  39\\ncommunication matter  189\\ncompound events  101\\nconditional probability  104\\nconfidence intervals  170, 172, 174\\nconfounding factor  142\\nconfusion matrices  110\\nconfusion matrix  110\\ncontinuous random variable  133, 136\\ncorrelation  201\\ncorrelation coefficients  157\\ncorrelation\\n   causation, lacking  206\\n\\n  versus causation  201, 203\\ncross-validation error\\n   versus training error  318, 320\\nD\\ndata levels\\n   about  42\\n   checking  46\\n   interval level  47\\n   nominal level  43\\n   ordinal level  44\\n   ratio level  51\\ndata pre-processing\\n   example  34\\n   special characters  35\\n   text, relative length  36\\n   topics, picking out  36\\n   word/phrase counts  35\\ndata science, case studies\\n   about  22\\n   dollars, marketing  25\\n   government paper pushing, automating  23\\n   job description  27, 29\\n   performance  24\\ndata science\\n   about  9, 55\\n   computer programming  12, 16\\n   consideration  10\\n   data mining  21\\n   data, exploring  56\\n   data, modeling  57\\n   data, obtaining  56\\n   domain knowledge  12, 20\\n   exploratory data analysis (EDA)  21\\n   machine learning  21\\n   math  13\\n   math/statistics  12\\n   organized data  9\\n   overview  56\\n   Python  16\\n   question  56\\n   results, communicating  57\\n   results, visualizing  57\\n   terminology  9, 21\\n   unorganized data  9   Venn diagram  12, 13\\n   xyz123 Technologies  11\\ndata\\n   about  42, 53\\n   experimental  139\\n   experimentation  139\\n   exploring  57, 58\\n   observational  139\\n   obtaining  139\\n   probability sampling  142\\n   random sampling  142\\n   sampling  139, 141\\n   titanic dataset  68\\n   types  32, 33\\n   unequal probability sampling  143\\n   yelp dataset  59\\nDataFrame  61\\ndecision trees\\n   about  264, 266\\n   classification tree, fitting  266, 268, 270, 271\\n   comparing, with random forests  329\\n   regression tree, building  266\\nDeep Neural Network Classifier  368\\ndimension reduction  221\\ndiscrete random variable\\n   about  121, 124, 126\\n   binomial random variables  127, 129\\n   continuous random variable  133, 136\\n   geometric random variable  129\\n   poisson random variable  131\\n   types  127\\nDNNClassifier  368\\ndomain knowledge  20\\ndummy variables  248, 249, 250, 251, 252\\nE\\neffective visualizations\\n   identifying  189\\nempirical rule  159, 160\\nensembling\\n   about  320, 321, 322\\n   random forests  323, 324, 325, 328\\nerror functions  309, 310\\nEuler\\'s number  241\\nevent  96\\n\\nxploratory data analysis (EDA)  338, 345, 349,\\n350\\n   about  339, 340, 341, 343\\n   classification route  353, 354, 355\\n   exploratory data analysis  339\\n   regression route  351\\nexponent  83, 84\\nF\\nfalse positive  181\\nfeature extraction  284, 286, 288, 291, 293, 295\\nFrequentist approach\\n   about  97, 98\\n   law of large num  99\\n   versus Bayesian approach  97\\nG\\ngeometric mean  51\\ngeometric random variable  130\\ngradient descent  365\\ngraphs  82, 201\\ngrid searching\\n   about  315, 316, 317, 318\\n   training error, versus cross-validation error  318,\\n320\\nH\\nhistograms  195\\nhypothesis test  175\\n   conducting  176\\n   one sample t-tests  177\\nhypothesis tests\\n   chi-square goodness of fit test  182\\n   for categorical variables  182\\n   type I errors  181\\n   type II errors  181\\nI\\nineffective visualizations\\n   identifying  189\\ninterval level\\n   about  47\\n   example  47\\n   mathematical operations  48\\n   measures of center  48   measures of variation  49\\nJ\\nJaccard measure  89\\nK\\nk folds cross-validation\\n   about  310, 311, 313, 314\\n   features  312\\nk-fold cross validation  367\\nk-means clustering\\n   about  272, 274\\n   beer, illustrative example  279, 281\\n   data points, illustrative example  274, 276, 278\\nK-Nearest Neighbors (KNN) algorithm  310\\nK\\n   optimal number, selecting for  282\\nkey performance indicator (KPI)  206\\nL\\nlabeled data  215\\nlikelihood  256\\nlikert  44\\nLikert scale  124\\nline graphs  191\\nlinear algebra  82\\n   about  90\\n   matrix multiplication  90\\nlinear regression  226, 227, 228, 229, 230, 231\\n   predictors, adding  231, 232, 233, 234\\n   regression metrics  234, 235, 237, 239, 240\\nlog odds  242, 243, 244, 245\\nlogarithm  83, 84\\nlogistic regression  240, 241, 242\\n   math  245, 247, 248\\nM\\nmachine learning\\n   about  15, 211\\n   data  220\\n   facial recognition  211\\n   limitations  213\\n   predictions  218\\n   probabilistic model  21\\n\\n  reinforcement learning  225\\n   statistical model  21\\n   supervised learning  215\\n   supervised learning, example  216, 217\\n   supervised learning, types  219\\n   supervised machine learning  224\\n   types  214, 215\\n   types, overview  224\\n   unsupervised learning  221, 222\\n   unsupervised machine learning  225\\n   working  214\\nmath  74\\n   about  14\\n   example  14\\nmathematics  74, 75\\n   symbols  75\\n   terminology  75\\nmatrices\\n   about  75\\n   answers  78\\n   multiplication  91, 93, 94\\nmatrix  76\\nmatrix multiplication  90\\nmean  48\\nMean Squared Error (MSE)  266\\nmeasures of center  43\\nmeasures of relative standing  151\\n   correlations, in data  156\\nmeasures of variation  49, 151\\nmeasures of variation, interval level\\n   standard deviation  49\\nmeasures of variation, statistics\\n   defining  149\\n   example  150\\nmedian  45, 145\\nmicrosoft data science environment\\n   about  375\\n   Microsoft Azure  375\\n   Microsoft Azure Machine Learning Studio  375\\n   Microsoft Databricks  375\\nMicrosoft Databricks\\n   about  376\\n   bike-sharing usage prediction, parallelization in\\nDatabricks  378, 379, 380, 381, 382, 383,\\n385, 386, 387, 389, 390, 391   cluster, setting up  377\\n   clusters/workers  376\\n   MLib  376\\n   MLlib Grid Search module, used to tune\\nhyperparameters  399, 402\\n   MLlib, used in Databricks to predict credit card \\n391, 392, 393, 394, 395, 397, 399\\n   Notebooks  376\\n   Python libraries, adding to cluster  403\\n   reference link  376\\n   Spark DataFrames  376\\n   spark_sklearn, used to build MNIST classifier \\n404, 405\\n   used, to optimize hyperparameter tuning  403,\\n404\\n   using  376\\nmodel coefficients  229\\nmodels  13\\nmultilayer perceptron (MLP)  332\\nmutually exhaustive  112\\nN\\nNaive Bayes\\n   classification  255, 259, 262, 263\\nNaïve Bayes algorithm  112\\nneural networks\\n   about  329\\n   basic structure  330\\n   for anomaly detection  330\\n   for entity movement  330\\n   for pattern recognition  330\\n   structure  331, 332, 333, 335, 336\\nnominal level  60\\n   about  43\\n   data  44\\n   mathematical operations  43\\n   measure of center  43\\nnormalizing constant  256\\nnotation  96\\nnull model  239\\nO\\nodds  242, 243, 244, 245\\none sample t-tests\\n   about  177\\n\\n  assumptions  178, 180\\n   example  178\\nordinal level  44, 60\\n   examples  44\\n   mathematical operations  44\\n   measure of center  45\\nout-of-sample (OOS) error  311\\noverfitting  308\\nP\\nparameter  138\\nperceptrons  330\\npoint estimates  162, 165, 166\\npoisson random variable  131\\npre-processing  34\\nprediction  217\\npredictive analytics models  215\\npresentation\\n   strategy  208\\nprincipal component analysis  284, 286, 288, 291,\\n293, 295\\nPrincipal Component Analysis (PCA)  288\\nprior probability  256\\nprobability  96, 242, 243, 244, 245\\nprobability density function (PDF)  133\\nprobability mass function (PMF)  122, 127\\nprobability\\n   addition rule  104\\n   complementary events  108\\n   event independence  108\\n   multiplication rule  107\\n   mutual exclusivity  106\\n   rules  104\\nprocedure  95\\nproper subset  88\\nPySpark  376\\nPython\\n   about  16\\n   example  18\\n   practices  17\\n   single tweet, parsing  19\\nQ\\nqualitative data\\n   about  37   example  37, 39\\n   exploration tips  62\\n   filtering in pandas  64\\n   nominal level columns  62\\n   ordinal level columns  67\\n   versus quantitative data  36\\nquantitative data\\n   about  37\\n   continuous data  41\\n   discrete data  41\\n   example  37, 39\\n   overview  41\\nR\\nrandom forests\\n   about  323, 324, 325, 328\\n   comparing, with decision trees  329\\nrandom variable  120\\n   discrete random variable  121, 124, 126\\nrandom variables  112\\nratio level\\n   about  51\\n   example  51\\n   measures of center  51\\n   problems  52\\nregression  219\\nregression metrics  234, 235, 237, 239, 240\\nregression route  351\\nregression tree\\n   building  266\\nreinforcement learning  223, 224, 225\\nrelative frequency  98\\nroot-mean-square error (RMSE)  351\\nS\\nsample space  96\\nsampling bias  142\\nsampling distributions  167, 169\\nscalar  80\\nscatter plots  190\\nset theory  86\\nSilhouette Coefficient  282, 284\\nsimpson\\'s paradox  204\\nsocial media\\n   exploratory data analysis (EDA)  340, 341, 343,\\n\\n45, 349, 350\\n   survey  356, 357, 359, 360, 362, 363\\nSpark  376\\nspark_sklearn\\n   reference link  405\\nsquare matrix  77\\nstandard deviation  146\\nstandard normal distribution  133\\nstatistical modeling  226\\nstatistics  137, 201\\n   measures of center  144\\n   measures of relative standing  150, 153, 156\\n   measures of variation  145, 148\\n   measuring  144\\nstock price\\n   example  355\\nstock prices\\n   predicting, on social media  338\\n   text sentiment analysis  338, 339\\nstructured data\\n   about  33\\n   versus unstructured data  33\\nsuperset  87\\nsupervised learning  215\\nsupervised learning models  271\\nsupervised learning\\n   classification  219\\n   regression  219\\n   types  219\\nsupervised machine learning  224\\nT\\nTensorFlow\\n   about  368, 369, 370, 371, 372, 374\\n   neural networks  368, 369, 370, 371, 372, 374   using  363, 364, 366, 367\\ntitanic dataset  68\\ntraining error\\n   versus cross-validation error  318, 320\\nU\\nunderfitting  308\\nunstructured data\\n   about  33\\n   versus structured data  33\\nunsupervised learning  222\\n   about  221, 271\\n   reinforcement learning  223, 224\\n   using  271\\nunsupervised machine learning  225\\nV\\nvectors\\n   about  75\\n   answers  78\\n   exercises  78\\nverbal communication\\n   about  206\\n   data findings, presentation  207\\nW\\nWorld Health Organization (WHO)  39\\nY\\nyelp dataset\\n   about  59\\n   DataFrame  61\\n   qualitative data, exploration tips  62\\n   series  62'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principles_of_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking the textbook with and without overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the text into chunks of a maximum number of tokens.\n",
    "import re\n",
    "import tiktoken\n",
    "# from tokenizers import BertWordPieceTokenizer\n",
    "# tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "\n",
    "def overlapping_chunks(text, max_tokens=500, overlapping_factor=5):\n",
    "    \"\"\" \n",
    "    max_tokens : tokens we want per chunk\n",
    "    overlapping_factor : number of sentences to start each chunk with that overlaps with the previous chunk\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the text using punctuation\n",
    "    sentences = re.split(r'[.?!]', text)\n",
    "    \n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    chunks, tokens_so_far, chunk = [], 0, []\n",
    "    \n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "        \n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            \n",
    "            if overlapping_factor > 0:\n",
    "                chunk = chunk[-overlapping_factor:]\n",
    "                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n",
    "            else:\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "                \n",
    "        # If the number of tokens in the current sentence is greater than the max number of\n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "        \n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        \n",
    "        tokens_so_far += token + 1\n",
    "    \n",
    "    if chunk:\n",
    "        chunk.append(\". \".join(chunk) + \".\")\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping chunking approach has 286 document with average length 474.2 tokens\n"
     ]
    }
   ],
   "source": [
    "split = overlapping_chunks(principles_of_ds, overlapping_factor=0)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'non-overlapping chunking approach has {len(split)} document with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlapping chunking approach has 393 documents with average length 485.6 tokens\n"
     ]
    }
   ],
   "source": [
    "# with 5 overlapping sentences per chunk\n",
    "\n",
    "split = overlapping_chunks(principles_of_ds, overlapping_factor=5)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking the textbook with natural whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 82243), ('\\n', 9220), ('  ', 1592), ('\\n\\n', 339), ('\\n   ', 250)]\n"
     ]
    }
   ],
   "source": [
    "# Import the Counter and re libraries\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Find all occurences of one or more spaces in 'principles_of_ds'\n",
    "matches = re.findall(r'[\\s]{1,}', principles_of_ds)\n",
    "\n",
    "# The 5 most frequent spaces that occur in the document\n",
    "most_common_spaces = Counter(matches).most_common(5)\n",
    "\n",
    "# Print the most common spaces and their frequencies\n",
    "print(most_common_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering pages of the document by semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# create embeddings\n",
    "embeddings = None\n",
    "for s in tqdm(range(0, len(split), 100)):\n",
    "    if embeddings is None:\n",
    "        embeddings = np.array(get_embeddings(split[s:s+100], engine=ENGINE))\n",
    "    else:\n",
    "        embeddings = np.vstack([embeddings, np.array(get_embeddings(split[s:s+100], engine=ENGINE))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 2 embeddings\n",
      "Cluster 1: 4 embeddings\n",
      "Cluster 2: 2 embeddings\n",
      "Cluster 3: 2 embeddings\n",
      "Cluster 4: 2 embeddings\n",
      "Cluster 5: 2 embeddings\n",
      "Cluster 6: 2 embeddings\n",
      "Cluster 7: 2 embeddings\n",
      "Cluster 8: 3 embeddings\n",
      "Cluster 9: 2 embeddings\n",
      "Cluster 10: 2 embeddings\n",
      "Cluster 11: 2 embeddings\n",
      "Cluster 12: 4 embeddings\n",
      "Cluster 13: 2 embeddings\n",
      "Cluster 14: 2 embeddings\n",
      "Cluster 15: 2 embeddings\n",
      "Cluster 16: 2 embeddings\n",
      "Cluster 17: 2 embeddings\n",
      "Cluster 18: 2 embeddings\n",
      "Cluster 19: 2 embeddings\n",
      "Cluster 20: 2 embeddings\n",
      "Cluster 21: 2 embeddings\n",
      "Cluster 22: 2 embeddings\n",
      "Cluster 23: 2 embeddings\n",
      "Cluster 24: 2 embeddings\n",
      "Cluster 25: 2 embeddings\n",
      "Cluster 26: 2 embeddings\n",
      "Cluster 27: 2 embeddings\n",
      "Cluster 28: 2 embeddings\n",
      "Cluster 29: 2 embeddings\n",
      "Cluster 30: 2 embeddings\n",
      "Cluster 31: 4 embeddings\n",
      "Cluster 32: 2 embeddings\n",
      "Cluster 33: 2 embeddings\n",
      "Cluster 34: 1 embeddings\n",
      "Cluster 35: 2 embeddings\n",
      "Cluster 36: 2 embeddings\n",
      "Cluster 37: 2 embeddings\n",
      "Cluster 38: 2 embeddings\n",
      "Cluster 39: 3 embeddings\n",
      "Cluster 40: 2 embeddings\n",
      "Cluster 41: 2 embeddings\n",
      "Cluster 42: 1 embeddings\n",
      "Cluster 43: 2 embeddings\n",
      "Cluster 44: 2 embeddings\n",
      "Cluster 45: 2 embeddings\n",
      "Cluster 46: 2 embeddings\n",
      "Cluster 47: 1 embeddings\n",
      "Cluster 48: 2 embeddings\n",
      "Cluster 49: 3 embeddings\n",
      "Cluster 50: 2 embeddings\n",
      "Cluster 51: 1 embeddings\n",
      "Cluster 52: 2 embeddings\n",
      "Cluster 53: 2 embeddings\n",
      "Cluster 54: 2 embeddings\n",
      "Cluster 55: 2 embeddings\n",
      "Cluster 56: 2 embeddings\n",
      "Cluster 57: 2 embeddings\n",
      "Cluster 58: 2 embeddings\n",
      "Cluster 59: 2 embeddings\n",
      "Cluster 60: 1 embeddings\n",
      "Cluster 61: 2 embeddings\n",
      "Cluster 62: 1 embeddings\n",
      "Cluster 63: 3 embeddings\n",
      "Cluster 64: 3 embeddings\n",
      "Cluster 65: 2 embeddings\n",
      "Cluster 66: 2 embeddings\n",
      "Cluster 67: 2 embeddings\n",
      "Cluster 68: 1 embeddings\n",
      "Cluster 69: 1 embeddings\n",
      "Cluster 70: 1 embeddings\n",
      "Cluster 71: 1 embeddings\n",
      "Cluster 72: 2 embeddings\n",
      "Cluster 73: 1 embeddings\n",
      "Cluster 74: 1 embeddings\n",
      "Cluster 75: 1 embeddings\n",
      "Cluster 76: 1 embeddings\n",
      "Cluster 77: 1 embeddings\n",
      "Cluster 78: 1 embeddings\n",
      "Cluster 79: 1 embeddings\n",
      "Cluster 80: 1 embeddings\n",
      "Cluster 81: 1 embeddings\n",
      "Cluster 82: 2 embeddings\n",
      "Cluster 83: 2 embeddings\n",
      "Cluster 84: 1 embeddings\n",
      "Cluster 85: 1 embeddings\n",
      "Cluster 86: 1 embeddings\n",
      "Cluster 87: 1 embeddings\n",
      "Cluster 88: 2 embeddings\n",
      "Cluster 89: 2 embeddings\n",
      "Cluster 90: 2 embeddings\n",
      "Cluster 91: 2 embeddings\n",
      "Cluster 92: 1 embeddings\n",
      "Cluster 93: 1 embeddings\n",
      "Cluster 94: 2 embeddings\n",
      "Cluster 95: 1 embeddings\n",
      "Cluster 96: 1 embeddings\n",
      "Cluster 97: 1 embeddings\n",
      "Cluster 98: 2 embeddings\n",
      "Cluster 99: 1 embeddings\n",
      "Cluster 100: 2 embeddings\n",
      "Cluster 101: 2 embeddings\n",
      "Cluster 102: 2 embeddings\n",
      "Cluster 103: 1 embeddings\n",
      "Cluster 104: 1 embeddings\n",
      "Cluster 105: 4 embeddings\n",
      "Cluster 106: 1 embeddings\n",
      "Cluster 107: 1 embeddings\n",
      "Cluster 108: 1 embeddings\n",
      "Cluster 109: 1 embeddings\n",
      "Cluster 110: 1 embeddings\n",
      "Cluster 111: 2 embeddings\n",
      "Cluster 112: 2 embeddings\n",
      "Cluster 113: 1 embeddings\n",
      "Cluster 114: 1 embeddings\n",
      "Cluster 115: 2 embeddings\n",
      "Cluster 116: 1 embeddings\n",
      "Cluster 117: 1 embeddings\n",
      "Cluster 118: 1 embeddings\n",
      "Cluster 119: 2 embeddings\n",
      "Cluster 120: 2 embeddings\n",
      "Cluster 121: 1 embeddings\n",
      "Cluster 122: 1 embeddings\n",
      "Cluster 123: 1 embeddings\n",
      "Cluster 124: 2 embeddings\n",
      "Cluster 125: 1 embeddings\n",
      "Cluster 126: 1 embeddings\n",
      "Cluster 127: 2 embeddings\n",
      "Cluster 128: 1 embeddings\n",
      "Cluster 129: 1 embeddings\n",
      "Cluster 130: 1 embeddings\n",
      "Cluster 131: 1 embeddings\n",
      "Cluster 132: 1 embeddings\n",
      "Cluster 133: 1 embeddings\n",
      "Cluster 134: 1 embeddings\n",
      "Cluster 135: 2 embeddings\n",
      "Cluster 136: 1 embeddings\n",
      "Cluster 137: 1 embeddings\n",
      "Cluster 138: 1 embeddings\n",
      "Cluster 139: 1 embeddings\n",
      "Cluster 140: 1 embeddings\n",
      "Cluster 141: 1 embeddings\n",
      "Cluster 142: 1 embeddings\n",
      "Cluster 143: 1 embeddings\n",
      "Cluster 144: 1 embeddings\n",
      "Cluster 145: 1 embeddings\n",
      "Cluster 146: 2 embeddings\n",
      "Cluster 147: 1 embeddings\n",
      "Cluster 148: 1 embeddings\n",
      "Cluster 149: 1 embeddings\n",
      "Cluster 150: 1 embeddings\n",
      "Cluster 151: 1 embeddings\n",
      "Cluster 152: 1 embeddings\n",
      "Cluster 153: 1 embeddings\n",
      "Cluster 154: 1 embeddings\n",
      "Cluster 155: 1 embeddings\n",
      "Cluster 156: 1 embeddings\n",
      "Cluster 157: 1 embeddings\n",
      "Cluster 158: 1 embeddings\n",
      "Cluster 159: 1 embeddings\n",
      "Cluster 160: 1 embeddings\n",
      "Cluster 161: 1 embeddings\n",
      "Cluster 162: 1 embeddings\n",
      "Cluster 163: 1 embeddings\n",
      "Cluster 164: 1 embeddings\n",
      "Cluster 165: 1 embeddings\n",
      "Cluster 166: 1 embeddings\n",
      "Cluster 167: 1 embeddings\n",
      "Cluster 168: 1 embeddings\n",
      "Cluster 169: 1 embeddings\n",
      "Cluster 170: 1 embeddings\n",
      "Cluster 171: 1 embeddings\n",
      "Cluster 172: 1 embeddings\n",
      "Cluster 173: 1 embeddings\n",
      "Cluster 174: 1 embeddings\n",
      "Cluster 175: 1 embeddings\n",
      "Cluster 176: 1 embeddings\n",
      "Cluster 177: 2 embeddings\n",
      "Cluster 178: 2 embeddings\n",
      "Cluster 179: 1 embeddings\n",
      "Cluster 180: 1 embeddings\n",
      "Cluster 181: 1 embeddings\n",
      "Cluster 182: 1 embeddings\n",
      "Cluster 183: 1 embeddings\n",
      "Cluster 184: 1 embeddings\n",
      "Cluster 185: 1 embeddings\n",
      "Cluster 186: 1 embeddings\n",
      "Cluster 187: 1 embeddings\n",
      "Cluster 188: 1 embeddings\n",
      "Cluster 189: 1 embeddings\n",
      "Cluster 190: 1 embeddings\n",
      "Cluster 191: 1 embeddings\n",
      "Cluster 192: 1 embeddings\n",
      "Cluster 193: 1 embeddings\n",
      "Cluster 194: 1 embeddings\n",
      "Cluster 195: 1 embeddings\n",
      "Cluster 196: 1 embeddings\n",
      "Cluster 197: 1 embeddings\n",
      "Cluster 198: 1 embeddings\n",
      "Cluster 199: 1 embeddings\n",
      "Cluster 200: 1 embeddings\n",
      "Cluster 201: 1 embeddings\n",
      "Cluster 202: 2 embeddings\n",
      "Cluster 203: 1 embeddings\n",
      "Cluster 204: 1 embeddings\n",
      "Cluster 205: 1 embeddings\n",
      "Cluster 206: 1 embeddings\n",
      "Cluster 207: 1 embeddings\n",
      "Cluster 208: 1 embeddings\n",
      "Cluster 209: 1 embeddings\n",
      "Cluster 210: 1 embeddings\n",
      "Cluster 211: 1 embeddings\n",
      "Cluster 212: 1 embeddings\n",
      "Cluster 213: 1 embeddings\n",
      "Cluster 214: 1 embeddings\n",
      "Cluster 215: 1 embeddings\n",
      "Cluster 216: 1 embeddings\n",
      "Cluster 217: 1 embeddings\n",
      "Cluster 218: 1 embeddings\n",
      "Cluster 219: 1 embeddings\n",
      "Cluster 220: 1 embeddings\n",
      "Cluster 221: 1 embeddings\n",
      "Cluster 222: 1 embeddings\n",
      "Cluster 223: 1 embeddings\n",
      "Cluster 224: 2 embeddings\n",
      "Cluster 225: 1 embeddings\n",
      "Cluster 226: 1 embeddings\n",
      "Cluster 227: 1 embeddings\n",
      "Cluster 228: 1 embeddings\n",
      "Cluster 229: 1 embeddings\n",
      "Cluster 230: 1 embeddings\n",
      "Cluster 231: 1 embeddings\n",
      "Cluster 232: 1 embeddings\n",
      "Cluster 233: 1 embeddings\n",
      "Cluster 234: 1 embeddings\n",
      "Cluster 235: 1 embeddings\n",
      "Cluster 236: 1 embeddings\n",
      "Cluster 237: 1 embeddings\n",
      "Cluster 238: 1 embeddings\n",
      "Cluster 239: 1 embeddings\n",
      "Cluster 240: 1 embeddings\n",
      "Cluster 241: 1 embeddings\n",
      "Cluster 242: 1 embeddings\n",
      "Cluster 243: 1 embeddings\n",
      "Cluster 244: 1 embeddings\n",
      "Cluster 245: 1 embeddings\n",
      "Cluster 246: 1 embeddings\n",
      "Cluster 247: 1 embeddings\n",
      "Cluster 248: 1 embeddings\n",
      "Cluster 249: 1 embeddings\n",
      "Cluster 250: 1 embeddings\n",
      "Cluster 251: 1 embeddings\n",
      "Cluster 252: 1 embeddings\n",
      "Cluster 253: 1 embeddings\n",
      "Cluster 254: 1 embeddings\n",
      "Cluster 255: 1 embeddings\n",
      "Cluster 256: 1 embeddings\n",
      "Cluster 257: 1 embeddings\n",
      "Cluster 258: 1 embeddings\n",
      "Cluster 259: 1 embeddings\n",
      "Cluster 260: 1 embeddings\n",
      "Cluster 261: 1 embeddings\n",
      "Cluster 262: 1 embeddings\n",
      "Cluster 263: 1 embeddings\n",
      "Cluster 264: 1 embeddings\n",
      "Cluster 265: 1 embeddings\n",
      "Cluster 266: 1 embeddings\n",
      "Cluster 267: 1 embeddings\n",
      "Cluster 268: 1 embeddings\n",
      "Cluster 269: 1 embeddings\n",
      "Cluster 270: 1 embeddings\n",
      "Cluster 271: 1 embeddings\n",
      "Cluster 272: 1 embeddings\n",
      "Cluster 273: 1 embeddings\n",
      "Cluster 274: 1 embeddings\n",
      "Cluster 275: 1 embeddings\n",
      "Cluster 276: 1 embeddings\n",
      "Cluster 277: 1 embeddings\n",
      "Cluster 278: 1 embeddings\n",
      "Cluster 279: 1 embeddings\n",
      "Cluster 280: 1 embeddings\n",
      "Cluster 281: 1 embeddings\n",
      "Cluster 282: 1 embeddings\n",
      "Cluster 283: 1 embeddings\n",
      "Cluster 284: 1 embeddings\n",
      "Cluster 285: 1 embeddings\n",
      "Cluster 286: 1 embeddings\n",
      "Cluster 287: 1 embeddings\n",
      "Cluster 288: 1 embeddings\n",
      "Cluster 289: 1 embeddings\n",
      "Cluster 290: 1 embeddings\n",
      "Cluster 291: 1 embeddings\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Assume you have a list of text embeddings called 'embeddings' \n",
    "# First, compute the cosine similarity matrix between all pairs of embeddings\n",
    "cosine_sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Instantiate the AgglomerativeClustering model\n",
    "agg_clustering = AgglomerativeClustering(\n",
    "    n_clusters=None,            # The algorithm will determine the optimal number of clusters based on the data\n",
    "    distance_threshold=0.1,     # Clusters will be formed until all pairwise distances between clusters are greater than 0.1\n",
    "    # affinity='precomputed',     # We are providing a precomputed distance matrix (1 - similarity matrix) as input # deprecated\n",
    "    metric='precomputed',\n",
    "    linkage='complete'          # form clusters by iteratively merging the smallest clusters based on the maximum distance between their components\n",
    ")\n",
    "\n",
    "# Fit the model to the cosine distance matrix (1 - similarity matrix)\n",
    "agg_clustering.fit(1 - cosine_sim_matrix)\n",
    "\n",
    "# Get the cluster labels from each embedding\n",
    "cluster_labels = agg_clustering.labels_\n",
    "\n",
    "# print the number of embeddings in each cluster\n",
    "unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f'Cluster {label}: {count} embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
