{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\woojin\\AppData\\Local\\Temp\\ipykernel_25780\\2542505225.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, openai_api_key=OPENAI_API_KEY)\n",
      "C:\\Users\\woojin\\AppData\\Local\\Temp\\ipykernel_25780\\2542505225.py:21: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "C:\\Users\\woojin\\AppData\\Local\\Temp\\ipykernel_25780\\2542505225.py:27: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(input_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main advantage of quantum computing over classical computing?\n",
      "Answser: The main advantage of quantum computing over classical computing is the ability to solve complex problems faster.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import keyring\n",
    "\n",
    "OPENAI_API_KEY = keyring.get_password('openai', 'key_for_windows')\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below.\n",
    "If the question cannot be answered using the information provided, answer with \"I don't know\". \n",
    "Context: Quantum conputing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
    "...  \n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# set the query you want to ask\n",
    "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
    "\n",
    "# run the LLMChain to get the AI-generated answer\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Question:\", input_data['query'])\n",
    "print(\"Answser:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tropical rainforests, grasslands, and mangrove swamps\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "examples = [\n",
    "    {\"animal\":\"lion\", \"habitat\":\"savanna\"},\n",
    "    {\"animal\":\"polar bear\", \"habitat\":\"Artic ice\"},\n",
    "    {\"animal\":\"elephant\", \"habitat\":\"African grasslands\"}\n",
    "]\n",
    "example_template = \"\"\"\n",
    "Animal: {animal}\n",
    "Habitat: {habitat}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['animal', 'habitat'],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Identify the habitat of the given animal\",\n",
    "    suffix=\"Animal: {input}\\n\\nHabitat:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# create the LLMChain for the few_shot_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "\n",
    "# run the LLMChain with input data\n",
    "input_data = {\"input\": \"tiger\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.save(\"awesome_prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.save(\"awesome_prompt.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='Answer the question based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\". \\nContext: Quantum conputing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\\n...  \\nQuestion: {query}\\nAnswer:')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt('awesome_prompt.json')\n",
    "loaded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just watch some YouTube tutorials and hope for the best.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck: it works wondes.\"\n",
    "    }, {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative and funny responses to users's questions. Here are some\n",
    "exmaples:\"\"\"\n",
    "\n",
    "suffix = \"\"\" \n",
    "User : {query}\n",
    "AI:\"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=['query'],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# create the LLMChain for the few_shot_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# run the LLMChain with input_data\n",
    "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do you feel today?\",\n",
    "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
    "    }, {\n",
    "        \"query\": \"What is the speed of light?\",\n",
    "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
    "    }, {\n",
    "        \"query\": \"What is a quantum computer?\",\n",
    "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
    "    }, {\n",
    "        \"query\": \"Who invented the telephone?\",\n",
    "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
    "    }, {\n",
    "        \"query\": \"What programming language is best for AI development?\",\n",
    "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is photosynthesis?\",\n",
    "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
    "    }, {\n",
    "        \"query\": \"What is the tallest mountain on Earth?\",\n",
    "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the most abundant element in the universe?\",\n",
    "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the largest mammal on Earth?\",\n",
    "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the fastest land animal?\",\n",
    "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the square root of 144?\",\n",
    "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the average temperature on Mars?\",\n",
    "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander Graham Bell, the man who made it possible for you to call your ex at 2am.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=['query'],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# create the LLMChain for the dynamic_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
    "\n",
    "# run the LLMChain with input_data\n",
    "input_data = {\"query\": \"Who invented the telephone?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompt and Examples Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Human/AI Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love programmin', matey.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "template = \"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The secret to happiness is a good internet connection and unlimited access to cat videos.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the secret to happiness?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the antonym of every input\n",
      "\n",
      " \n",
      "Word: happy\n",
      "Antonym : sad\n",
      "\n",
      "\n",
      " \n",
      "Word: tall\n",
      "Antonym : short\n",
      "\n",
      "\n",
      " \n",
      "Word: energetic\n",
      "Antonym : lethargic\n",
      "\n",
      "\n",
      "Word: big\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\" \n",
    "Word: {word}\n",
    "Antonym : {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=example,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give me the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator='\\n\\n'\n",
    ")\n",
    "\n",
    "print(dynamic_prompt.format(input='big'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The secret to happiness is a good internet connection and unlimited access to cat videos.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the secret to happiness?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\woojin\\AppData\\Local\\Temp\\ipykernel_25780\\3284201311.py:31: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model='text-embedding-ada-002', api_key=OPENAI_API_KEY)\n",
      "Creating 5 embeddings in 1 batches of size 5:: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (5, 1)      str     None   \n",
      " metadata     json      (5, 1)      str     None   \n",
      " embedding  embedding  (5, 1536)  float32   None   \n",
      "    id        text      (5, 1)      str     None   \n",
      "Conver the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 10 °C\n",
      "Output: 50 °F\n",
      "\n",
      "Input: 10 °C\n",
      "Output:\n",
      "Conver the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 30 °C\n",
      "Output: 86 °F\n",
      "\n",
      "Input: 30 °C\n",
      "Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1 embeddings in 1 batches of size 1:: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (6, 1)      str     None   \n",
      " metadata     json      (6, 1)      str     None   \n",
      " embedding  embedding  (6, 1536)  float32   None   \n",
      "    id        text      (6, 1)      str     None   \n",
      "Conver the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 40 °C\n",
      "Output: 104 °F\n",
      "\n",
      "Input: 40 °C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# create a PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "# define some examples\n",
    "examples = [\n",
    "    {\"input\": \"0 °C\", \"output\": \"32 °F\"},\n",
    "    {\"input\": \"10 °C\", \"output\": \"50 °F\"},\n",
    "    {\"input\": \"20 °C\", \"output\": \"68 °F\"},\n",
    "    {\"input\": \"30 °C\", \"output\": \"86 °F\"},\n",
    "    {\"input\": \"40 °C\", \"output\": \"104 °F\"},\n",
    "]\n",
    "\n",
    "# create Deep Lake dataset\n",
    "ACTIVELOOP_API_KEY = keyring.get_password('activeloop', 'key_for_windows')\n",
    "my_activation_org_id = 'ahn283'\n",
    "my_activation_dataset_name = 'langchain_course_fewshot_selector'\n",
    "dataset_path = f\"hub://{my_activation_org_id}/{my_activation_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, token=ACTIVELOOP_API_KEY)\n",
    "\n",
    "# embedding function\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002', api_key=OPENAI_API_KEY)\n",
    "\n",
    "# instantiate SemanticSimilarityExampleSelector using the examples\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, embeddings, db, k=1\n",
    ")\n",
    "\n",
    "# creae a FewShotPromptTemplate using the example_selector\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Conver the temperature from Celsius to Fahrenheit\",\n",
    "    suffix=\"Input: {temperature}\\nOutput:\",\n",
    "    input_variables=[\"temperature\"]\n",
    ")\n",
    "\n",
    "# test the similar_prompt with different inputs\n",
    "print(similar_prompt.format(temperature=\"10 °C\"))    # test with an input\n",
    "print(similar_prompt.format(temperature=\"30 °C\"))    # test with an another input\n",
    "\n",
    "# add a new example to the SemanticSimilarityExampleSelector\n",
    "similar_prompt.example_selector.add_example({\"input\": \"50 °C\", \"output\": \"122 °F\"})\n",
    "print(similar_prompt.format(temperature=\"40 °C\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
