{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 22:43:03.831 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "d:\\projects\\github\\learning\\llm\\venv_llm\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.0.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n",
      "2024-11-13 22:43:05.380 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import keyring\n",
    "import os \n",
    "import openai\n",
    "import streamlit as st \n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "from elevenlabs import ElevenLabs\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from streamlit_chat import message\n",
    "\n",
    "# Constants\n",
    "TEMP_AUDIO_PATH = './temp/temp_audio.wav'\n",
    "AUDIO_FORMAT = 'audio/wav'\n",
    "\n",
    "# API Key\n",
    "OPENAI_API_KEY = keyring.get_password('openai', 'key_for_windows')\n",
    "ELEVENLABS_API_KEY = keyring.get_password('elevenlabs', 'key_for_windows')\n",
    "ACTIVELOOP_TOKEN = keyring.get_password('activeloop', 'key_for_windows')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['ELEVEN_API_KEY'] = ELEVENLABS_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_database(active_loop_data_set_path):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = DeepLake(\n",
    "        dataset_path=active_loop_data_set_path,\n",
    "        read_only=True,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transcribe_audio(audio_file_path, openai_key):\n",
    "#     \"\"\"\n",
    "#     Transcribe audio using OpenAI Whisper API (updated for openai>=1.0.0).\n",
    "\n",
    "#     Args:\n",
    "#         audio_file_path (str): Path to the audio file.\n",
    "#         openai_key (str): Your OpenAI API key.\n",
    "\n",
    "#     Returns:\n",
    "#         str: Transcribed text if successful, None otherwise.\n",
    "#     \"\"\"\n",
    "#     # Set the OpenAI API key\n",
    "#     openai.api_key = openai_key\n",
    "\n",
    "#     try:\n",
    "#         with open(audio_file_path, \"rb\") as audio_file:\n",
    "#             # Use the updated method to transcribe\n",
    "#             response = openai.Audio.transcribe(\n",
    "#                 model=\"whisper-1\",\n",
    "#                 file=audio_file,\n",
    "#             )\n",
    "#         return response[\"text\"]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error calling Whisper API: {str(e)}\")\n",
    "#         return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "def transcribe_audio(audio_file_path):\n",
    "   \n",
    "    try:\n",
    "        with open(audio_file_path, 'rb') as audio_file:\n",
    "            # load the Whisper model\n",
    "            model = whisper.load_model(\"base\")\n",
    "            # perform transcription\n",
    "            result = model.transcribe(audio_file_path)\n",
    "            return result['text']\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Whisper API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\github\\learning\\llm\\venv_llm\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n"
     ]
    }
   ],
   "source": [
    "# whisper low-level access\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio('./download/harvard.wav')\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the transcription of the audio on the app\n",
    "def display_transcription(transcription):\n",
    "    if transcription:\n",
    "        st.write(f\"Transciption: {transcription}\")\n",
    "        with open(\"/data/transcription.txt\", 'w+') as f:\n",
    "            f.write(transcription)\n",
    "    else:\n",
    "        st.write(\"Error transcribing audio.\")\n",
    "\n",
    "# get user input from Streamlit text input field\n",
    "def get_user_input(transcription):\n",
    "    return st.text_input(\"\", value=transcription if transcription else \"\", key=\"input\")\n",
    "\n",
    "\n",
    "# record audio using audio_recorder and transcribe using transcibe_audio\n",
    "def record_and_transribe_audio():\n",
    "    audio_bytes = audio_recorder()\n",
    "    transcription = None\n",
    "    if audio_bytes:\n",
    "        st.audio(audio_bytes, format=AUDIO_FORMAT)\n",
    "        \n",
    "        with open(TEMP_AUDIO_PATH, 'wb') as f:\n",
    "            f.write(audio_bytes)\n",
    "            \n",
    "        if st.button(\"Transcribe\"):\n",
    "            transcription = transcribe_audio(TEMP_AUDIO_PATH)\n",
    "            \n",
    "        os.remove(TEMP_AUDIO_PATH)\n",
    "        display_transcription(transcription)\n",
    "        \n",
    "    return transcription\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the database for a response based on the user's query\n",
    "\n",
    "def search_db(user_input, db):\n",
    "    print(user_input)\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "    retriever.search_kwargs['fetch_k'] = 100\n",
    "    retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "    retriever.search_kwargs['k'] = 4\n",
    "    model = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "    qa = RetrievalQA.from_llm(model, retriever=retriever, return_source_documents=True)\n",
    "    return qa({'query': user_input})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
