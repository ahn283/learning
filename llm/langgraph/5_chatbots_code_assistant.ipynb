{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation with RAG and self-corection\n",
    "\n",
    "AlphaCodium presented an approach for code generation that uses control flow.\n",
    "\n",
    "Main idea: [construct an answer to a coding question iteratively](https://x.com/karpathy/status/1748043513156272416?s=20)..\n",
    "\n",
    "[AlphaCodium](https://github.com/Codium-ai/AlphaCodium) iteravely tests and improves an answer on public and AI-generated tests for a particular question.\n",
    "\n",
    "We will implement some of these ideas from scratch using LangGraph:\n",
    "\n",
    "1. We start with a set of documentation specified by a user\n",
    "2. We use a long context LLM to ingest it and perform RAG to answer a question based upon it\n",
    "3. We will invoke a tool to produce a structured output\n",
    "4. We will perform two unit tests (check imports and code execution) prior returning the solution to the user\n",
    "\n",
    "<img src='./images/code_assistant.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install our required packages and set the API keys we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keyring\n",
    "\n",
    "# API KEY\n",
    "OPENAI_API_KEY = keyring.get_password('openai', 'key_for_mac')\n",
    "ANTHROPIC_API_KEY = keyring.get_password('anthropic', 'key_for_mac')\n",
    "TAVILY_API_KEY = keyring.get_password('tavily', 'key_for_mac')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY\n",
    "\n",
    "# Set up LangSmith observability\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = keyring.get_password('langsmith', 'learning_agent')\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"pr-stupendous-hood-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs\n",
    "\n",
    "Load [LangChain Expression Language](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel) (LCEL) docs as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/concepts/lcel/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "### Code solution\n",
    "\n",
    "First, we will try OpenAI and Claude3 with function calling.\n",
    "\n",
    "We will create a `code_gen_chain` w/ either OpenAI or Claude and test them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='To build a Retrieval Augmented Generation (RAG) chain in LCEL, you will typically combine a retriever with a language model (LLM) and an output parser. The retriever fetches relevant documents based on a query, the LLM generates a response based on those documents, and the output parser formats the response. This can be achieved using the `RunnableSequence` to chain these components together. Below is an example of how to set this up.', imports='from langchain_core.runnables import RunnableSequence, RunnableParallel\\nfrom langchain_core.runnables import LLM, Retriever, OutputParser', code='# Define the retriever, LLM, and output parser\\nretriever = Retriever()  # Replace with actual retriever implementation\\nllm = LLM()  # Replace with actual LLM implementation\\noutput_parser = OutputParser()  # Replace with actual output parser implementation\\n\\n# Build the RAG chain using RunnableSequence\\nrag_chain = RunnableSequence([\\n    retriever,  # Step 1: Retrieve relevant documents\\n    llm,        # Step 2: Generate response using LLM\\n    output_parser  # Step 3: Parse the output\\n])\\n\\n# Example invocation of the RAG chain with a query\\nquery = \"What is the capital of France?\"\\nfinal_output = rag_chain.invoke(query)  # This will return the final parsed output.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n\n",
    "            Here is a full set of LCEL documentation: \\n ------ \\n {context} \\n ------ \\n Answer the user\n",
    "            question based on the above provided documentation. Ensure any code you provide can be executed \\n\n",
    "            with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "            Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solution to questions about LCEL.\"\"\"\n",
    "    \n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block including import statements\")\n",
    "    \n",
    "expt_llm = 'gpt-4o-mini'\n",
    "llm = ChatOpenAI(model=expt_llm, temperature=0)\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I builf a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "### Anthropic\n",
    "code_gen_prompt_claude = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"<instructions> You are a coding assistant with expertise in LCEL. LangChain expression language. \\n\n",
    "            Here is the LCEL documentation: \\n ------ \\n {context} \\n ------ \\n  Answer the user question based on the \\n\n",
    "            above provided documentation. Ensuare any code youn provide can be executed with all required imports and variabels \\n\n",
    "            defined. Structure your answer : 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
    "            Invoke the code tool to structure the output correctly. <\\Instructions> \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLM \n",
    "exp_llm = \"claude-3-5-haiku-latest\"\n",
    "llm = ChatAnthropic(\n",
    "    model=exp_llm,\n",
    "    default_headers={\"antropic-beta\": \"tools-2024-04-04\"}\n",
    ")\n",
    "\n",
    "sturctured_llm_claude = llm.with_structured_output(code, include_raw=True)\n",
    "\n",
    "# Optional: check for errors in case tool use in flaky\n",
    "def chcek_claude_output(tool_output):\n",
    "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
    "    \n",
    "    # Error with parsing\n",
    "    if tool_output[\"parsing_error\"]:\n",
    "        # Report back output and parsing errors\n",
    "        print(\"Parsing error!\")\n",
    "        raw_output = str(tool_output[\"raw\"].content)\n",
    "        error = tool_output[\"parsing_error\"]\n",
    "        raise ValueError(\n",
    "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parser error: {error}\"\n",
    "        )\n",
    "    \n",
    "    # Tool was not invoked\n",
    "    elif not tool_output[\"parsed\"]:\n",
    "        print(\"Failed to invoke tool!\")\n",
    "        raise ValueError(\n",
    "            \"You did not use the provided tool! be sure to invoke the tool to structure the output.\"\n",
    "        )\n",
    "    return tool_output\n",
    "\n",
    "# Chain with output check\n",
    "code_chain_claude_raw = (\n",
    "    code_gen_prompt_claude | sturctured_llm_claude | chcek_claude_output\n",
    ")\n",
    "\n",
    "def insert_error(inputs):\n",
    "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
    "    \n",
    "    # Get errors\n",
    "    error = inputs[\"error\"]\n",
    "    messages = inputs[\"messages\"]\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
    "        )\n",
    "    ]\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"context\": inputs[\"context\"],\n",
    "    }\n",
    "\n",
    "# This will be run as a fallback chain\n",
    "fallback_chain = insert_error | code_chain_claude_raw\n",
    "N = 3 # Max re-tries\n",
    "code_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(\n",
    "    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n",
    ")\n",
    "\n",
    "def parse_output(solution):\n",
    "    \"\"\"When we add 'include_raw=True' to structured output,\n",
    "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
    "    \n",
    "    return solution[\"parsed\"]\n",
    "\n",
    "# Optional : With re-try to correct for failure to invoke tool\n",
    "code_gen_chain = code_gen_chain_re_try | parse_output\n",
    "\n",
    "# No re-try\n",
    "code_gen_chain = code_gen_prompt_claude | sturctured_llm_claude | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='This example demonstrates building a Retrieval Augmented Generation (RAG) chain using LangChain Expression Language (LCEL). The chain will:\\n1. Use a document loader to load documents\\n2. Create text splitter to chunk documents\\n3. Create embeddings for vector storage\\n4. Create a vector store retriever\\n5. Create a prompt template for RAG\\n6. Use a chat model to generate responses\\n7. Combine these components into a RAG chain', imports='from langchain_community.document_loaders import TextLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_community.embeddings import OllamaEmbeddings\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_community.chat_models import ChatOllama', code='# Load documents\\nloader = TextLoader(\"sample_document.txt\")\\ndocs = loader.load()\\n\\n# Split documents into chunks\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\\nsplits = text_splitter.split_documents(docs)\\n\\n# Create embeddings and vector store\\nembeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\\nvectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\\nretriever = vectorstore.as_retriever()\\n\\n# Create prompt template\\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\"\"\")\\n\\n# Create chat model\\nmodel = ChatOllama(model=\"llama2\")\\n\\n# Construct RAG chain\\nrag_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()} \\n    | prompt \\n    | model\\n)\\n\\n# Example usage\\nresponse = rag_chain.invoke(\"What is the main topic of the document?\")\\nprint(response)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State\n",
    "\n",
    "Our state is a dict that will contain keys (errors, question, code generation) relevant to code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\" \n",
    "    Represents the state of out graph.\n",
    "    \n",
    "    Attributes:\n",
    "        error: Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages: With user question, error messages, reasoning\n",
    "        generation: Code solution\n",
    "        iterations: Number of tries\n",
    "    \"\"\"\n",
    "    \n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "Our graph lays out the logical flow shown in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect' \n",
    "flag = 'do not reflect'\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Generate a code solution\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---GENERATION CODE SOLUTION---\")\n",
    "    \n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "    \n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, import, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\" \n",
    "    Check code\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): New key added to state, error \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---CHECKING CODE---\")\n",
    "    \n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    \n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "    \n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "        \n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "    \n",
    "def reflect(state: GraphState):\n",
    "    \"\"\" \n",
    "    Reflect on errors\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---GENERATION CODE SOLUTION---\")\n",
    "    \n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    \n",
    "    # Prompt reflection\n",
    "    \n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "### Edges\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    \n",
    "    if error == 'no' or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)     # generate solution\n",
    "workflow.add_node(\"check_code\", code_check) # check code\n",
    "workflow.add_node(\"reflect\", reflect)       # reflect\n",
    "\n",
    "# Buold graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATION CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to contruct the input needed for my prompt?\"\n",
    "soltution = app.invoke({\"messages\": [\"user\", question], \"iterations\": 0, \"error\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix=\"This example demonstrates how to directly pass a string to a runnable and use it to construct input for a prompt template in LCEL. We'll show how to use string coercion and function conversion to easily create input for a prompt.\", imports='from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_openai import ChatOpenAI', code='# Create a prompt template\\nprompt = PromptTemplate.from_template(\"Tell me a {type} joke about {subject}\")\\n\\n# Create a language model\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\\n\\n# Create a chain that transforms a simple string input\\n# This demonstrates how you can directly pass a string and transform it\\ndef prepare_joke_input(subject):\\n    return {\"type\": \"silly\", \"subject\": subject}\\n\\n# Construct the chain using LCEL composition\\njoke_chain = RunnableLambda(prepare_joke_input) | prompt | llm\\n\\n# Invoke the chain with a direct string input\\nresult = joke_chain.invoke(\"programming\")\\n\\nprint(result.content)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soltution[\"generation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval\n",
    "\n",
    "[Here](https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d?paginationState=%7B%22pageIndex%22%3A0%2C%22pageSize%22%3A10%7D) is a public dataset of LCEL questions.\n",
    "\n",
    "I saved this as lcel-teacher-eval.\n",
    "\n",
    "You can also find the csv [here](https://github.com/langchain-ai/lcel-teacher/blob/main/eval/eval.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "client = langsmith.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the dataset to your tenant to use it\n",
    "try:\n",
    "    public_dataset = (\n",
    "        \"https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d\"\n",
    "    )\n",
    "    client.clone_public_dataset(public_dataset)\n",
    "except:\n",
    "    print(\"Please setup LangSmith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def check_import(run: Run, example: Example) -> dict:\n",
    "    imports = run.outputs.get(\"imports\")\n",
    "    try:\n",
    "        exec(imports)\n",
    "        return {\"key\": \"import_check\", \"score\": 1}\n",
    "    except Exception:\n",
    "        return {\"key\": \"import_check\", \"score\": 0}\n",
    "    \n",
    "def check_execution(run: Run, example: Example) -> dict:\n",
    "    imports = run.outputs.get(\"imports\")\n",
    "    code = run.outputs.get(\"code\")\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "        return {\"key\": \"code_execution_check\", \"score\": 1}\n",
    "    except Exception:\n",
    "        return {\"key\": \"code_execution_check\", \"score\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare LangGraph to Context Stuffing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_case(example: dict):\n",
    "    \"\"\"Context stuffing\"\"\"\n",
    "    solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": [(\"user\", example[\"question\"])]}\n",
    "    )\n",
    "    return {\"imports\": solution.imports, \"code\": solution.code}\n",
    "\n",
    "def predict_langgraph(example: dict):\n",
    "    \"\"\"LangGrpah\"\"\"\n",
    "    graph = app.invoke(\n",
    "        {\"messages\": [(\"user\", example[\"question\"])], \"iterations\": 0, \"error\": \"\"}\n",
    "    )\n",
    "    solution = graph[\"generation\"]\n",
    "    return {\"imports\": solution.imports, \"code\": solution.code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Evaluator\n",
    "code_evaluator = [check_import, check_execution]\n",
    "\n",
    "# Dataset\n",
    "dataset_name = \"lcel-teacher-eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-without-langgraph-claude-3-5-haiku-latest-b4ebd342' at:\n",
      "https://smith.langchain.com/o/1002547c-0a48-4fe3-9a6d-802598ca95d3/datasets/47840fd5-365d-4ae6-b17c-21c0309f99c3/compare?selectedSessions=31eb5553-adb4-42d5-a8c8-920a4512f90f\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:12,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why do programmers prefer dark mode?\\n\\nBecause the light attracts bugs!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 14, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-67c99244-6edc-440c-9438-a7938104aff0-0' usage_metadata={'input_tokens': 14, 'output_tokens': 14, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:20,  4.39s/it]<string>:25: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "7it [00:33,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode? Because the light attracts bugs!\n",
      "Machine learning is a subset of artificial intelligence that involves developing algorithms and statistical models that allow computers to learn from and make predictions or decisions based on data. It is a method of data analysis that automates analytical model building. \n",
      "\n",
      "In machine learning, computers are trained to recognize patterns in data without being explicitly programmed to do so. Instead, they learn from examples and experience, adjusting their algorithms and models to improve their performance over time. This is typically achieved through the use of algorithms that can analyze and interpret data, identify patterns, and make decisions or predictions based on those patterns.\n",
      "\n",
      "There are several different types of machine learning algorithms, including supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on labeled data, where the correct answers are provided along with the input data. Unsupervised learning involves training a model on unlabeled data and allowing it to find patterns and relationships on its own. Reinforcement learning involves training a model to make decisions or take actions in an environment in order to maximize a reward.\n",
      "\n",
      "Machine learning is used in a wide range of applications, including image and speech recognition, natural language processing, recommendation systems, and autonomous vehicles. It has the potential to revolutionize many industries by enabling more efficient and effective data analysis and decision-making processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:39,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Translate the following question to italian: where did harrison work', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:44,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' age=30 hobbies=['reading', 'hiking', 'painting']\n",
      "Name: John Doe\n",
      "Age: 30\n",
      "Hobbies: ['reading', 'hiking', 'painting']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:49,  4.14s/it]<string>:13: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.utils.pydantic.PromptInput'>\n",
      "\n",
      "# Detailed schema representation:\n",
      "{'properties': {'adjective': {'title': 'Adjective', 'type': 'string'}}, 'required': ['adjective'], 'title': 'PromptInput', 'type': 'object'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:50,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemgetter result: What is LCEL?\n",
      "lambda result: What is LCEL?\n",
      "get() result: What is LCEL?\n",
      "itemgetter raises KeyError when key is missing\n",
      "lambda raises KeyError when key is missing\n",
      "\n",
      "get() method: None\n",
      "get() method with default: No question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:56,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 12, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4f52b1d4-77c1-46bb-a22c-6af30a9be808-0' usage_metadata={'input_tokens': 12, 'output_tokens': 18, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:23,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 1 result:\n",
      "content=\"Why couldn't the bicycle stand up by itself? Because it was two tired!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 22, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-de554645-73b0-41f3-bba8-fff127cf936f-0' usage_metadata={'input_tokens': 22, 'output_tokens': 17, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "\"HELLO WORLD\" is a common phrase used in computer programming to test the output of a program or to demonstrate a basic functionality. It is typically used as the first program or code example when learning a new programming language. The phrase simply displays \"HELLO WORLD\" on the screen or console to indicate that the program is functioning correctly.\n",
      "\n",
      "Approach 2 result:\n",
      "Modified: Greetings, planet Earth!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [01:29,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_result': 'Processed first chain: Hello, parallel processing!', 'second_result': 'Processed second chain: Hello, parallel processing!'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [01:31,  4.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run base case\n",
    "try:\n",
    "    experimental_results_ = evaluate(\n",
    "        predict_base_case,\n",
    "        data=dataset_name,\n",
    "        evaluators=code_evaluator,\n",
    "        experiment_prefix=f\"test-without-langgraph-{exp_llm}\",\n",
    "        max_concurrency=2,\n",
    "        metadata={\n",
    "            \"llm\": exp_llm\n",
    "        }\n",
    "    )\n",
    "except:\n",
    "    print(\"Please setup LangSmith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "- `LangGraph outperforms base case`: adding re-try loop improve performance\n",
    "- `Reflection did not help`: reflection prior to re-try regression vs just passing errors directly back to the LLM\n",
    "- `GPT-4 outperforms Claude3`: Claude3 had 3 and 1 run fail due to tool-use error for Opus and Haiku, respectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
