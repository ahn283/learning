{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81aMF-hnLpMG"
      },
      "source": [
        "# Self-RAG\n",
        "\n",
        "Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations.\n",
        "\n",
        "In the [paper](https://arxiv.org/abs/2310.11511), a few decisions are made:\n",
        "\n",
        "1. Should I retrieve from retriever, `R` -\n",
        "2. Input: `x (question)` OR `x (question)`, `y (generation)`\n",
        "3. Decides when to retrieve `D` chunks with `R`\n",
        "4. Output: `yes`, `no`, `continue`\n",
        "5. Are the retrieved passages `D` relevant to the question `x` -\n",
        "6. Input: `(x (question)`, `d (chunk))` for `d` in `D`\n",
        "7. `d` provides useful information to solve `x`\n",
        "8. Output: `relevant`, `irrelevant`\n",
        "9. Are the LLM generation from each chunk in `D` is relevant to the chunk (hallucinations, etc) -\n",
        "10. Input: `x (question)`, `d (chunk)`, `y (generation)` for `d` in `D`\n",
        "11. All of the verification-worthy statements in `y (generation)` are supported by `d`\n",
        "12. Output: `{fully supported, partially supported, no support}`\n",
        "13. The LLM generation from each chunk in `D` is a useful response to `x (question)` -\n",
        "14. Input: `x (question)`, `y (generation)` for `d` in `D`\n",
        "15. `y (generation)` is a useful response to `x (question)`.\n",
        "16. Output: `{5, 4, 3, 2, 1}`\n",
        "\n",
        "We will implement some of these ideas from scratch using LangGraph.\n",
        "\n",
        "<img src='./images/self_rag.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdX3BZ5cNS2z"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First let's install our required packages and set our API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mnDmRsoLYsC"
      },
      "outputs": [],
      "source": [
        "pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T45RCorCNfDT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keyring\n",
        "from google.colab import userdata\n",
        "\n",
        "# API KEY\n",
        "OPENAI_API_KEY = userdata.get('openai')\n",
        "ANTHROPIC_API_KEY = userdata.get('anthropic')\n",
        "TAVILY_API_KEY = userdata.get('tavily')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY\n",
        "\n",
        "# Set up LangSmith observability\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('langsmith')\n",
        "os.environ['LANGCHAIN_PROJECT'] = \"pr-stupendous-hood-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL5GBWZ5NrVK"
      },
      "source": [
        "## Retriever\n",
        "\n",
        "Let's index 3 blog posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0rJ33EDNukV"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name='rag-chroma',\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMX5IxU5Oc1p"
      },
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLHA1k-AOenJ"
      },
      "outputs": [],
      "source": [
        "### Retriever Grader\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "  \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "  binary_score: str = Field(\n",
        "      description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "  )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessng relevance of a retrieved document to a user question. \\n\n",
        "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "Give a binary core 'yes' or 'no' score to indicate whtether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"document\": doc_txt, \"question\": question}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ2ES1yUpW8a"
      },
      "outputs": [],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmJ6zKVZp_Kg"
      },
      "outputs": [],
      "source": [
        "### Hallucination grader\n",
        "\n",
        "# Data model\n",
        "class GradeHallucinations(BaseModel):\n",
        "  \"\"\"Binary score for hallucination present in generateion answer.\"\"\"\n",
        "\n",
        "  binary_score: str = Field(\n",
        "      description=\"Answer is grouned in the facts, 'yes' or 'no'\"\n",
        "  )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "Give a binary score 'yes' or 'no'. 'Yes' means thst the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqZ_JudlrJUM"
      },
      "outputs": [],
      "source": [
        "### Answer Grader\n",
        "\n",
        "# Data model\n",
        "class GradeAnswer(BaseModel):\n",
        "  \"\"\"Binary score to assess answer adresses question.\"\"\"\n",
        "\n",
        "  binary_score: str = Field(\n",
        "      description=\"Answer adresses the question, 'yes' or 'no'\"\n",
        "  )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question. \\n\n",
        "Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FklBn5_sJAX"
      },
      "outputs": [],
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# LLM\n",
        "llm  = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meanin.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n{question} \\n Formualte an improved question.\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MnFy0rIKdDh"
      },
      "source": [
        "## Graph\n",
        "\n",
        "Capture the flow in as a graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hgnn8oaKg1W"
      },
      "source": [
        "### Graph state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQKdaqp_Ki-L"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "  \"\"\"\n",
        "  Represents the state of our graph.\n",
        "\n",
        "  Attributes:\n",
        "    question: question\n",
        "    generation: LLM generation\n",
        "    documents: list of documents\n",
        "  \"\"\"\n",
        "  question: str\n",
        "  generation: str\n",
        "  documents: List[str]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3M4yLL0K0nv"
      },
      "outputs": [],
      "source": [
        "### Nodes\n",
        "\n",
        "def retrieve(state):\n",
        "  \"\"\"\n",
        "  Retrieve documents\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): New key added to state, documents, that contains retrieved documents\n",
        "  \"\"\"\n",
        "  print(\"---RETIRVE---\")\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  # Retrieval\n",
        "  documents = retriever.invoke(question)\n",
        "  return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def generate(state):\n",
        "  \"\"\"\n",
        "  Generate answer\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): New key added to state, generation, that contains LLM generation\n",
        "  \"\"\"\n",
        "  print(\"---GENERATE---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  # RAG generation\n",
        "  generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "  return {\"documents\": documents, \"generation\": generation, \"question\": question}\n",
        "\n",
        "def grade_documents(state):\n",
        "  \"\"\"\n",
        "  Define whether the retrieved documents are relevant to the quesion\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "  Returns:\n",
        "    state (dict): Updates documents key with only filtered relecant documents\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---CHECK DOCUMENT RELEVANT TO QUESTION---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  # Score each doc\n",
        "  filtered_docs = []\n",
        "  for d in documents:\n",
        "    score = retrieval_grader.invoke(\n",
        "        {\"question\": question, \"document\": d.page_content}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "    if grade == \"yes\":\n",
        "      print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "      filtered_docs.append(d)\n",
        "    else:\n",
        "      print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "      continue\n",
        "\n",
        "  return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "def transform_query(state):\n",
        "  \"\"\"\n",
        "  Transform the query to produce a better question.\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "  Returns:\n",
        "    state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "  print(\"---TRANSFORM QUERY---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  # Re-write question\n",
        "  better_question = question_rewriter.invoke({\"question\": question})\n",
        "  return {\"question\": better_question, \"documents\": documents}\n",
        "\n",
        "### Edges\n",
        "\n",
        "def decide_to_generate(state):\n",
        "  \"\"\"\n",
        "  Determines whether to generate an asnwer, or re-generate a question.\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    str: Binary decisions for next node to call\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "  state[\"question\"]\n",
        "  filtered_documents = state[\"documents\"]\n",
        "\n",
        "  if not filtered_documents:\n",
        "    # All documents have been filtered check_relevance\n",
        "    # We will re-generate a new query\n",
        "    print(\n",
        "        \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESION.TRANSFORM QUERY---\"\n",
        "    )\n",
        "    return \"transform_query\"\n",
        "\n",
        "  else:\n",
        "    # We have relevant documents, so generate answer\n",
        "    print(\"---DEICSION: GENERATE---\")\n",
        "    return \"generate\"\n",
        "\n",
        "def grade_gneration_v_documents_and_question(state):\n",
        "  \"\"\"\n",
        "  Determins whether the generation is grounded in the document and answers question.\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    str: Binary decisions for next node to call\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---CHECK HALLUCINATIONS---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "  generation = state[\"generation\"]\n",
        "\n",
        "  score = hallucination_grader.invoke(\n",
        "      {\"documents\": documents, \"generation\": generation}\n",
        "  )\n",
        "  grade = score.binary_score\n",
        "\n",
        "  # Check hallucination\n",
        "  if grade == \"yes\":\n",
        "    print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "    # Check question-answering\n",
        "    print(\"---GRADE GENERATION vs QUESION---\")\n",
        "    score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "    grade = score.binary_score\n",
        "    if grade == \"yes\":\n",
        "      print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "      return \"useful\"\n",
        "    else:\n",
        "      print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "      return \"not useful\"\n",
        "\n",
        "  else:\n",
        "    pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "    return \"not supproted\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7orhiBEPNDV"
      },
      "source": [
        "## Build Graph\n",
        "\n",
        "The just follows the flow we outlined in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-9wR3c6L7-I"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)   # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
        "workflow.add_node(\"generate\", generate) # generate\n",
        "workflow.add_node(\"transform_query\", transform_query) # transform query\n",
        "\n",
        "# Define the edges\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_gneration_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mygf4d1QIm0"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
        "for output in app.stream(inputs):\n",
        "  for key, value in output.items():\n",
        "    # Nodes\n",
        "    pprint(f\"Node '{key}':\")\n",
        "    # Optional: print full state at each node\n",
        "    # pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
