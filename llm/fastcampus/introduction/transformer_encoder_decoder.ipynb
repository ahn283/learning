{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "\n",
    "def encode(input_sequence, embedding_matrix):\n",
    "    ''' \n",
    "    입력 시퀀스 컨텍스트 벡터로 인코딩하는 간소화된 함수입니다.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    - input_sequence : 문장 내 단어들을 대표하는 정수의 리스트\n",
    "    - embedding_matrix : 단어 인덱스를 임베딩으로 변환하는 행렬\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - 입력 시퀀스 전체의 정보를 담은 단일 벡터를 반환\n",
    "    '''\n",
    "    \n",
    "    # step 1. word embedding\n",
    "    # transforms input word into high dimension vector\n",
    "    embedded_sequence = [embedding_matrix[word] for word in input_sequence]\n",
    "    print(embedded_sequence)\n",
    "    \n",
    "    # step 2. self-attention : calculation interaction between words\n",
    "    # 여기서는 모든 임베딩의 합을 임베딩의 수로 나누어 평균 벡터를 생성합니다.\n",
    "    # 2차원 리스트의 합 \n",
    "    attention_output = sum(sum(embedded_sequence, [])) / len(embedded_sequence)\n",
    "    \n",
    "    # step 3. position-wise feedforwad network : 각 위치의 출력을 독립적으로 변환하는 과정을 단순화\n",
    "    # 여기서는 결과 벡터에 고정된 스칼라(1.5)를 곱합니다.\n",
    "    context_vector = attention_output * 1.5\n",
    "    \n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      "4.2\n"
     ]
    }
   ],
   "source": [
    "a = [[0.1, 0.2], [0.4, 0.5], [0.6, 0.7], [0.8, 0.9]]\n",
    "print(sum(a, []))\n",
    "print(sum(sum(a, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1, 0.2], [0.4, 0.5], [0.6, 0.7], [0.8, 0.9]]\n",
      "context vector:  1.5750000000000002\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "input_sequence = [3, 14, 15, 92]        # 단어 인덱스로 표현된 예제 문장\n",
    "embedding_matrix = {\n",
    "    3: [0.1, 0.2],\n",
    "    14: [0.4, 0.5],\n",
    "    15: [0.6, 0.7],\n",
    "    92: [0.8, 0.9]\n",
    "}   # 간소화된 임베딩 행렬\n",
    "\n",
    "context_vector = encode(input_sequence, embedding_matrix)\n",
    "print(\"context vector: \", context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예를 들어, 문장을 입력으로 받았다면, 이 컨텍스트 벡터는 해당 문장의 전반적인 의미, 중요 단어나 구, 문맥 등을 모델이 이해한 바를 요약하여 담고 있습니다. \n",
    "# 따라서, 컨텍스트 벡터는 후속 처리 과정(예: 번역, 요약, 감성 분석 등)에서 매우 중요한 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "\n",
    "import random\n",
    "\n",
    "def simple_decoder(encoded_text, start_token, vocabulary, embedding_matrix, max_length):\n",
    "    \"\"\" \n",
    "    간소화된 디코더 함수, 인코딩된 컨텍스트 벡터를 사용하여 새로운 시퀀스를 생성합니다.\n",
    "    Args\n",
    "    ----\n",
    "    - encoded_context : 인코더로부터 넘어온 컨텍스트 벡터\n",
    "    - start_token : 출력 시퀀스 생성을 시작하는 토큰\n",
    "    - vocabulary : 가능한 모든 출력 토큰의 집합\n",
    "    - embedding_matrix : 단어 임베딩을 위한 행렬\n",
    "    - max_length : 생성할 시퀀스의 최대 길이\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - 생성한 시퀀스의 단어 인덱스 리스트\n",
    "    \"\"\"\n",
    "    output_sequence = [start_token]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # 현재까지의 출력 시퀀스를 기반으로 다음 토큰 예측 (단순화를 예시로, 무작위 선택을 사용)\n",
    "        next_token = random.choice(list(vocabulary))\n",
    "        output_sequence.append(next_token)\n",
    "        \n",
    "        # 'end' 토큰을 만나면 시퀀스 생성 종료\n",
    "        if next_token == 'end':\n",
    "            break\n",
    "        \n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence:  ['hello', 'start', 'start', 'world', 'end']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "encoded_context = [0.5, -1]     # 인코더로부터의 컨텍스트 벡터 예시\n",
    "start_token = 'hello'           # 시퀀스 생성을 위한 시작 토큰\n",
    "vocabulary = {'start', 'hello', 'world', 'end'}     # 가능한 단어 집합\n",
    "max_length = 5          # 생성할 시퀀스의 최대 길이\n",
    "\n",
    "# execute decoder\n",
    "ouput_sentence = simple_decoder(encoded_context, start_token, vocabulary, embedding_matrix, max_length)\n",
    "print(\"Generated Sequence: \", ouput_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
