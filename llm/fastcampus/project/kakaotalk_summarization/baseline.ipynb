{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 모델 개발 및 평가 지표 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from eval import get_eval_data, pointwise_eval\n",
    "from utils import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비용 기반 후보 모델 선정\n",
    "- Claude 3 Haiku\n",
    "- Gemini 1.5 Flash\n",
    "- ChatGPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_BASELINE = f\"\"\"아래 사용자 대화에 대해 3문장 내로 요약해주세요:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P01: 언니는 연애 할 때 자주 만나는 편이야?\n",
      "P02: 나는 일주일에 한 번이 좋다고 생각하는데! 나는 그게 딱 맞는 거 같아.\n",
      "P02: 너는 자주 만나는 걸 좋아해?\n",
      "P01: 오. 차가운 심장을 가졌네.\n",
      "P01: 나도 자주 만나는 건 별로야.\n",
      "P01: 일주일에 두세 번 정도가 적당한 거 같아!\n",
      "P02: 적당히 애틋하고 그게 좋아 난! 맨날 만나면 질려...\n",
      "P02: 나의 개인주의 조금 알지?\n",
      "P01: 알지 알지. 그러면 연락은 자주 하는 편이야?\n",
      "P02: 연락은 실시간으로 하지는 않지만 틈 날 때마다 하면... 그게 실시간인가...\n",
      "P02: 너는?\n",
      "P01: 그렇지. 실시간이지.\n",
      "P01: 나도 막 붙잡고 있진 않아도 잘하는 거 같애.\n",
      "P01: 그러면 카톡이 좋아 전화 통화가 좋아?\n",
      "P02: 나는 전화! 목소리를 들어야 돼! 전화 어려운 거 아니니까...\n",
      "P02: 너도 전화를 선호하는 편이야?\n",
      "P01: 그렇구나. 아니 나는 전화가 귀찮아.\n",
      "P01: 그래서 카톡이 더 편하고 전화보다는 그냥 만나는 게 더 좋은 거 같아 하하\n",
      "P02: 좀 다르네 키키 나는 전화하는 거 좋아해 목소리 듣는 거 좋아해!\n",
      "P02: 근데 이제 자주 만나는 건 괜찮은...\n",
      "P01: 그렇게 좀 반대다 키키\n",
      "P01: 그러면 결혼을 만약에 만약에 한다면 신혼여행은 어디로 가고 싶어?\n",
      "P02: 유럽 유럽! 나는 유럽에 약간 지금 미쳐있는 상태야... 너무 가고 싶어...\n",
      "P02: 너는 휴양지 가고 싶어?\n",
      "P01: 키키 유럽에 진심이네.\n",
      "P01: 응응! 나는 휴양지로 가고 싶어.\n",
      "P01: 놀지 못하고 죽은 귀신이 붙었나~하하\n",
      "P02: 키키 쉬지 못하고 죽은 귀신이 붙은 거 같은데\n",
      "P02: 하와이? 피지? 몰디브? 이런데?\n",
      "P01: 맞아! 바로 그런 곳으로!\n",
      "P01: 나 바다를 너무 좋아하네 ^^\n",
      "P02: 그러게 나는 유럽 길바닥 좋아하는데!\n"
     ]
    }
   ],
   "source": [
    "print(get_eval_data()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 두 사람은 연애할 때 만나는 횟수와 연락 방식에 대해 서로 다른 선호도를 가지고 있다.\n",
      "2. P01은 전화보다는 카톡이나 직접 만나는 것을 더 선호하지만, P02는 전화를 통해 목소리를 듣는 것을 좋아한다.\n",
      "3. 두 사람은 신혼여행 장소에 대해서도 차이를 보이는데, P01은 휴양지를, P02는 유럽을 선호한다.\n",
      "The AI assistant's response provides a concise summary of the conversation, highlighting the key differences in preferences between P01 and P02 regarding meeting frequency, communication methods, and honeymoon destinations. The response is relevant and accurately reflects the content of the conversation. However, it could be improved by adding more details or insights to make it more comprehensive.\n",
      "\n",
      "Rating: [[8]]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(\n",
    "    conversation=get_eval_data()[0],\n",
    "    prompt=PROMPT_BASELINE,\n",
    "    model='claude-3-haiku-20240307'\n",
    ")\n",
    "\n",
    "eval_comment = pointwise_eval(get_eval_data()[0], summary)\n",
    "\n",
    "print(summary)\n",
    "print(eval_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 명의 여성이 연애할 때의 만남 빈도, 연락 방식, 신혼여행지에 대한 선호도를 이야기하며 서로 다른 취향을 보여줍니다. 한 명은 일주일에 한두 번 만나고, 전화 통화를 선호하며, 신혼여행지는 유럽을 원하는 반면, 다른 한 명은 일주일에 두세 번 만나고, 카톡을 선호하며, 신혼여행지는 휴양지를 원합니다. 두 사람은 서로의 취향에 대해 웃으며 흥미롭게 이야기합니다. \n",
      "\n",
      "The AI assistant's response provides a concise summary of the conversation, highlighting the key points of discussion between the two individuals. It accurately captures the differences in their preferences regarding meeting frequency, communication methods, and honeymoon destinations. However, the summary could be more engaging by including some of the humor and light-heartedness present in the original conversation. Overall, the response is helpful, relevant, and accurate.\n",
      "\n",
      "Rating: [[8]]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(\n",
    "    conversation=get_eval_data()[0],\n",
    "    prompt=PROMPT_BASELINE,\n",
    "    model='gemini-1.5-flash-001'\n",
    ")\n",
    "\n",
    "eval_comment = pointwise_eval(get_eval_data()[0], summary)\n",
    "\n",
    "print(summary)\n",
    "print(eval_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 사용자는 연애할 때 자주 만나는 것에 대해 의견이 다르고, 연락 방법에 대해서도 다른 선호를 가지고 있습니다. 결혼을 한다면 신혼여행은 유럽 또는 휴양지로 가고 싶어하는데, 이에 대해 서로 다른 관심을 가지고 있습니다.\n",
      "The AI assistant's response provides a concise summary of the conversation, highlighting the key points of difference between the two users regarding their preferences in dating frequency, communication methods, and honeymoon destinations. However, the summary is somewhat superficial and does not delve into the nuances of the conversation, such as the reasons behind their preferences or the playful tone of their interaction. A more detailed analysis could have added depth to the response.\n",
      "\n",
      "Rating: [[7]]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(\n",
    "    conversation=get_eval_data()[0],\n",
    "    prompt=PROMPT_BASELINE,\n",
    "    model='gpt-3.5-turbo-0125'\n",
    ")\n",
    "\n",
    "eval_comment = pointwise_eval(get_eval_data()[0], summary)\n",
    "\n",
    "print(summary)\n",
    "print(eval_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b96eb10ead4ddc8d8bdd0ade65de04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a26a7613ec4405da36d373fc4bca1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08ce72fabab43e9afe11f850e98d51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    'gemini-1.5-flash-001',\n",
    "    'claude-3-haiku-20240307',\n",
    "    'gpt-3.5-turbo-0125'\n",
    "]\n",
    "\n",
    "scores = {model: [] for model in models}\n",
    "pattern = r'\\[\\[\\d+\\]\\]'\n",
    "\n",
    "for model in models:\n",
    "    for i in tqdm(range(len(get_eval_data()))):\n",
    "        summary = summarize(\n",
    "            conversation=get_eval_data()[i],\n",
    "            prompt=PROMPT_BASELINE,\n",
    "            model=model\n",
    "        )\n",
    "        eval_comment = pointwise_eval(get_eval_data()[i], summary)\n",
    "        match = re.search(pattern, eval_comment)\n",
    "        matched_string = match.group(0)\n",
    "        score = int(matched_string[2])\n",
    "        scores[model].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 7, 8, 8, 7, 8, 8, 5, 6, 6, 1, 9, 6, 8, 6, 6, 6, 6, 7, 1, 9, 1, 6, 6, 6, 9, 7, 8, 5, 7, 7, 6, 6, 8, 7, 9, 7, 7, 8, 7, 8, 8, 6, 6, 7, 8, 6, 5, 4] gemini-1.5-flash-001\n",
      "[8, 9, 7, 7, 8, 6, 6, 4, 5, 5, 5, 4, 7, 5, 6, 6, 6, 8, 7, 6, 9, 9, 7, 9, 9, 7, 7, 6, 7, 7, 6, 6, 6, 7, 8, 7, 6, 5, 7, 8, 6, 9, 7, 6, 6, 7, 8, 4, 6, 6] claude-3-haiku-20240307\n",
      "[7, 7, 6, 4, 8, 7, 2, 6, 6, 4, 4, 5, 5, 5, 4, 6, 5, 6, 5, 4, 6, 7, 4, 5, 6, 6, 7, 5, 6, 3, 6, 7, 6, 6, 4, 6, 6, 5, 6, 6, 3, 5, 6, 4, 4, 4, 5, 6, 4, 5] gpt-3.5-turbo-0125\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(scores[model], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-1.5-flash-001: 6.56 / 1.82\n",
      "claude-3-haiku-20240307: 6.66 / 1.33\n",
      "gpt-3.5-turbo-0125: 5.3 / 1.23\n"
     ]
    }
   ],
   "source": [
    "for model in scores:\n",
    "    mean = sum(scores[model]) / len(scores[model])\n",
    "    variance = sum((x - mean) ** 2 for x in scores[model]) / (len(scores[model]) - 1)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    print(f'{model}: {mean} / {round(std_dev, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-1.5-flash-001 9 1\n",
      "claude-3-haiku-20240307 9 4\n",
      "gpt-3.5-turbo-0125 8 2\n"
     ]
    }
   ],
   "source": [
    "for model in scores:\n",
    "    print(model, max(scores[model]), min(scores[model]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
