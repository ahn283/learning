from langchain_community.retrievers import BM25Retriever
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import EnsembleRetriever
import transformers
import torch
import streamlit as st 

data = [
    {
        "ê¸°ì—…ëª…": "ì‚¼ì„±ì „ì",
        "ë‚ ì§œ": "2024-03-02",
        "ë¬¸ì„œ ì¹´í…Œê³ ë¦¬": "ì¸ìˆ˜í•©ë³‘",
        "ìš”ì•½": "ì‚¼ì„±ì „ìê°€ HVAC(ëƒ‰ë‚œë°©ê³µì¡°) ì‚¬ì—… ì¸ìˆ˜ë¥¼ íƒ€ì§„ ì¤‘ì´ë©°, ì´ëŠ” ê¸°ì¡´ ê°€ì „ ì‚¬ì—…ì˜ ì•½ì  ë³´ì™„ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.",
        "ì£¼ìš” ì´ë²¤íŠ¸": ["ê¸°ì—… ì¸ìˆ˜í•©ë³‘"]
    },
    {
        "ê¸°ì—…ëª…": "ì‚¼ì„±ì „ì",
        "ë‚ ì§œ": "2024-03-24",
        "ë¬¸ì„œ ì¹´í…Œê³ ë¦¬": "ì¸ìˆ˜í•©ë³‘",
        "ìš”ì•½": "í…ŒìŠ¤íŠ¸ í•˜ë‚˜ ë‘˜ ì…‹",
        "ì£¼ìš” ì´ë²¤íŠ¸": ["ì‹ ì œí’ˆ ì¶œì‹œ"]
    },
    {
        "ê¸°ì—…ëª…": "í˜„ëŒ€ì°¨",
        "ë‚ ì§œ": "2024-04-02",
        "ë¬¸ì„œ ì¹´í…Œê³ ë¦¬": "ì¸ìˆ˜í•©ë³‘",
        "ìš”ì•½": "ì‚¼ì„±ì „ìê°€ HVAC(ëƒ‰ë‚œë°©ê³µì¡°) ì‚¬ì—… ì¸ìˆ˜ë¥¼ íƒ€ì§„ ì¤‘ì´ë©°, ì´ëŠ” ê¸°ì¡´ ê°€ì „ ì‚¬ì—…ì˜ ì•½ì  ë³´ì™„ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.",
        "ì£¼ìš” ì´ë²¤íŠ¸": ["ê¸°ì—… ì¸ìˆ˜í•©ë³‘", "ì‹ ì œí’ˆ ì¶œì‹œ"]
    },
]

doc_list = [item['ìš”ì•½'] for item in data]

# elastic search based retriever
bm25_retriever = BM25Retriever.from_texts(
    doc_list, metadatas=[{'source':i} for i in range(len(data))]
)
bm25_retriever.k = 1

# vector db faiss retriever
# with sentence-transformer embedding
embedding = SentenceTransformerEmbeddings(model_name='distiluse-base-multilingual-cased-v1')
faiss_vecorstore = FAISS.from_texts(
    doc_list, embedding, metadatas=[{'source': i} for i in data]
)
faiss_retriever = faiss_vecorstore.as_retriever(
    retriever=[bm25_retriever, faiss_vecorstore],
    weight=[0.5, 0.5]
)

# ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
)

# sLLM name
model_name = '42dot/42dot_LLM-SFT-1.3B'

# check if cuda is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

pipeline = transformers.pipeline(
    'text-generation',
    model=model_name,
    device=device,
    model_kwargs={"torch_dtype": torch.float16}
)

# evaluation mode
pipeline.model.eval()

# revrieve docs
def retrieve(query):
    ensemble_docs = ensemble_retriever.invoke(query)
    return ensemble_docs

# sllm generation (output)
def sllm_generate(query):
    answer = pipeline(
        query,
        max_new_tokens=50,
        do_sample=True,
        temperature=0.9,
        repetition_penalty=1.2,
    )
    return answer[0]['generated_text'][len(query):]

# prompt and generation
def prompt_and_generation(query):
    docs = [doc for doc in retrieve(query)]
    prompt = f"""ì•„ë˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ëœ ë‰´ìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤.
    # ì§ˆë¬¸: {query} 
    """
    
    for i in range(len(docs)):
        prompt += f"ë‰´ìŠ¤{i+1}\n"
        prompt += f"ìš”ì•½: {docs[i].page_content}\n"
        prompt += "\n"
    prompt += "# ë‹µë³€:"
    
    answer = sllm_generate(prompt)
    return answer

# streamlit ui

st.title("ğŸ¤– íˆ¬ì ì–´ì‹œìŠ¤í„´íŠ¸")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])
        
if prompt := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    response = f"bot: {prompt_and_generation(prompt.strip())}"
    
    with st.chat_message('assistant'):
        st.markdown(response)
    st.session_state.messages.append({'role': 'assistant', 'content': response})
