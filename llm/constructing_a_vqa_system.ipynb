{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./image/download.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, ViTModel, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0, 1\"     # This means that the program can see and use GPU 0 and GPU 1.\n",
    "\n",
    "device = torch.device('cuda')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTFeatureExtractor, AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODER_MODEL = 'gpt2'\n",
    "TEXT_ENCODER_MODEL = 'distilbert-base-uncased'\n",
    "IMAGE_ENCODER_MODEL = 'facebook/dino-vitb16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER_MODEL)\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n",
    "image_feature_extractor = ViTFeatureExtractor.from_pretrained(IMAGE_ENCODER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def retrieve_image(image_file):\n",
    "    try:\n",
    "        image = Image.open(image_file)\n",
    "        return image\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def load_vqa_data(\n",
    "    annotations_file, questions_file, images_folder, load_images=False, \n",
    "    start_at=None, end_at=None, max_images=None, max_questions=None,\n",
    "    agree_threshold=5\n",
    "):\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        annotations_data = json.load(f)\n",
    "\n",
    "    with open(questions_file, \"r\") as f:\n",
    "        questions_data = json.load(f)\n",
    "\n",
    "    data = []\n",
    "    images_used = defaultdict(int)\n",
    "    # Create a dictionary to map question_id to the annotation data\n",
    "    annotations_dict = {annotation[\"question_id\"]: annotation for annotation in annotations_data[\"annotations\"]}\n",
    "    print(len(annotations_dict))\n",
    "    for question in tqdm(questions_data[\"questions\"][start_at:end_at]):\n",
    "        question_id = question[\"question_id\"]\n",
    "        annotation = annotations_dict[question_id]\n",
    "\n",
    "        image_id = question[\"image_id\"]\n",
    "        image_file = f\"{images_folder}/COCO_{images_folder}_{str(image_id).zfill(12)}.jpg\"\n",
    "        if max_questions and images_used[image_file] >= max_questions:\n",
    "            continue\n",
    "        all_answers = [ans[\"answer\"] for ans in annotation[\"answers\"]]\n",
    "        if all_answers.count(annotation['multiple_choice_answer']) < agree_threshold:\n",
    "            continue\n",
    "\n",
    "        if load_images:\n",
    "            # Load the image and convert it to a numpy array\n",
    "            image = retrieve_image(image_file)\n",
    "            if not image:\n",
    "                continue\n",
    "            image.close()  # Close the image object\n",
    "            \n",
    "        else:\n",
    "            if not os.path.exists(image_file):\n",
    "                continue\n",
    "            # Store the image file path\n",
    "            image = image_file\n",
    "\n",
    "        # Add the data as a dictionary\n",
    "        data.append(\n",
    "            {\n",
    "                \"image_id\": image_id,\n",
    "                \"question_id\": question_id,\n",
    "                \"question\": question[\"question\"],\n",
    "                \"answer\": decoder_tokenizer.bos_token + ' ' + annotation[\"multiple_choice_answer\"]+decoder_tokenizer.eos_token,\n",
    "                \"all_answers\": all_answers,\n",
    "                \"image\": image,\n",
    "            }\n",
    "        )\n",
    "        images_used[image_file] += 1\n",
    "        \n",
    "        if max_images and len(images_used) >= max_images:\n",
    "            break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text tokenizer\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_ENCODER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T \n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np \n",
    "\n",
    "def preprocess_image(image):\n",
    "    # open the image if the input is a file path\n",
    "    if type(image) == str:\n",
    "        img = image.open(image)\n",
    "    else:\n",
    "        img = image\n",
    "        \n",
    "    # check the number of channels in the images and convert to RGB if necessary\n",
    "    if img.mode == \"L\":         # \"L\" stands for grayscale mode\n",
    "        img_rgb = img.convert('RGB')\n",
    "    else:\n",
    "        img_rgb = img\n",
    "        \n",
    "    return img_rgb\n",
    "        \n",
    "def science_qa_data_collator(batch):\n",
    "    # Preprocess and tokenize text\n",
    "    if batch[0].get('answer'):\n",
    "        text_inputs = [sample['question'] for sample in batch]\n",
    "    else:\n",
    "        text_inputs = [f\"{sample['question']} Choices are: {'. '.join(sample['choices'])}\" for sample in batch]\n",
    "    text_tensors = text_tokenizer(text_inputs, padding=True, return_tensors='pt')\n",
    "    \n",
    "    try:\n",
    "        image_inputs = image_feature_extractor([preprocess_image(sample[\"image\"]) for sample in batch])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print([sample[\"image\"] for sample in batch])\n",
    "    image_tensors = torch.from_numpy(np.stack(image_inputs['pixel_values']))\n",
    "    \n",
    "    # prepare decoder inputs [targets]\n",
    "    target_inputs = [sample[\"answer\"] for sample in batch]\n",
    "    target_tensors = decoder_tokenizer(target_inputs, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # return input tensors\n",
    "    return {\n",
    "        \"input_text\": text_tensors[\"input_ids\"],\n",
    "        \"attention_mask\": text_tensors[\"attention_mask\"],\n",
    "        \"input_image\": image_tensors,\n",
    "        \"decoder_input_ids\": target_tensors[\"input_ids\"],\n",
    "        \"labels\": target_tensors[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A MultiModal class used to perform visual question answering (VAQ).\n",
    "    It consists of encoders for text and image and a decoder for generating the answer\n",
    "    \n",
    "    Attributes\n",
    "    ---------\n",
    "    text_encoder : A model to encode text input.\n",
    "    image_encoder : A model to encode image input.\n",
    "    decoder : A model to decode and genrate answers.\n",
    "    text_projection : A linear layer to project text encoding to a specific size.\n",
    "    image_projection : A linear layer to project image encoding to a specific size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_encoder_model, image_encoder_model, decoder_model, freeze=None, load_from=None):\n",
    "        \"\"\"\n",
    "        Initialize the MultiModalModel\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_encoder_model (str): Pre-trained text encoder model name.\n",
    "        image_encoder_model (str): Pre-trained image encoder model name.\n",
    "        decode_model (str): Pre-trained decoder model name.\n",
    "        freeze (str. optional): Which parts of the model to freeze. Can be 'encoders', 'decoder', 'all' or specific encoder.\n",
    "        load_from (str. optional): Path to a checkpoint file to load the model.\n",
    "        \"\"\"\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        \n",
    "        # Initialize text and image encoders\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_encoder_model)\n",
    "        self.image_encoder = ViTModel.from_pretrained(image_encoder_model)\n",
    "        \n",
    "        # Initialize the GPT-2 decoder\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
    "            decoder_model, add_cross_attention=True, tie_word_embeddings=True\n",
    "        )\n",
    "        \n",
    "        # Initialize linear layers for projecting embedded features\n",
    "        self.text_projection = nn.Linear(self.text_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
    "        self.image_projection = nn.Linear(self.image_encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
    "        \n",
    "        # Freeze specified if required or load from a checkpoint\n",
    "        if load_from:\n",
    "            self.load_model_checkpoint(load_from)\n",
    "        else:\n",
    "            self.freeze(freeze)\n",
    "            \n",
    "    def freeze(self, freeze):\n",
    "        \"\"\"\n",
    "        Freeze specific parts of the model to prevent them from being updated during training\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        freeze (str): which parts to freeze. Can be 'encoders', 'decoder', 'all' or specific encoder\n",
    "        \"\"\"\n",
    "        if not freeze:\n",
    "            return\n",
    "        print('Freezing...')\n",
    "        if freeze in ('encoders', 'all') or 'text_encoder' in freeze:\n",
    "            print('Freezing text encoder')\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        if freeze in ('encoders', 'all') or 'image_encoder' in freeze:\n",
    "            print('Freezing image encoder')\n",
    "            for param in self.image_encoer.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        if freeze in ('decoder', 'all'):\n",
    "            print('Freezing decoder (except for cross attention)')\n",
    "            for name, param in self.decoder.named_parameters():\n",
    "                if 'corssattention' not in name:\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "    def load_model_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        Load the model from a saved checkpoint.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        path(str) : Path to the saved checkpoint.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        checkpoint = {k.replace(\"module.\", \"\"): v for k, v in checkpoint.item()}\n",
    "        self.load_state_dict(checkpoint)\n",
    "        \n",
    "    def encode_text(self, input_text, attention_mask):\n",
    "        \"\"\" \n",
    "        Encode text using the text encoder and project it to a specific size.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        torch.Tensor: Projected text encoding\n",
    "        \"\"\"\n",
    "        self.check_input(input_text, \"input_text\")\n",
    "        text_encoded = self.text_encoder(input_text, attention_mask=attention_mask).last_hidden_state.mean(dim=1)\n",
    "        return self.text_projection(text_encoded)\n",
    "    \n",
    "    def encode_image(self, input_image):\n",
    "        \"\"\"\n",
    "        Encode image using the image encoder and project it to a specific size.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_image(torch.Tensor) : input image tensor.\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        torch.Tensor : Projected image encoding\n",
    "        \"\"\"\n",
    "        self.check_input(input_image, \"input_image\")\n",
    "        image_encoded = self.image_encoder(input_image).last_hidden_state.mean(dim=1)\n",
    "        return self.image_projection(image_encoded)\n",
    "    \n",
    "    def forward(self, input_text, input_image, decoder_input_ids, attention_mask, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        input_text (torch.Tensor): input text tensor\n",
    "        input_image (torch.Tensor): input image tensor\n",
    "        decoder_input_ids (torch.Tensor) : Decode input IDs tensor\n",
    "        attention_mask (torch.Tensor): Attention mask for the input text\n",
    "        labels (torch.Tensor, optional): Ground truth labels for the target\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        torch.Tensor: Decoder output\n",
    "        \"\"\"\n",
    "        self.check_input(decoder_input_ids, \"decoder_input_ids\")\n",
    "        \n",
    "        # Encode text and image\n",
    "        text_projected = self.encode_text(input_text, attention_mask)\n",
    "        image_projected = self.encode_image(input_image)\n",
    "        \n",
    "        # Combined encoded features\n",
    "        combined_features = (text_projected + image_projected) / 2\n",
    "        if labels is not None:\n",
    "            labels = torch.where(labels == decoder_tokenizer.pad_token_id, -100, labels)\n",
    "            \n",
    "        # Decode with GPT-2\n",
    "        decoder_outputs = self.decode(\n",
    "            input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "            encoder_hidden_state=combined_features.unsqueeze()\n",
    "        )\n",
    "        return decoder_outputs\n",
    "    \n",
    "    def generate(self, image, questions, max_text_length=5):\n",
    "        \"\"\"\n",
    "        Generate answers for the given image and list of questions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        image (Image): input image\n",
    "        question (list): List of questions related to the image\n",
    "        max_text_length (int, optional): Maximum text length for generated answers\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        Image: input image\n",
    "        \"\"\"\n",
    "        \n",
    "        # encode text and image\n",
    "        image = retrieve_image(image)\n",
    "        image_input = image_feature_extractor(images=[preprocess_image(image)], return_tensors='pt')\n",
    "        input_image = image_input[\"pixel_values\"]\n",
    "        image_projected = self.encode_image(input_image)\n",
    "        \n",
    "        for question in questions:\n",
    "            i = text_tokenizer(question, return_tensors='pt')\n",
    "            text_projected = self.encode_text(i['input_ids'], i['attention_mask'])\n",
    "            \n",
    "            # combine encoded features\n",
    "            combined_features = (text_projected + image_projected) / 2\n",
    "            \n",
    "            generated_so_far = torch.LongTensor([[decoder_tokenizer.bos_token_id]])\n",
    "            with torch.no_grad():\n",
    "                for _ in tqdm(range(max_text_length)):\n",
    "                    \n",
    "                    decode_outputs = self.decoder(\n",
    "                        input_ids=generated_so_far,\n",
    "                        encoder_hidden_states=combined_features.unsqueeze()\n",
    "                    )\n",
    "                    next_token_logits = decode_outputs.logits[:, -1, :]\n",
    "                    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "                    next_token = next_token_logits.argmax(-1)\n",
    "                    confidence = next_token_probs[0, next_token].item()\n",
    "                    print(\"Next token:\", decoder_tokenizer.decode(next_token), \"Confidence:\", confidence)\n",
    "                    generated_so_far = torch.cat((generated_so_far, next_token.unsqueeze(0)), dim=1)\n",
    "            print(question, decoder_tokenizer.decode(generated_so_far[0]))\n",
    "            \n",
    "        return image\n",
    "                \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './data/vga_custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bd58619e884167977876df6f8500f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/443757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = load_vga_data(\n",
    "    \"./data/v2_mscoco_train2014_annotations.json\", \"./data/v2_OpenEnded_mscoco_train2014_questions.json\", \"./data/train2014\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
