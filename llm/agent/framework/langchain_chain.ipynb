{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keyring\n",
    "OPENAI_API_KEY = keyring.get_password('openai', 'key_for_windows')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AX0OijLOSPJ22yAaAM8nLqrVbPk5Q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ë¡œì„œëŠ” 2022ë…„ ì›”ë“œì»µ 4ê°• êµ­ê°€ë¥¼ ì•Œë ¤ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì‹œì ì—ì„œëŠ” ì•„ì§ ì›”ë“œì»µì´ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ ì˜ˆì •ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ì›”ë“œì»µì´ ì§„í–‰ ì¤‘ì¸ ì‹œê¸°ì— ë¬¼ì–´ ë³´ì‹œë©´ ë” ë‚˜ì€ ëŒ€ë‹µì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732429716, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=137, prompt_tokens=32, total_tokens=169, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"2022ë…„ ì›”ë“œì»µ 4ê°• êµ­ê°€ ì•Œë ¤ì¤˜.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì–¸ì–´ ëª¨ë¸ì¸ ChatGPTì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ëŒ€í™”í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ë©° ì •ë³´ ì œê³µì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸í•™, ê³¼í•™, ê¸°ìˆ , ì¼ìƒì ì¸ ì§ˆë¬¸ ë“± ì—¬ëŸ¬ ë¶„ì•¼ì— ê±¸ì³ ì—¬ëŸ¬ë¶„ê³¼ ì†Œí†µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 17, 'total_tokens': 89, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-53f42f3c-f208-4e01-b189-875293b4cb10-0', usage_metadata={'input_tokens': 17, 'output_tokens': 72, 'total_tokens': 89, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "chat.invoke(\"ì•ˆë…•! ë„ˆë¥¼ ì†Œê°œí•´ì¤„ë˜?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['ê°œìˆ˜', 'ì¬ë£Œ'], input_types={}, partial_variables={}, template=' \\n        ë„ˆëŠ” ìš”ë¦¬ì‚¬ì•„. ë‚´ê°€ ê°€ì§„ ì¬ë£Œë“¤ì„ ê°–ê³  ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ë¥¼ {ê°œìˆ˜}ì¶”ì²œí•˜ê³ ,\\n        ê·¸ ìš”ë¦¬ì˜ ë ˆì‹œí”¼ë¥¼ ì œì‹œí•´ì¤˜. ë‚´ê°€ ê°€ì§„ ì¬ë£ŒëŠ” ì•„ë˜ì™€ ê°™ì•„.\\n        <ì¬ë£Œ>\\n        {ì¬ë£Œ}\\n        ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\" \n",
    "        ë„ˆëŠ” ìš”ë¦¬ì‚¬ì•„. ë‚´ê°€ ê°€ì§„ ì¬ë£Œë“¤ì„ ê°–ê³  ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ë¥¼ {ê°œìˆ˜}ì¶”ì²œí•˜ê³ ,\n",
    "        ê·¸ ìš”ë¦¬ì˜ ë ˆì‹œí”¼ë¥¼ ì œì‹œí•´ì¤˜. ë‚´ê°€ ê°€ì§„ ì¬ë£ŒëŠ” ì•„ë˜ì™€ ê°™ì•„.\n",
    "        <ì¬ë£Œ>\n",
    "        {ì¬ë£Œ}\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=' \\n        ë„ˆëŠ” ìš”ë¦¬ì‚¬ì•„. ë‚´ê°€ ê°€ì§„ ì¬ë£Œë“¤ì„ ê°–ê³  ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ë¥¼ 6ì¶”ì²œí•˜ê³ ,\\n        ê·¸ ìš”ë¦¬ì˜ ë ˆì‹œí”¼ë¥¼ ì œì‹œí•´ì¤˜. ë‚´ê°€ ê°€ì§„ ì¬ë£ŒëŠ” ì•„ë˜ì™€ ê°™ì•„.\\n        <ì¬ë£Œ>\\n        ì‚¬ê³¼, ì¼\\n        ')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt templateì— input_variablesë¥¼ ì±„ìš¸ ê²½ìš°\n",
    "prompt.invoke({\"ê°œìˆ˜\": 6, \"ì¬ë£Œ\": \"ì‚¬ê³¼, ì¼\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #SystemMessage : ìœ ìš©í•œ ì±—ë´‡ì´ë¼ëŠ” ì—­í• ê³¼ ì´ë¦„ ë¶€ì—¬\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        #HumanMessageì™€ AIMessage : ì„œë¡œ ì•ˆë¶€ë¥¼ ë¬»ê³  ë‹µí•˜ëŠ” ëŒ€í™” íˆìŠ¤í† ë¦¬ ì£¼ì…\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        #HumanMessageë¡œ ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸ ì „ë‹¬\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name='Bob', user_input=\"What is your name?\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down? \\n\\nBecause it drove over a rocky road! ğŸ¦'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser       # llmì˜ ì‘ë‹µê°’ ì¤‘ metadataëŠ” ì œì™¸í•˜ê³  str ì •ë³´ë§Œ parsing\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# set prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "# call the LLMs\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# connect prompt template, llm output parser with LCEL\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# execute the chain by calling invoke\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bear sit in front of the computer?\n",
      "\n",
      "Because it wanted to keep an eye on its favorite \"websites\"!"
     ]
    }
   ],
   "source": [
    "# declare Chain\n",
    "model = ChatOpenAI(model='gpt-4o')\n",
    "prompt = ChatPromptTemplate.from_template('tell me a joke about {topic}')\n",
    "chain = prompt | model\n",
    "\n",
    "# add streaming with Chain's stream() function\n",
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='[\"ë‹¤í¬ ë‚˜ì´íŠ¸\", \"ì¡´ ìœ…\", \"ë§¤ë“œë§¥ìŠ¤: ë¶„ë…¸ì˜ ë„ë¡œ\"]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 72, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-975c3890-73d9-423d-8a15-d8305cd32f0a-0', usage_metadata={'input_tokens': 72, 'output_tokens': 23, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# ChatPromptTemplateì— SystemMessageë¡œ LLMì˜ ì—­í• ê³¼ ì¶œë ¥ í˜•ì‹ ì§€ì •\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"ë„ˆëŠ” ì˜í™” ì „ë¬¸ê°€ AIì•¼. ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì¥ë¥´ì˜ ì˜í™”ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶”ì²œí•´ì¤˜.\"\n",
    "                'ex) Query: SFì˜í™” 3ê°œ ì¶”ì²œí•´ì¤˜ / ë‹µë³€: [\"ì¸í„°ìŠ¤í…”ë¼\", \"ìŠ¤í˜ì´ìŠ¤ì˜¤ë””ì„¸ì´\", \"í˜¹ì„±íƒˆì¶œ\"]'\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "chain = chat_template | model\n",
    "chain.invoke(\"ì•¡ì…˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# CSV parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# load prompt for the CSV parser\n",
    "format_instuctions = output_parser.get_format_instructions()\n",
    "format_instuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['í† ì´ ìŠ¤í† ë¦¬',\n",
       " 'ê²¨ìš¸ì™•êµ­',\n",
       " 'ì£¼í† í”¼ì•„',\n",
       " 'ìŠˆë ‰',\n",
       " 'ëª¨ì•„ë‚˜',\n",
       " 'ì½”ì½”',\n",
       " 'ë²…ìŠ¤ ë¼ì´í”„',\n",
       " 'ë¯¸ë‹ˆì–¸ì¦ˆ',\n",
       " 'ì¸ì‚¬ì´ë“œ ì•„ì›ƒ',\n",
       " 'ì£¼ë¨¹ì™• ë„í”„']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# CSV parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# load prompt for the CSV parser\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# insert formatted instructions to the partial_variables of the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List {subject}. answer in Korean \\n{format_instructions}\",\n",
    "    input_variables=['subject'],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# Chaining PromptTemplate - Model - OutputParser\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"subject\": \"ì–´ë¦°ì´ ì˜í™”\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contient': 'ë‚¨ì•„ë©”ë¦¬ì¹´', 'popluation': '45195777'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# define data structure\n",
    "class Country(BaseModel):\n",
    "    contient: str = Field(description=\"ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ë‚˜ë¼ê°€ ì†í•œ ëŒ€ë¥™\")\n",
    "    popluation: str = Field(description=\"ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ë‚˜ë¼ì˜ ì¸êµ¬(int í˜•ì‹)\")\n",
    "    \n",
    "# JsonOutputParserë¥¼ ì„¤ì •í•˜ê³  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— format_instructionsì„ ì‚½ì…í•œë‹¤.\n",
    "parser = JsonOutputParser(pydantic_object=Country)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. \\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "country_query = \"ì•„ë¥´í—¨í‹°ë‚˜ëŠ” ì–´ë–¤ ë‚˜ë¼ì•¼?\"\n",
    "chain.invoke({\"query\": country_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCELì˜ Runnable ê°ì²´ì— ëŒ€í•´ ì•Œì•„ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough ì•Œì•„ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ í†µê³¼\n",
    "RunnablePassthrough().invoke(\"ì•ˆë…•í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elle lit un livre chaque matin.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"ë‹¤ìŒ í•œê¸€ ë¬¸ì¥ì„ í”„ë‘ìŠ¤ì–´ë¡œ ë²ˆì—­í•´ì¤˜ {sentence}\n",
    "French sentence: (print from here)\"\"\")\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "runnable_chain = {\"sentence\": RunnablePassthrough()} | prompt | model | output_parser\n",
    "runnable_chain.invoke({\"sentence\": \"ê·¸ë…€ëŠ” ë§¤ì¼ ì•„ì¹¨ ì±…ì„ ì½ìŠµë‹ˆë‹¤.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 3, 'mult': 9}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ ì¶”ê°€ ê°€ëŠ¥\n",
    "# numìœ¼ë¡œ ë“¤ì–´ì˜¨ ìˆ«ìì— 3ì„ ê³±í•´ multì— ì €ì¥\n",
    "(RunnablePassthrough.assign(mult=lambda x: x[\"num\"]*3)).invoke({\"num\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# ë³‘ë ¬ë¡œ ì‹¤í–‰\n",
    "# extra(RunnablePassthrough, assign), modified ë‘ê°œ ë™ì‹œ ì‹¤í–‰\n",
    "runnable = RunnableParallel(\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x['num']*3),\n",
    "    modified=lambda x: x['num'] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda ì•Œì•„ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_smile(x):\n",
    "    return x + \":)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Runnable ê°ì²´ ë³€í™˜\n",
    "add_smile = RunnableLambda(add_smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt_str = \"{topic}ì˜ ì—­ì‚¬ì— ëŒ€í•´ ì„¸ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_thanks(x):\n",
    "    return x + \" ë“¤ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ :)\"\n",
    "\n",
    "add_thanks = RunnableLambda(add_thanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë°˜ë„ì²´ì˜ ì—­ì‚¬ëŠ” 19ì„¸ê¸° ì¤‘ë°˜ìœ¼ë¡œ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©°, ì´ˆê¸°ì—ëŠ” ì£¼ë¡œ ì—°êµ¬ì™€ ì‹¤í—˜ì ì¸ í˜•íƒœë¡œ ì¡´ì¬í–ˆìŠµë‹ˆë‹¤. 1947ë…„ ë²¨ ì—°êµ¬ì†Œì˜ ê³¼í•™ìë“¤ì´ íŠ¸ëœì§€ìŠ¤í„°ë¥¼ ë°œëª…í•˜ë©´ì„œ ë°˜ë„ì²´ ê¸°ìˆ ì€ í˜ì‹ ì ì¸ ë°œì „ì„ ì´ë£¨ì—ˆê³ , ì´ë¥¼ í†µí•´ ì „ìê¸°ê¸°ì™€ ì»´í“¨í„°ì˜ ë°œì „ì´ ê°€ì†í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì´í›„ ë°˜ë„ì²´ëŠ” í˜„ëŒ€ ì „ì ì‚°ì—…ì˜ í•µì‹¬ ìš”ì†Œë¡œ ìë¦¬ì¡ìœ¼ë©°, ì •ë³´í†µì‹ , ìë™ì°¨, ì˜ë£Œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•„ìˆ˜ì ì¸ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë“¤ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ :)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser | add_thanks\n",
    "chain.invoke(\"ë°˜ë„ì²´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel ì•Œì•„ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'modified': 2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': 'ì•ˆë…•í•˜ì„¸ìš”.', 'modified': 'ì•ˆë…•í•˜ì„¸ìš”. ë“¤ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ :)'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=add_thanks,\n",
    ")\n",
    "\n",
    "runnable.invoke(\"ì•ˆë…•í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history_chain': 'AIëŠ” \"Artificial Intelligence\"ì˜ ì•½ìë¡œ, í•œêµ­ì–´ë¡œëŠ” \"ì¸ê³µì§€ëŠ¥\"ì´ë¼ê³  í•©ë‹ˆë‹¤. ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ í•™ìŠµ, ì¶”ë¡ , ë¬¸ì œ í•´ê²°, ì–¸ì–´ ì´í•´ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.',\n",
       " 'celeb': '1. ì•¤ë“œë¥˜ ì‘ (Andrew Ng)\\n2. ì œí”„ë¦¬ íŒíŠ¼ (Geoffrey Hinton)\\n3. ì–€ ë¥´ì¿¤ (Yann LeCun)'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4o-mini', max_tokens=128, temperature=0)\n",
    "\n",
    "history_prompt = ChatPromptTemplate.from_template(\"{topic}ê°€ ë¬´ì—‡ì˜ ì•½ìì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”.\")\n",
    "celeb_prompt = ChatPromptTemplate.from_template(\"{topic} ë¶„ì•¼ì˜ ìœ ëª…ì¸ì‚¬ 3ëª…ì˜ ì´ë¦„ë§Œ ì•Œë ¤ì£¼ì„¸ìš”.\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "history_chain = history_prompt | model | output_parser\n",
    "celeb_chain = celeb_prompt | model | output_parser\n",
    "\n",
    "map_chain = RunnableParallel(history_chain=history_chain, celeb=celeb_chain)\n",
    "\n",
    "result = map_chain.invoke({\"topic\": \"AI\"})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
